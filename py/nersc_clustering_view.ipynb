{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import fitsio\n",
    "from pycorr import TwoPointCorrelationFunction, TwoPointEstimator, project_to_multipoles, project_to_wp, utils, setup_logging\n",
    "from scipy.optimize import curve_fit\n",
    "from LSS.common_tools import mknz\n",
    "from astropy.table import Table\n",
    "import itertools\n",
    "\n",
    "from dataloc import *\n",
    "from groupcatalog import read_wp_file\n",
    "\n",
    "# MAKE ALL PLOTS TEXT BIGGER\n",
    "plt.rcParams.update({'font.size': 15})\n",
    "# But legend a bit smaller\n",
    "plt.rcParams.update({'legend.fontsize': 12})\n",
    "# Set DPI up a bit\n",
    "plt.rcParams.update({'figure.dpi': 150})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the prepared data\n",
    "dir = os.path.join(CUSTOM_CLUSTERING_RESULTS_FOLDER, 'Y1', 'LSS', 'iron', 'LSScats', 'v1.5pip')\n",
    "tbl1 = Table(fitsio.read(os.path.join(dir, 'BGS_BRIGHT_clustering.dat.fits')))\n",
    "tbl2 = Table(fitsio.read(os.path.join(dir, 'BGS_BRIGHT_0_clustering.ran.fits')))\n",
    "print(len(tbl1), len(tbl2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print off how many galaxies there are in each mag bin (for QUEISCENT and ~QUEISCENT)\n",
    "# The 'how many randoms file to use' is wrong... the randoms are processed in the same way as the data,\n",
    "# So they get cut from 19M or so per file to far less. There are about 5x mroe randoms than data after cuts, per file.\n",
    "# That's only 18*5 = 90x as many randoms as data...\n",
    "#zmaxes = [0.01650, 0.02595, 0.4067, 0.06336, 0.09792, 0.14977, 0.22620, 0.33694, 0.49523]\n",
    "#mags = [-14, -15, -16, -17, -18, -19, -20, -21, -22, -23]\n",
    "\n",
    "zmaxes = [0.06336, 0.09792, 0.14977, 0.22620, 0.33694, 0.49523]\n",
    "mags = [-17, -18, -19, -20, -21, -22, -23]\n",
    "for i in range(len(zmaxes)):\n",
    "    r1 = (tbl1['ABSMAG_R'] < mags[i]) & (tbl1['ABSMAG_R'] > mags[i + 1]) & (tbl1['QUIESCENT']) & (tbl1['Z'] < zmaxes[i])\n",
    "    #r2 = (tbl2['ABSMAG_R'] < mags[i]) & (tbl2['ABSMAG_R'] > mags[i + 1]) & (tbl2['QUIESCENT']) & (tbl2['Z'] < zmaxes[i])\n",
    "    rows = np.sum(r1) #np.sum(r2) + np.sum(r1)\n",
    "    print(f'RED {mags[i + 1]} {mags[i]} : {rows}.')\n",
    "    #print(f'RED {mags[i]} {mags[i + 1]}: {rows}. Random Files to Use for 500x {np.ceil((rows*500)/(2500*7500))}')\n",
    "    b1 = (tbl1['ABSMAG_R'] < mags[i]) & (tbl1['ABSMAG_R'] > mags[i + 1]) & (~tbl1['QUIESCENT']) & (tbl1['Z'] < zmaxes[i])\n",
    "    #b2 = (tbl2['ABSMAG_R'] < mags[i]) & (tbl2['ABSMAG_R'] > mags[i + 1]) & (~tbl2['QUIESCENT']) & (tbl2['Z'] < zmaxes[i])\n",
    "    rows = np.sum(b1) #np.sum(b2) + np.sum(b1)\n",
    "    print(f'BLUE {mags[i + 1]} {mags[i]} : {rows}.')\n",
    "    #print(f'BLUE {mags[i]} {mags[i + 1]}: {rows}. Random Files to Use for 500x {np.ceil((rows*500)/(2500*7500))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## No longer needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE BINS FOR WP CALCULATION\n",
    "edges = np.geomspace(0.1, 20, 16)\n",
    "\n",
    "# For my own use\n",
    "with open(WP_RADIAL_BINS_DESI_FILE, 'w') as f:\n",
    "    for i in range(len(edges)-1):\n",
    "        f.write(f'{edges[i]:.8f} {edges[i+1]:.8f}\\n')\n",
    "\n",
    "# For xirunpc.py script\n",
    "with open(WP_RADIAL_EDGE_DESI_FILE, 'w') as f:\n",
    "    for i in range(len(edges)):\n",
    "        f.write(f'{edges[i]:.8f}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_cols_for_tbl(path, colname):\n",
    "    if os.path.isfile(path) == False:\n",
    "        print(f'{path} not found')\n",
    "        return\n",
    "    cols = fitsio.FITS(path)[1].get_colnames()\n",
    "    if colname not in cols:\n",
    "        print(f'{colname} not in {path}')\n",
    "    else:\n",
    "        print(f'{colname} found in {path}')\n",
    "    print(cols)\n",
    "check_cols_for_tbl('/dvs_ro/cfs/cdirs/desi/survey/catalogs/Y1/LSS/iron/LSScats/v1.5pip/BGS_BRIGHT_full_HPmapcut.dat.fits', 'QUIESCENT')\n",
    "check_cols_for_tbl('/global/cfs/cdirs/desi/users/ianw89/clustering/Y1/LSS/iron/LSScats/v1.5pip/BGS_BRIGHT_NGC_clustering.dat.fits', 'QUIESCENT')\n",
    "check_cols_for_tbl('/global/cfs/cdirs/desi/users/ianw89/clustering/Y1/LSS/iron/LSScats/v1.5pip/BGS_BRIGHT_NGC_0_clustering.ran.fits', 'QUIESCENT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install corrfunc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wp_for(tracer, weights, survey, verspec, ver, bins, zmax, nran, njack, region, quiescent=None):\n",
    "    dir = os.path.join(CUSTOM_CLUSTERING_RESULTS_FOLDER, survey, 'LSS', verspec, 'LSScats', ver, 'rppi')\n",
    "\n",
    "    if not os.path.exists(dir):\n",
    "        print(\"WARNING: Directory does not exist: \", dir)\n",
    "        return None\n",
    "\n",
    "    if quiescent is not None:\n",
    "        addon = '_QUIESCENT' + ('1' if quiescent else '0')\n",
    "    else:\n",
    "        addon = ''\n",
    "        \n",
    "    wp_fn = os.path.join(dir, f'wp_{tracer}_{region}_0.001_{zmax}_{weights}_{bins}_njack{njack}_nran{nran}_split20{addon}.txt')\n",
    "\n",
    "    if not os.path.exists(wp_fn):\n",
    "        print(\"WARNING: File does not exist: \", wp_fn)\n",
    "        return None\n",
    "\n",
    "    return np.loadtxt(wp_fn)\n",
    "\n",
    "def get_fn_for(weights, survey, verspec, ver, zmax, nran, njack, region, quiescent=None):\n",
    "    dir = os.path.join(CUSTOM_CLUSTERING_RESULTS_FOLDER, survey, 'LSS', verspec, 'LSScats', ver, 'rppi')\n",
    "\n",
    "    if not os.path.exists(dir):\n",
    "        print(\"WARNING: Directory does not exist: \", dir)\n",
    "        return None\n",
    "\n",
    "    if quiescent is not None:\n",
    "        addon = '_QUIESCENT' + ('1' if quiescent else '0')\n",
    "    else:\n",
    "        addon = ''\n",
    "        \n",
    "    allcounts_fn = os.path.join(dir, f'allcounts_BGS_BRIGHT_{region}_0.001_{zmax}_{weights}_custom_njack{njack}_nran{nran}_split20{addon}.npy')\n",
    "    return allcounts_fn\n",
    "    #if not os.path.exists(allcounts_fn):\n",
    "    #    print(\"WARNING: File does not exist: \", allcounts_fn)\n",
    "    #    return None\n",
    "\n",
    "   # return np.load(allcounts_fn, allow_pickle=True)\n",
    "\n",
    "\n",
    "def save_wp(savedir, red_results, blue_results, all_results, magbins):\n",
    "    \n",
    "     # Save the results to text files in the format we want, and also save the covariance matrix as numpy array\n",
    "    for i in range(len(red_results)):\n",
    "        red_wp, red_cov = red_results[i]\n",
    "        blue_wp, blue_cov = blue_results[i]\n",
    "        all_wp, all_cov = all_results[i]\n",
    "\n",
    "        # Currently we're choosing not to use the full covariance matrix, just the diagonal for our chi squared\n",
    "        # since the result of the jackknife tests was kinda weird correlation matrices.\n",
    "\n",
    "        # Format is: rp wp wp_err\n",
    "        if red_wp is not None:\n",
    "            with open(os.path.join(savedir, f'wp_red_M{-magbins[i]:d}.dat'), 'w') as f:\n",
    "                for j in range(len(red_wp)):\n",
    "                    f.write(f'{red_wp[j,0]:.8f} {red_wp[j,2]:.8f} {red_wp[j,3]:.8f}\\n')\n",
    "            np.save(os.path.join(savedir, f'wp_red_M{-magbins[i]:d}_cov.npy'), red_cov)\n",
    "\n",
    "        if blue_wp is not None:\n",
    "            with open(os.path.join(savedir, f'wp_blue_M{-magbins[i]:d}.dat'), 'w') as f:\n",
    "                for j in range(len(blue_wp)):\n",
    "                    f.write(f'{blue_wp[j,0]:.8f} {blue_wp[j,2]:.8f} {blue_wp[j,3]:.8f}\\n')\n",
    "            np.save(os.path.join(savedir, f'wp_blue_M{-magbins[i]:d}_cov.npy'), blue_cov)\n",
    "            \n",
    "        if all_wp is not None:\n",
    "            with open(os.path.join(savedir, f'wp_all_M{-magbins[i]:d}.dat'), 'w') as f:\n",
    "                for j in range(len(all_wp)):\n",
    "                    f.write(f'{all_wp[j,0]:.8f} {all_wp[j,2]:.8f} {all_wp[j,3]:.8f}\\n')\n",
    "            np.save(os.path.join(savedir, f'wp_all_M{-magbins[i]:d}_cov.npy'), all_cov)\n",
    "    \n",
    "\n",
    "    \n",
    "def plot_wp(red_results, blue_results, all_results, zmaxes, magbins):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for i in range(len(red_results)):\n",
    "        plt.subplot(3, 3, i+1)\n",
    "        red_wp, red_cov = red_results[i]\n",
    "        blue_wp, blue_cov = blue_results[i]\n",
    "        all_wp, all_cov = all_results[i]\n",
    "        if red_wp is not None:\n",
    "            plt.errorbar(red_wp[:,0], red_wp[:,2], yerr=red_wp[:,3], label=f'Quiescent {zmaxes[i]}', fmt='r.', capsize=3)\n",
    "        if blue_wp is not None and red_wp is not None:\n",
    "            plt.errorbar(blue_wp[:,0], blue_wp[:,2], yerr=blue_wp[:,3], label=f'Star-Forming {zmaxes[i]}', fmt='b.', capsize=3)\n",
    "        if all_wp is not None:\n",
    "            plt.errorbar(all_wp[:,0], all_wp[:,2], yerr=all_wp[:,3], label=f'All {zmaxes[i]}', fmt='k.', capsize=3)\n",
    "        plt.xscale('log')\n",
    "        plt.yscale('log')\n",
    "        plt.ylim(2, 4000)\n",
    "        plt.ylabel(r'$w_p(r_p)$')\n",
    "        plt.xlabel(r'$r_p$ [Mpc/h]')\n",
    "        plt.title(f'${magbins[i]} > M_r > {magbins[i+1]}$')\n",
    "        plt.text(0.75, 0.9, f'$z<{zmaxes[i]}$', transform=plt.gca().transAxes, ha='center', va='center', fontsize=10)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jackknife Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiescent = False\n",
    "#quiescent = None\n",
    "nran = 18\n",
    "#nran = 8\n",
    "zmax = 0.22620\n",
    "#zmax = 0.14977\n",
    "tracer = \"BGS_BRIGHT\" \n",
    "njack = [4, 8, 16, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 192, 256, 512, 1028]\n",
    "jacktest_results = []\n",
    "jacktset_obj = []\n",
    "jacktest_cov = []\n",
    "jacktest_corr = []\n",
    "for nj in njack:\n",
    "    jacktest_results.append(get_wp_for(tracer, 'pip_angular_bitwise', 'Y1', 'iron', 'v1.5pip', 'custom', zmax, nran, nj, 'GCcomb', quiescent=quiescent))\n",
    "    tpc = TwoPointEstimator.load(get_fn_for('pip_angular_bitwise', 'Y1', 'iron', 'v1.5pip', zmax, nran, nj, 'GCcomb', quiescent=quiescent))\n",
    "    s, xiell, cov = tpc.get_corr(return_sep=True, return_cov=True, mode='wp')\n",
    "    jacktset_obj.append(tpc)\n",
    "    jacktest_cov.append(cov)\n",
    "\n",
    "for cov in jacktest_cov:    \n",
    "    # ~C_ij = C_ij / sqrt(C_ii C_jj)\n",
    "    jacktest_corr.append(cov / np.sqrt(np.outer(np.diag(cov), np.diag(cov))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test how the cov matrix is changing as the njack increases using subplots\n",
    "fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(15, 15))\n",
    "axes = axes.flatten()\n",
    "i=0\n",
    "for ii in [0,1,2,3,7,15,21,22,23]:\n",
    "    im = axes[i].imshow(jacktest_corr[ii], cmap='coolwarm', vmin=-1, vmax=1)\n",
    "    axes[i].set_title(f\"Corr Matrix (njack={njack[ii]})\")\n",
    "    \n",
    "    # Remvoe bin # labels\n",
    "    axes[i].set_xticks(np.arange(len(jacktest_corr[ii])))\n",
    "    axes[i].set_xticklabels([])\n",
    "    axes[i].set_yticks(np.arange(len(jacktest_corr[ii])))\n",
    "    axes[i].set_yticklabels([])\n",
    "    i+=1\n",
    "    \n",
    "# Create a single colorbar for all subplots\n",
    "fig.subplots_adjust(right=0.85)\n",
    "cbar_ax = fig.add_axes([0.87, 0.15, 0.03, 0.7])\n",
    "cbar = fig.colorbar(im, cax=cbar_ax)\n",
    "cbar.set_label('Correlation Coefficient')\n",
    "\n",
    "# Switch from bin numbers to rp values\n",
    "plt.tight_layout(rect=[0, 0, 0.85, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT THE SIGMA OF THE JACKKNIFE TESTS ONLY\n",
    "plt.figure()\n",
    "c=1.0\n",
    "for i in [1,2,3,7,15,21,22,23]:\n",
    "    c -= 0.11\n",
    "    wp = jacktest_results[i]\n",
    "    if wp is not None:\n",
    "        # color 128 one specially\n",
    "        #color = 'red' if njack[i] == 64 else \n",
    "        color =[0.0, c, 0.0]\n",
    "        plt.plot(wp[:,0], wp[:,3], label=f'njack={njack[i]}', marker='o', color=color)\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlabel(r'$r_p$ $[h^{-1} Mpc]$')\n",
    "plt.ylabel(r'$\\sigma_{\\omega_p}$')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's fit the results to a power law\n",
    "def power_law(x, a, b):\n",
    "    return a * x**b\n",
    "\n",
    "rps = jacktest_results[0][:,0] # They are all the same\n",
    "\n",
    "to_fit = jacktest_results[10]\n",
    "popt, pcov = curve_fit(power_law, rps, to_fit[:,2], sigma=to_fit[:,3], absolute_sigma=True)\n",
    "print(popt)\n",
    "\n",
    "# And plot the data and the fit\n",
    "plt.figure()\n",
    "for i in range(len(jacktest_results)):\n",
    "    wp = jacktest_results[i]\n",
    "    if wp is not None:\n",
    "        plt.errorbar(wp[:,0], wp[:,2], yerr=wp[:,3], label=f'njack={njack[i]}', fmt='.', capsize=2)\n",
    "plt.plot(rps, power_law(rps, *popt), label='Fit', linestyle='--')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.ylabel(r'$w_p(r_p)$')\n",
    "plt.xlabel(r'$r_p$ [Mpc/h]') \n",
    "\n",
    "def chi2(y, yfit, cov):\n",
    "    return np.dot(y-yfit, np.linalg.solve(cov, y-yfit))\n",
    "\n",
    "chi2s = []\n",
    "for i in range(len(jacktest_results)):\n",
    "    wp = jacktest_results[i]\n",
    "    if wp is not None:\n",
    "        chi2s.append(chi2(wp[:,2], power_law(wp[:,0], *popt), jacktest_cov[i]))\n",
    "    else:\n",
    "        chi2s.append(None)\n",
    "\n",
    "# Plot the chi2 values as a function of njack\n",
    "plt.figure()\n",
    "plt.plot(njack[3:], chi2s[3:],  'o', label='Full Covariance')\n",
    "plt.xlabel('njack')\n",
    "plt.ylabel(r'$\\chi^2$')\n",
    "plt.xscale('log')\n",
    "plt.legend()\n",
    "plt.ylim(100,250)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's fit it to a broken power law\n",
    "def broken_power_law(x, a1, b1, a2, b2, x0):\n",
    "    return np.where(x < x0, a1 * x**b1, a2 * x**b2)\n",
    "\n",
    "rps = jacktest_results[0][:,0] # They are all the same\n",
    "\n",
    "to_fit = jacktest_results[10]\n",
    "popt, pcov = curve_fit(broken_power_law, rps, to_fit[:,2], sigma=to_fit[:,3], absolute_sigma=True)\n",
    "print(popt)\n",
    "\n",
    "# And plot the data and the fit\n",
    "plt.figure()\n",
    "for i in range(len(jacktest_results)):\n",
    "    wp = jacktest_results[i]\n",
    "    if wp is not None:\n",
    "        plt.errorbar(wp[:,0], wp[:,2], yerr=wp[:,3], label=f'njack={njack[i]}', fmt='.', capsize=2)\n",
    "plt.plot(rps, broken_power_law(rps, *popt), label='Fit', linestyle='--')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.ylabel(r'$w_p(r_p)$')\n",
    "plt.xlabel(r'$r_p$ [Mpc/h]')\n",
    "\n",
    "# Calculate chi squared for the fit, using the covariance matrix from each njack test\n",
    "def chi2(y, yfit, cov):\n",
    "    return np.dot(y-yfit, np.linalg.solve(cov, y-yfit))\n",
    "\n",
    "chi2s = []\n",
    "for i in range(len(jacktest_results)):\n",
    "    wp = jacktest_results[i]\n",
    "    if wp is not None:\n",
    "        chi2s.append(chi2(wp[:,2], broken_power_law(wp[:,0], *popt), jacktest_cov[i]))\n",
    "    else:\n",
    "        chi2s.append(None)\n",
    "\n",
    "# Plot the chi2 values as a function of njack\n",
    "plt.figure()\n",
    "plt.plot(njack[3:], chi2s[3:],  'o', label='Full Covariance')\n",
    "plt.xlabel('njack')\n",
    "plt.ylabel(r'$\\chi^2$')\n",
    "plt.xscale('log')\n",
    "plt.legend()\n",
    "plt.ylim(100,250)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zmaxes = [0.06336, 0.09792, 0.14977, 0.22620, 0.33694, 0.49523] \n",
    "magbins = [-17, -18, -19, -20, -21, -22, -23]\n",
    "tracer = \"BGS_BRIGHT\" \n",
    "jack_official = 128\n",
    "mini_red_results = []\n",
    "mini_blue_results = []\n",
    "mini_all_results = []\n",
    "for z, q in itertools.product(zmaxes, [True, False, None]):\n",
    "    wp = get_wp_for(tracer, 'pip_angular_bitwise', 'Y1', 'iron', 'v1.5pip/mini', 'custom', z, 18, jack_official, 'NGC', quiescent=q)\n",
    "    cov = None\n",
    "    if wp is not None:\n",
    "        fn = get_fn_for('pip_angular_bitwise', 'Y1', 'iron', 'v1.5pip/mini', z, 18, jack_official, 'NGC', quiescent=q)\n",
    "        tpc = TwoPointEstimator.load(fn)\n",
    "        s, xiell, cov = tpc.get_corr(return_sep=True, return_cov=True, mode='wp')\n",
    "\n",
    "    if q is True:\n",
    "        mini_red_results.append((wp, cov))\n",
    "    elif q is False:\n",
    "        mini_blue_results.append((wp, cov))\n",
    "    else:\n",
    "        mini_all_results.append((wp, cov))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wp(mini_red_results, mini_blue_results, mini_all_results, zmaxes, magbins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_wp(PARAMS_BGSY1MINI_FOLDER, mini_red_results, mini_blue_results, mini_all_results, magbins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zmaxes = [0.02595, 0.04067, 0.06336, 0.09792, 0.14977, 0.22620, 0.33694, 0.49523] \n",
    "magbins = [-15, -16, -17, -18, -19, -20, -21, -22, -23]\n",
    "tracer = \"BGS_BRIGHT\" \n",
    "jack_official = 128\n",
    "main_red_results = []\n",
    "main_blue_results = []\n",
    "main_all_results = []\n",
    "for z, q in itertools.product(zmaxes, [True, False, None]):\n",
    "\n",
    "    wp = get_wp_for(tracer, 'pip_angular_bitwise', 'Y1', 'iron', 'v1.5pip', 'custom', z, 18, jack_official, 'GCcomb', quiescent=q)\n",
    "    cov = None\n",
    "    if wp is not None:\n",
    "        savedir = get_fn_for('pip_angular_bitwise', 'Y1', 'iron', 'v1.5pip', z, 18, jack_official, 'GCcomb', quiescent=q)\n",
    "        tpc = TwoPointEstimator.load(savedir)\n",
    "        s, xiell, cov = tpc.get_corr(return_sep=True, return_cov=True, mode='wp')\n",
    "\n",
    "    if q is True:\n",
    "        main_red_results.append((wp, cov))\n",
    "    elif q is False:\n",
    "        main_blue_results.append((wp, cov))\n",
    "    else:\n",
    "        main_all_results.append((wp, cov))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wp(main_red_results, main_blue_results, main_all_results, zmaxes, magbins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_wp(PARAMS_BGSY1_FOLDER, main_red_results, main_blue_results, main_all_results, magbins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SDSS Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maglim = -22.4\n",
    "zmax = 0.245169\n",
    "zmin = 0.02\n",
    "# Make a luminosity threshold sample like SDSS to see if n(z) is same\n",
    "def make_sdsslike_cuts(fcd, maglim, zmax, zmin):\n",
    "    writename = fcd.replace('.dat.fits', f'_testcut.dat.fits')  \n",
    "    arr = fitsio.read(fcd)\n",
    "    arr = arr[arr['Z'] < zmax]\n",
    "    arr = arr[arr['Z'] > zmin]\n",
    "    arr = arr[arr['ABSMAG_R'] < maglim]\n",
    "    fitsio.write(writename, arr, clobber=True)\n",
    "\n",
    "outpath = '/global/cfs/cdirs/desi/users/ianw89/clustering/Y1/LSS/iron/LSScats/v1.5pip/BGS_BRIGHT_NGC_FOOTPRINT.txt'\n",
    "outpath2 = '/global/cfs/cdirs/desi/users/ianw89/clustering/Y1/LSS/iron/LSScats/v1.5pip/BGS_BRIGHT_SGC_FOOTPRINT.txt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_sdsslike_cuts('/global/cfs/cdirs/desi/users/ianw89/clustering/Y1/LSS/iron/LSScats/v1.5pip/BGS_BRIGHT_NGC_clustering.dat.fits', maglim, zmax, zmin)\n",
    "make_sdsslike_cuts('/global/cfs/cdirs/desi/users/ianw89/clustering/Y1/LSS/iron/LSScats/v1.5pip/BGS_BRIGHT_SGC_clustering.dat.fits', maglim, zmax, zmin)\n",
    "\n",
    "cat = '/global/cfs/cdirs/desi/users/ianw89/clustering/Y1/LSS/iron/LSScats/v1.5pip/BGS_BRIGHT_NGC_clustering_testcut.dat.fits'\n",
    "ran = '/global/cfs/cdirs/desi/users/ianw89/clustering/Y1/LSS/iron/LSScats/v1.5pip/BGS_BRIGHT_NGC_0_clustering.ran.fits'\n",
    "mknz(cat, ran, outpath, zmax=0.3)\n",
    "\n",
    "cat = '/global/cfs/cdirs/desi/users/ianw89/clustering/Y1/LSS/iron/LSScats/v1.5pip/BGS_BRIGHT_SGC_clustering_testcut.dat.fits'\n",
    "ran = '/global/cfs/cdirs/desi/users/ianw89/clustering/Y1/LSS/iron/LSScats/v1.5pip/BGS_BRIGHT_SGC_0_clustering.ran.fits'\n",
    "mknz(cat, ran, outpath2, zmax=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zmid zlow zhigh n(z) Nbin Vol_bin\n",
    "# zmid zlow zhigh n(z) Nbin Vol_bin\n",
    "NGC_nz = np.loadtxt(outpath)\n",
    "SGC_nz = np.loadtxt(outpath2)\n",
    "\n",
    "# Function to combine the two to get an overall density, appropiatly weighting each by volume\n",
    "def combine_nz(nz1, nz2):\n",
    "    combined = np.zeros((len(nz1), 6))\n",
    "    combined[:,0] = nz1[:,0]\n",
    "    combined[:,1] = nz1[:,1]\n",
    "    combined[:,2] = nz1[:,2]\n",
    "    combined[:,4] = nz1[:,4] + nz2[:,4]\n",
    "    combined[:,5] = nz1[:,5] + nz2[:,5]\n",
    "    combined[:,3] = combined[:,4] / combined[:,5]\n",
    "    \n",
    "    return combined\n",
    "\n",
    "total_nz = combine_nz(NGC_nz, SGC_nz)\n",
    "\n",
    "print(f\"BGS BRIGHT Y1 number of galaxies with M_r < {maglim} and z < {zmax}: {total_nz[:,4].sum():,}\")\n",
    "print(f\"SDSS Zehavi 2011 number of galaxies with M_r < -22 and z < 0.245169 in : 11,385 \")\n",
    "\n",
    "# Function to integrate the two across a z range\n",
    "def galdensity(zmin, zmax):\n",
    "    # Find idx for zmin and zmax\n",
    "    idx1 = np.argmin(np.abs(NGC_nz[:,0] - zmin))\n",
    "    idx2 = np.argmin(np.abs(NGC_nz[:,0] - zmax))\n",
    "\n",
    "    # NGC\n",
    "    zwidth = NGC_nz[:,2] - NGC_nz[:,1]\n",
    "    integrated = NGC_nz[:,3] * zwidth \n",
    "    summed = integrated[idx1:idx2].sum()\n",
    "\n",
    "    # And now for SGC\n",
    "    idx1 = np.argmin(np.abs(SGC_nz[:,0] - zmin))\n",
    "    idx2 = np.argmin(np.abs(SGC_nz[:,0] - zmax))\n",
    "    zwidth = SGC_nz[:,2] - SGC_nz[:,1]\n",
    "    integrated = SGC_nz[:,3] * zwidth\n",
    "    summed += integrated[idx1:idx2].sum()\n",
    "\n",
    "    return summed\n",
    "\n",
    "#galdensity(0.02, 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "#plt.plot(NGC_nz[:,0], NGC_nz[:,3], label='BGS BRIGHT NGC')\n",
    "#plt.plot(SGC_nz[:,0], SGC_nz[:,3], '--', label='BGS BRIGHT SGC')\n",
    "plt.plot(total_nz[:,0], total_nz[:,3], label='BGS BRIGHT')\n",
    "plt.xlabel('$z$')\n",
    "plt.ylabel('$n(z)~[(h/$Mpc$)^3]$')\n",
    "plt.xlim(0.01, 0.26)\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.title(\"BGS BRIGHT Y1 Density; $M_r<-22$, $z<0.245$\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Blue/Red Lum Bins Comparison\n",
    "datafolder = PARAMS_SDSS_FOLDER\n",
    "\n",
    "# BGS BRIGHT with zlims like what Jeremy did for SDSS\n",
    "zmaxes = [0.02586, 0.0406, 0.06336, 0.0981, 0.1504]\n",
    "magbins = [-17, -18, -19, -20, -21, -22]\n",
    "tracer = \"BGS_BRIGHT\" \n",
    "red_likesdss_results = []\n",
    "blue_likesdss_results = []\n",
    "for z in zmaxes:\n",
    "    red_likesdss_results.append(get_wp_for(tracer, 'pip_angular_bitwise', 'Y1', 'iron', 'v1.5pip', 'custom', z, 18, 0, 'GCcomb', True)) \n",
    "    blue_likesdss_results.append(get_wp_for(tracer, 'pip_angular_bitwise', 'Y1', 'iron', 'v1.5pip', 'custom', z, 18, 0, 'GCcomb', False))\n",
    "\n",
    "plt.figure(figsize=(10, 6.67))\n",
    "for i in range(len(zmaxes)):\n",
    "    magmin = magbins[i]\n",
    "    magmax = magbins[i+1]\n",
    "\n",
    "    if red_likesdss_results[i] is None and blue_likesdss_results[i] is None:\n",
    "        continue\n",
    "\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    if red_likesdss_results[i] is not None:\n",
    "        if np.shape(red_likesdss_results[i])[1] == 3:\n",
    "            plt.plot(red_likesdss_results[i][:,0], red_likesdss_results[i][:,2], '.', color='r', label='BGS Quiescent')\n",
    "        else:\n",
    "            plt.errorbar(red_likesdss_results[i][:,0], red_likesdss_results[i][:,2], yerr=red_likesdss_results[i][:,3], fmt='.', color='r', capsize=2)\n",
    "    if blue_likesdss_results[i] is not None:\n",
    "        if np.shape(blue_likesdss_results[i])[1] == 3:\n",
    "            plt.plot(blue_likesdss_results[i][:,0], blue_likesdss_results[i][:,2], '.', color='b', label='BGS Star-Forming')\n",
    "        else:\n",
    "            plt.errorbar(blue_likesdss_results[i][:,0], blue_likesdss_results[i][:,2], yerr=blue_likesdss_results[i][:,3], fmt='.', color='b', capsize=2)\n",
    "\n",
    "    # SDSS Data\n",
    "    # TODO think about error bars I'm plotting differently now\n",
    "    fname = datafolder + f'wp_red_M{np.abs(magmin):d}.dat'\n",
    "    wp, wp_err, radius = read_wp_file(fname)\n",
    "    if wp is not None:\n",
    "        plt.errorbar(radius, wp, yerr=wp_err, fmt='.', color='darkred', capsize=3, ecolor='k', label='SDSS Quiescent')\n",
    "\n",
    "    fname = datafolder + f'wp_blue_M{np.abs(magmin):d}.dat'\n",
    "    wp, wp_err, radius = read_wp_file(fname)\n",
    "    if wp is not None:\n",
    "        plt.errorbar(radius, wp, yerr=wp_err, fmt='.', color='darkblue', capsize=3, ecolor='k', label='SDSS Star-Forming')\n",
    "\n",
    "    plt.xscale('log')\n",
    "    plt.yscale('log')\n",
    "    plt.ylim(4, 5000)\n",
    "    plt.ylabel(r'$w_p(r_p)$')\n",
    "    plt.xlabel(r'$r_p$ [Mpc/h]') \n",
    "    plt.title(f'${magbins[i+1]}<M_r<{magbins[i]}$')\n",
    "    plt.text(.6, .92, f'$z<{zmaxes[i]}$', transform=plt.gca().transAxes, fontsize=10)\n",
    "\n",
    "# use the sixth area for a legend\n",
    "plt.subplot(2, 3, 6)\n",
    "plt.plot([], [], 'r.', label='BGS Quiescent')\n",
    "plt.plot([], [], 'b.', label='BGS Star-Forming')\n",
    "plt.plot([], [], 'darkred', label='SDSS Quiescent') \n",
    "plt.plot([], [], 'darkblue', label='SDSS Star-Forming')\n",
    "plt.legend(loc='center', fontsize=10, frameon=False)\n",
    "plt.axis('off')\n",
    "plt.subplots_adjust(left=0.05, right=0.95, top=0.9, bottom=0.1, hspace=0.3, wspace=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# Print off the quiescent fraction in each sample\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now do similar but for the luminosity threshold version which is not color split\n",
    "def get_threshold_wp(tracer, weights, survey, verspec, ver, bins, zmax, nran, njack, quiescent=None):\n",
    "    dir = os.path.join(CUSTOM_CLUSTERING_RESULTS_FOLDER, survey, 'LSS', verspec, 'LSScats', ver, 'rppi', 'v1')\n",
    "    if not os.path.exists(dir):\n",
    "        print(\"WARNING: Directory does not exist: \", dir)\n",
    "        return None\n",
    "    if quiescent is not None:\n",
    "        addon = '_QUIESCENT' + ('1' if quiescent else '0')\n",
    "    else:\n",
    "        addon = ''\n",
    "    wp_fn = os.path.join(dir, f'wp_{tracer}_GCcomb_0.02_{zmax}_{weights}_{bins}_njack{njack}_nran{nran}_split20{addon}.txt')\n",
    "    if not os.path.exists(wp_fn):\n",
    "        print(\"WARNING: File does not exist: \", wp_fn)\n",
    "        return None\n",
    "    return np.loadtxt(wp_fn)\n",
    "\n",
    "# BGS BRIGHT with zlims like Zehavi et al 2011 Luminosity threshold samples\n",
    "zmaxes = [0.041695, 0.052536, 0.064211, 0.084892, 0.106407, 0.132425, 0.15894, 0.198804, 0.245169]\n",
    "magbins = [-18, -18.5, -19, -19.5, -20, -20.5, -21, -21.5, -22.4]\n",
    "thresholds_likesdss = []\n",
    "for z in zmaxes:\n",
    "    thresholds_likesdss.append(get_threshold_wp(tracer, 'pip_angular_bitwise', 'Y1', 'iron', 'v1.5pip', 'custom', z, 18, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot them all\n",
    "plt.figure(figsize=(6, 6))\n",
    "colors = ['k', 'k', 'cyan', 'blue', 'darkgreen', 'limegreen', 'k', 'r', 'magenta']\n",
    "#for i in [8,7,6,5,3]:\n",
    "for i in [8]:\n",
    "    if thresholds_likesdss[i] is None:\n",
    "        continue\n",
    "\n",
    "    if thresholds_likesdss[i] is not None:\n",
    "        if np.shape(thresholds_likesdss[i])[1] == 3:\n",
    "            plt.plot(thresholds_likesdss[i][:,0], thresholds_likesdss[i][:,2], 'o', color=colors[i], label=f'$BGS~M_r<{magbins[i]}$')\n",
    "        else:\n",
    "            plt.errorbar(thresholds_likesdss[i][:,0], thresholds_likesdss[i][:,2], yerr=thresholds_likesdss[i][:,3], fmt='.', color=colors[i], capsize=2)\n",
    "\n",
    "\n",
    "# SDSS Data\n",
    "#sdss_magbins = [19.5, 20.5, 21, 21.5, 22]\n",
    "sdss_magbins = [22]\n",
    "colors = ['blue', 'limegreen', 'k', 'r', 'magenta']\n",
    "sdss_magbins.reverse()\n",
    "colors.reverse()\n",
    "for m in sdss_magbins:\n",
    "    # TODO\n",
    "    savedir = PARAMS_SDSS_FOLDER + f'sdss-thresh-{m:.1f}.csv'\n",
    "    if not os.path.exists(savedir):\n",
    "        print(f'File {savedir} not found')\n",
    "        continue\n",
    "    data = np.loadtxt(savedir, skiprows=1, dtype='float', delimiter=',')\n",
    "    plt.plot(data[:,0], data[:,1], '-', label=f'SDSS $M_r<-{m}$', color=colors.pop(0))\n",
    "\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.ylabel(r'$w_p(r_p)$')\n",
    "plt.xlabel(r'$r_p$ [Mpc/h]') \n",
    "plt.legend()\n",
    "plt.ylim(2, 1500)\n",
    "plt.xlim(0.1, 40)\n",
    "# Add ticks on top and bottom\n",
    "plt.gca().yaxis.set_ticks_position('both')\n",
    "plt.gca().xaxis.set_ticks_position('both')\n",
    "plt.gca().yaxis.set_tick_params(which='both', direction='in')\n",
    "plt.gca().xaxis.set_tick_params(which='both', direction='in')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "baseclone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
