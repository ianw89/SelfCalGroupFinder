{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append(os.path.join(os.getcwd(), '../desi/'))\n",
    "import numpy as np\n",
    "import emcee\n",
    "import corner\n",
    "from IPython.display import display, Latex, Math\n",
    "from copy import deepcopy\n",
    "\n",
    "if './SelfCalGroupFinder/py/' not in sys.path:\n",
    "    sys.path.append('./SelfCalGroupFinder/py/')\n",
    "from pyutils import *\n",
    "from groupcatalog import *\n",
    "import plotting as pp\n",
    "import catalog_definitions as cat\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "param_ranges = [(10,20),(1,7),(-2,30),(-5,20),(5,80),(-10,15),(-8,6),(4,80),(8,20),(-25,5)]\n",
    "\n",
    "# 10 Parameters associated with galaxy colors\n",
    "\n",
    "# A zeroth and first order polynomial in log L_gal for B_sat, which controls the satelite threshold\n",
    "# Bsat,r = β_0,r + β_L,r(log L_gal − 9.5)\n",
    "# Bsat,b = β_0,b + β_L,b(log L_gal − 9.5)\n",
    "# Constrained from projected two-point clustering comparison for r,b seperately\n",
    "\n",
    "# Weights for each galaxy luminosity, when abundance matching\n",
    "# log w_cen,r = (ω_0,r / 2) (1 + erf[(log L_gal - ω_L,r) / σ_ω,r)] ) \n",
    "# log w_cen,b = (ω_0,b / 2) (1 + erf[(log L_gal - ω_L,b) / σ_ω,b)] ) \n",
    "# Constrained from Lsat,r/Lsat,b ratio and projected two-point clustering.\n",
    "\n",
    "# A secondary, individual galaxy property can be introduced to affect the weight for abundance matching.\n",
    "#  2 Parameters (one for each red and blue)\n",
    "# w_χ,r = exp(χ/ω_χ,r)\n",
    "# w_χ,b = exp(χ/ω_χ,b)\n",
    "# Constrained from Lsat(χ|L_gal) data.\n",
    "\n",
    "# SDSS\n",
    "# Ideal Chi^2 Estimate = N_dof = N_data - N_params = 100 - 10 = 90\n",
    "\n",
    "# BGS\n",
    "# Ideal Chi^2 Estimate = N_dof = N_data - N_params = 15*2 + 30*6 + 20 - 10 = 220\n",
    "# Chi squared per dof, divide by 220\n",
    "\n",
    "labels = ['$\\\\omega_{L,b}$', '$\\\\sigma_{\\\\omega,b}$', '$\\\\omega_{L,r}$', '$\\\\sigma_{\\\\omega,r}$', '$\\\\omega_{0,b}$', '$\\\\omega_{0,r}$', '$\\\\beta_{0,r}$', '$\\\\beta_{L,r}$', '$\\\\beta_{0,b}$', '$\\\\beta_{L,b}$']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gc = deepcopy(cat.bgs_sv3_hybrid_mcmc) # 43170, 18000\n",
    "#gc = deepcopy(cat.bgs_y3_like_sv3_hybrid_mcmc_new)\n",
    "#gc = deepcopy(cat.bgs_y1mini_hybrid_mcmc)\n",
    "\n",
    "#gc = deepcopy(cat.bgs_y1_hybrid_mcmc)\n",
    "#gc = deserialize(cat.bgs_y1_hybrid8_mcmc)\n",
    "#gc = deserialize(cat.bgs_y1_hybrid8_v1_mcmc)\n",
    "gc = deepcopy(cat.bgs_y1_hybrid8_v1_mcmc)\n",
    "best = gc.load_best_params_across_runs(median_best=True)\n",
    "print(best)\n",
    "\n",
    "# 16.34605155  4.54724616 17.22001332  7.26752538 23.83699212  5.20258597 -3.85224961 17.38279755 10.71420271 -0.91545774]\n",
    "# 12.30536655,  1.67078568, 11.82548149,  1.71729165,  8.40348227,  0.94817885, -3.24537345, 16.16321933, 10.58781695, 1.26489381"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.GF_props['halomassfunc'] = HMF_T08_P18_FILE\n",
    "gc.preprocess()\n",
    "gc.run_group_finder(popmock=True, silent=False)\n",
    "gc.calc_wp_for_mock()\n",
    "gc.chisqr()\n",
    "gc.postprocess()\n",
    "gc.dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.proj_clustering_plot(gc)\n",
    "pp.lsat_data_compare_plot(gc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chains, logprob = combine_emcee_backends(gc.get_backends()[0])\n",
    "dims = chains.shape[2]\n",
    "chains_flat = chains.reshape(-1, dims)\n",
    "logprob_flat = logprob.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.gfparams_plots(gc, chains_flat)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove nans from both and ensure they are at same indexes\n",
    "valid_indices = ~np.isnan(logprob_flat) & ~np.any(np.isnan(chains_flat), axis=1)\n",
    "chains_flat = chains_flat[valid_indices]\n",
    "logprob_flat = logprob_flat[valid_indices]  \n",
    "\n",
    "print(f\"Flat chains shape: {chains_flat.shape}\")\n",
    "print(f\"Flat log probabilities shape: {logprob_flat.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Define the central 68% region for each parameter ---\n",
    "bounds = []\n",
    "for i in range(dims):\n",
    "    lower, upper = np.percentile(chains_flat[:, i], [16, 84])\n",
    "    bounds.append((lower, upper))\n",
    "\n",
    "# --- 2. Filter the chains to find points where all parameters are within their 68% CI ---\n",
    "# Start with a mask of all True\n",
    "within_bounds_mask = np.full(chains_flat.shape[0], True)\n",
    "for i in range(dims):\n",
    "    # Update the mask, keeping only points within the bounds for the current parameter\n",
    "    within_bounds_mask &= (chains_flat[:, i] >= bounds[i][0]) & (chains_flat[:, i] <= bounds[i][1])\n",
    "\n",
    "# Create the subset of chains and log probabilities\n",
    "subset_chains = chains_flat[within_bounds_mask]\n",
    "subset_logprob = logprob_flat[within_bounds_mask]\n",
    "\n",
    "print(f\"Found {len(subset_chains)} of {len(chains_flat)} within the central 68% region of all parameters.\")\n",
    "\n",
    "# --- 3. Find the best-fit parameters from this subset ---\n",
    "if len(subset_chains) > 0:\n",
    "    best_fit_idx_in_subset = np.argmax(subset_logprob)\n",
    "    best_fit_params = subset_chains[best_fit_idx_in_subset]\n",
    "else:\n",
    "    print(\"Warning: No points found in the central 68% region. Using the absolute best-fit instead.\")\n",
    "    best_fit_idx = np.argmax(logprob_flat)\n",
    "    best_fit_params = chains_flat[best_fit_idx]\n",
    "\n",
    "\n",
    "# --- 4. Generate LaTeX table rows ---\n",
    "# The median and errors are still calculated from the full distribution\n",
    "print(\"\\\\hline\")\n",
    "for i in range(dims):\n",
    "    # Calculate the 16th, 50th (median), and 84th percentiles from the full chain\n",
    "    mcmc = np.percentile(chains_flat[:, i], [16, 50, 84])\n",
    "    \n",
    "    median = mcmc[1]\n",
    "    lower_err = median - mcmc[0]\n",
    "    upper_err = mcmc[2] - median\n",
    "\n",
    "    low_val = median - lower_err\n",
    "    high_val = median + upper_err\n",
    "    \n",
    "    # Get the best-fit value found from the subset\n",
    "    best_val = best_fit_params[i]\n",
    "    \n",
    "    param_label = labels[i]\n",
    "    \n",
    "    #latex_row = f\"{param_label.ljust(20)} & ${median:.2f}_{{-{lower_err:.2f}}}^{{+{upper_err:.2f}}}$ & ${best_val:.2f}$ \\\\\\\\\"\n",
    "    latex_row = f\"{param_label.ljust(20)}   & {best_val:.2f}    &  [{low_val:.2f}, {high_val:.2f}]  \\\\\\\\\"\n",
    "    print(latex_row)\n",
    "\n",
    "print(\"\\\\hline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the best N unique models (lowest chi squared) from both the full chain and the subset\n",
    "N = 5  # Number of best models to display\n",
    "\n",
    "# --- 1. Show best models from the ENTIRE chain ---\n",
    "print(\"--- Top N Overall Models (from all chains) ---\")\n",
    "best_indices_all = np.argsort(logprob_flat)[::-1]\n",
    "\n",
    "shown = 0\n",
    "seen = set()\n",
    "for idx in best_indices_all:\n",
    "    # Convert to tuple for hashable comparison (rounded to avoid float precision issues)\n",
    "    fit_tuple = tuple(np.round(chains_flat[idx], 5))\n",
    "    if fit_tuple in seen:\n",
    "        continue\n",
    "    seen.add(fit_tuple)\n",
    "    \n",
    "    best_fit = chains_flat[idx]\n",
    "    chi_squared = -2 * logprob_flat[idx]\n",
    "    \n",
    "    with np.printoptions(precision=5, suppress=True, formatter={'all': lambda x: f\"{x:.3f},\"}, linewidth=500):\n",
    "        print(f\"Model {shown+1} (chi_sq={chi_squared:.2f}): {best_fit}\")\n",
    "\n",
    "    shown += 1\n",
    "    if shown >= N:\n",
    "        break\n",
    "\n",
    "# --- 2. Show best models from the RESTRICTED 68% SUBSET ---\n",
    "print(\"\\n--- Top N Models from 68% Central Region ---\")\n",
    "if len(subset_chains) > 0:\n",
    "    best_indices_subset = np.argsort(subset_logprob)[::-1]\n",
    "\n",
    "    shown = 0\n",
    "    seen = set()\n",
    "    for idx in best_indices_subset:\n",
    "        # Convert to tuple for hashable comparison\n",
    "        fit_tuple = tuple(np.round(subset_chains[idx], 5))\n",
    "        if fit_tuple in seen:\n",
    "            continue\n",
    "        seen.add(fit_tuple)\n",
    "        \n",
    "        best_fit = subset_chains[idx]\n",
    "        chi_squared = -2 * subset_logprob[idx]\n",
    "\n",
    "        with np.printoptions(precision=5, suppress=True, formatter={'all': lambda x: f\"{x:.3f},\"}, linewidth=500):\n",
    "            print(f\"Model {shown+1} (chi_sq={chi_squared:.2f}): {best_fit}\")\n",
    "\n",
    "        shown += 1\n",
    "        if shown >= N:\n",
    "            break\n",
    "else:\n",
    "    print(\"No models found in the subset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for reader in gc.get_backends()[0]:\n",
    "    if isinstance(reader, emcee.backends.backend.Backend):\n",
    "        samples = reader.get_chain()\n",
    "        print(f'Number of steps: {samples.shape[0] * samples.shape[1]} (total); {samples.shape[0]} (per walker), ')\n",
    "        print(f'Number of walkers: {samples.shape[1]}')\n",
    "        print(f'Number of parameters: {dims}')\n",
    "\n",
    "        try:\n",
    "            tau = reader.get_autocorr_time()\n",
    "            print(tau)\n",
    "        except:\n",
    "            print(\"Not burnt in yet\")\n",
    "\n",
    "        # Print off the current walker positions in a nice arrays\n",
    "        # One line per walker in order\n",
    "        PRINT_WALKERS = False\n",
    "        if PRINT_WALKERS:\n",
    "            with np.printoptions(precision=5, suppress=True, linewidth=500,  formatter={'all': lambda x: f\"{x:.3f},\"}):\n",
    "                current = samples[-1]\n",
    "                chisqr = -2 * reader.get_log_prob(flat=False)[-1]\n",
    "                median_chisqr = np.median(chisqr)\n",
    "                good = np.where(chisqr < median_chisqr)\n",
    "                print(np.array2string(current[good]))\n",
    "                print(chisqr[good])\n",
    "\n",
    "        burn_number = 0 # TODO choose this by inspecting the chains above. wait for convergence in all parameters\n",
    "        thin_number = 1\n",
    "        flat_samples = reader.get_chain(discard=burn_number, thin=thin_number, flat=True)\n",
    "        flat_samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(dims, figsize=(10, 2.5*dims), sharex=True)\n",
    "for i in range(dims):\n",
    "    ax = axes[i]\n",
    "    ax.plot(chains[:, :, i], alpha=0.3)\n",
    "    ax.set_xlim(0, len(samples))\n",
    "    ax.set_ylabel(labels[i])\n",
    "\n",
    "    # Set y-limits to cover 90% of the walkers locations\n",
    "    min_val = np.nanpercentile(chains[:, :, i], 5)\n",
    "    max_val = np.nanpercentile(chains[:, :, i], 95)\n",
    "    ax.set_ylim(min_val, max_val)\n",
    "    \n",
    "    # label each walker number\n",
    "    #for j in good_walkers:\n",
    "    #    ax.text(10000, samples[10000, j, i], f'{j}', color='k', fontsize=6)\n",
    "\n",
    "axes[-1].set_xlabel(\"step number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the nth walker\n",
    "n = 0  # Replace with the desired walker index\n",
    "p = 8\n",
    "walker_samples = samples[:, n, :]\n",
    "walker_chisqr = -2*reader.get_log_prob(discard=0, flat=False)[:, 0]\n",
    "\n",
    "# Create a boolean array indicating whether parameter values were updated at each step\n",
    "updated = np.any(np.diff(walker_samples, axis=0) != 0, axis=1)\n",
    "\n",
    "# Add a False at the beginning since the first step has no previous step to compare\n",
    "updated = np.insert(updated, 0, False)\n",
    "\n",
    "with np.printoptions(threshold=np.inf, linewidth=np.inf, suppress=True, formatter={'float': '{:6.1f}'.format, 'bool': '{:6}'.format}):\n",
    "    print(updated)\n",
    "    print(walker_chisqr)\n",
    "    print(walker_samples[:, p])\n",
    "    # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The corner plot shows all 1D and 2D projections of the posterior probabilities of your parameters.\n",
    "# This is useful because it quickly demonstrates all of the covariances between parameters. \n",
    "# Also, the way that you find the marginalized distribution for a parameter or set of parameters \n",
    "#   using the results of the MCMC chain is to project the samples into that plane and then make \n",
    "#   an N-dimensional histogram. \n",
    "# That means that the corner plot shows the marginalized distribution for each parameter independently \n",
    "#   in the histograms along the diagonal and then the marginalized two dimensional distributions \n",
    "#   in the other panels.\n",
    "all_flat_samples = chains.reshape(-1, dims)\n",
    "\n",
    "fig = corner.corner(all_flat_samples, labels=labels, range=param_ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then print means of the posteriors\n",
    "print(\"MEAN MODEL\")\n",
    "for i in range(10):\n",
    "    mcmc = np.percentile(flat_samples[:, i], [16, 50, 84])\n",
    "    q = np.diff(mcmc)\n",
    "    txt = f\"{labels[i]} = ${mcmc[1]:.3f}_{{-{q[0]:.3f}}}^{{{q[1]:.3f}}}$\"\n",
    "    display(Latex(txt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View Variances of fsat, LHMR from chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will save off a .npy file with the array of fsat values\n",
    "#save_from_log(PY_SRC_FOLDER + 'exec.out', overwrite=False)\n",
    "\n",
    "# TODO From Blobs to error estimates\n",
    "backends, folders = gc.get_backends()\n",
    "save_from_backend(backends, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsat_std, fsatr_std, fsatb_std, fsat_mean, fsatr_mean, fsatb_mean = fsat_variance_from_saved()\n",
    "#np.save(OUTPUT_FOLDER + 'std_fsat.npy', (fsat_std, fsatr_std, fsatb_std, fsat_mean, fsatr_mean, fsatb_mean))\n",
    "\n",
    "plt.figure()\n",
    "plt.errorbar(L_gal_bins, fsat_mean, yerr=fsat_std, fmt='.', color='k', label='All', capsize=3, alpha=0.7)\n",
    "plt.errorbar(L_gal_bins, fsatr_mean, yerr=fsatr_std, fmt='.', color='r', label='Quiescent', capsize=3, alpha=0.7)\n",
    "plt.errorbar(L_gal_bins, fsatb_mean, yerr=fsatb_std, fmt='.', color='b', label='Star-forming', capsize=3, alpha=0.7)\n",
    "plt.xlabel('$L_{\\mathrm{gal}}$')\n",
    "plt.ylabel(r'$\\langle f_{\\mathrm{sat}} \\rangle$')\n",
    "plt.legend()\n",
    "plt.xscale('log')\n",
    "plt.xlim(1E7, 2E11)\n",
    "plt.ylim(0.0, 1.0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.LHMR_from_logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsat_r_mean, lsat_r_std, lsat_b_mean, lsat_b_std = lsat_variance_from_saved()\n",
    "\n",
    "data = np.loadtxt(LSAT_OBSERVATIONS_SDSS_FILE, skiprows=0, dtype='float')\n",
    "pp.lsat_compare_plot(data, lsat_r_mean, lsat_b_mean, lsat_r_std, lsat_b_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
