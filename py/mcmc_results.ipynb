{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append(os.path.join(os.getcwd(), '../desi/'))\n",
    "import numpy as np\n",
    "import emcee\n",
    "import corner\n",
    "from IPython.display import display, Latex, Math\n",
    "from copy import deepcopy\n",
    "\n",
    "if './SelfCalGroupFinder/py/' not in sys.path:\n",
    "    sys.path.append('./SelfCalGroupFinder/py/')\n",
    "from pyutils import *\n",
    "from groupcatalog import *\n",
    "import plotting as pp\n",
    "import catalog_definitions as cat\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "param_ranges = [(10,20),(0,5),(-2,30),(-5,20),(0,35),(-10,15),(-6,6),(4,25),(8,25),(-25,5)]\n",
    "\n",
    "# 10 Parameters associated with galaxy colors\n",
    "# Multiple configurations could produce the same LHMR, fsat, etc.\n",
    "# The actual values of these parameters is not of central interest;\n",
    "# it's the implied LHMR, fsat, etc. that we really care about.\n",
    "# Thus any degeneracies in these parameters are not a concern.\n",
    "\n",
    "# A zeroth and first order polynomial in log L_gal for B_sat, which controls the satelite threshold\n",
    "# Bsat,r = β_0,r + β_L,r(log L_gal − 9.5)\n",
    "# Bsat,b = β_0,b + β_L,b(log L_gal − 9.5)\n",
    "# Constrained from projected two-point clustering comparison for r,b seperately\n",
    "\n",
    "# Weights for each galaxy luminosity, when abundance matching\n",
    "# log w_cen,r = (ω_0,r / 2) (1 + erf[(log L_gal - ω_L,r) / σ_ω,r)] ) \n",
    "# log w_cen,b = (ω_0,b / 2) (1 + erf[(log L_gal - ω_L,b) / σ_ω,b)] ) \n",
    "# Constrained from Lsat,r/Lsat,b ratio and projected two-point clustering.\n",
    "\n",
    "# A secondary, individual galaxy property can be introduced to affect the weight for abundance matching.\n",
    "#  2 Parameters (one for each red and blue)\n",
    "# w_χ,r = exp(χ/ω_χ,r)\n",
    "# w_χ,b = exp(χ/ω_χ,b)\n",
    "# Constrained from Lsat(χ|L_gal) data.\n",
    "\n",
    "# SDSS\n",
    "# Ideal Chi^2 Estimate = N_dof = N_data - N_params = 100 - 10 = 90\n",
    "\n",
    "# BGS\n",
    "# Ideal Chi^2 Estimate = N_dof = N_data - N_params = 30*6 + 20 - 10 = 190\n",
    "# Chi squared per dof, divide by 200\n",
    "\n",
    "# BGS Mini\n",
    "# Ideal Chi^2 Estimate = N_dof = N_data - N_params = 20*3 + 10*3 + 20 - 10 = 100\n",
    "# Chi squared per dof, divide by 110\n",
    "\n",
    "\n",
    "#Job ID                    Name             User            Time Use S Queue\n",
    "#------------------------- ---------------- --------------- -------- - -----\n",
    "#15016.master.local         ian.optuna0      imw2293         415:05:3 R default        (TPE)\n",
    "#15017.master.local         ian.optuna1      imw2293         70:22:47 R default        (Q MC)\n",
    "#15018.master.local         ian.optuna2      imw2293         404:17:4 R default        (GP)\n",
    "#15019.master.local         ian.emcee3       imw2293         72:43:09 R default        (emcee Stretch)\n",
    "\n",
    "labels = ['$\\\\omega_{L,b}$', '$\\\\sigma_{\\\\omega,b}$', '$\\\\omega_{L,r}$', '$\\\\sigma_{\\\\omega,r}$', '$\\\\omega_{0,b}$', '$\\\\omega_{0,r}$', '$\\\\beta_{0,r}$', '$\\\\beta_{L,r}$', '$\\\\beta_{0,b}$', '$\\\\beta_{L,b}$']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gc = deepcopy(cat.bgs_sv3_hybrid_mcmc) # 43170, 18000\n",
    "#gc = deepcopy(cat.bgs_y3_like_sv3_hybrid_mcmc_new)\n",
    "#gc = deepcopy(cat.bgs_y1mini_hybrid_mcmc)\n",
    "\n",
    "#gc = deepcopy(cat.bgs_y1_hybrid_mcmc)\n",
    "gc = deserialize(cat.bgs_y1_hybrid8_mcmc)\n",
    "best = gc.load_best_params_across_runs()\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.preprocess()\n",
    "gc.run_group_finder(popmock=True, silent=False)\n",
    "gc.calc_wp_for_mock()\n",
    "gc.chisqr()\n",
    "gc.postprocess()\n",
    "gc.dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.proj_clustering_plot(gc)\n",
    "pp.lsat_data_compare_plot(gc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO fsat with saved variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO LHMR with saved variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.LHMR_withscatter(gc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.single_plots(gc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.hod_plot(gc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_emcee_backends(backends):\n",
    "    \"\"\"\n",
    "    Combine multiple emcee backends into a single set of chains.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    backends : list of emcee.backends.backend.Backend\n",
    "        List of emcee backends to combine.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    combined_samples : np.ndarray\n",
    "        Combined chains of shape (nsteps_total, nwalkers, ndim)\n",
    "    \"\"\"\n",
    "    chains = [b.get_chain() for b in backends]\n",
    "    shapes = [c.shape for c in chains]\n",
    "    print(f\"Shapes: {shapes}\")\n",
    "    to_drop = np.full((len(chains),), False)\n",
    "    walkers = 0\n",
    "    dims = chains[0].shape[2] \n",
    "\n",
    "    longest_steps = max(shape[0] for shape in shapes)\n",
    "    print(f\"Longest chain has {longest_steps} steps.\")\n",
    "    # Pad shorter chains with NaNs to match the longest chain length\n",
    "    for i in range(len(chains)):\n",
    "        if shapes[i][0] < longest_steps:\n",
    "            pad_length = longest_steps - shapes[i][0]\n",
    "            if pad_length > 0:\n",
    "                if pad_length > shapes[i][0]:\n",
    "                    print(f\"Chain {i} is too short ({shapes[i][0]} steps), dropping it.\")\n",
    "                    to_drop[i] = True\n",
    "                else:\n",
    "                    print(f\"Padding chain {i} with {pad_length} NaN steps to match the longest chain length.\")\n",
    "                chains[i] = np.pad(chains[i], ((0, pad_length), (0, 0), (0, 0)), mode='constant', constant_values=np.nan)\n",
    "\n",
    "    for i in range(len(chains)):\n",
    "        if not to_drop[i]:\n",
    "            walkers += shapes[i][1]\n",
    "\n",
    "    combined = np.full((longest_steps, walkers, dims), np.nan)\n",
    "    print(f\"Combined shape will be: {combined.shape}\")\n",
    "\n",
    "    # Fill the combined array with the chains, skipping those marked for dropping\n",
    "    walker_index = 0\n",
    "    for i in range(len(chains)):\n",
    "        if not to_drop[i]:\n",
    "            nwalkers = chains[i].shape[1]\n",
    "            combined[:, walker_index:walker_index + nwalkers, :] = chains[i]\n",
    "            walker_index += nwalkers\n",
    "    \n",
    "    return combined\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chains = combine_emcee_backends(gc.get_backends()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for reader in gc.get_backends()[0]:\n",
    "    if isinstance(reader, emcee.backends.backend.Backend):\n",
    "        samples = reader.get_chain()\n",
    "        ndim = reader.shape[1]\n",
    "        print(f'Number of steps: {samples.shape[0] * samples.shape[1]} (total); {samples.shape[0]} (per walker), ')\n",
    "        print(f'Number of walkers: {samples.shape[1]}')\n",
    "        print(f'Number of parameters: {ndim}')\n",
    "\n",
    "        try:\n",
    "            tau = reader.get_autocorr_time()\n",
    "            print(tau)\n",
    "        except:\n",
    "            print(\"Not burnt in yet\")\n",
    "\n",
    "        # Print off the current walker positions in a nice arrays\n",
    "        # One line per walker in order\n",
    "        PRINT_WALKERS = False\n",
    "        if PRINT_WALKERS:\n",
    "            with np.printoptions(precision=5, suppress=True, linewidth=500,  formatter={'all': lambda x: f\"{x:.3f},\"}):\n",
    "                current = samples[-1]\n",
    "                chisqr = -2 * reader.get_log_prob(flat=False)[-1]\n",
    "                median_chisqr = np.median(chisqr)\n",
    "                good = np.where(chisqr < median_chisqr)\n",
    "                print(np.array2string(current[good]))\n",
    "                print(chisqr[good])\n",
    "\n",
    "        burn_number = 0 # TODO choose this by inspecting the chains above. wait for convergence in all parameters\n",
    "        thin_number = 1\n",
    "        flat_samples = reader.get_chain(discard=burn_number, thin=thin_number, flat=True)\n",
    "        flat_samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ndim, figsize=(10, 2.5*ndim), sharex=True)\n",
    "for i in range(ndim):\n",
    "    ax = axes[i]\n",
    "    ax.plot(chains[:, :, i], alpha=0.3)\n",
    "    ax.set_xlim(0, len(samples))\n",
    "    ax.set_ylabel(labels[i])\n",
    "\n",
    "    # Set y-limits to cover 90% of the walkers locations\n",
    "    min_val = np.nanpercentile(chains[:, :, i], 5)\n",
    "    max_val = np.nanpercentile(chains[:, :, i], 95)\n",
    "    ax.set_ylim(min_val, max_val)\n",
    "    \n",
    "    # label each walker number\n",
    "    #for j in good_walkers:\n",
    "    #    ax.text(10000, samples[10000, j, i], f'{j}', color='k', fontsize=6)\n",
    "\n",
    "axes[-1].set_xlabel(\"step number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the nth walker\n",
    "n = 0  # Replace with the desired walker index\n",
    "p = 8\n",
    "walker_samples = samples[:, n, :]\n",
    "walker_chisqr = -2*reader.get_log_prob(discard=0, flat=False)[:, 0]\n",
    "\n",
    "# Create a boolean array indicating whether parameter values were updated at each step\n",
    "updated = np.any(np.diff(walker_samples, axis=0) != 0, axis=1)\n",
    "\n",
    "# Add a False at the beginning since the first step has no previous step to compare\n",
    "updated = np.insert(updated, 0, False)\n",
    "\n",
    "with np.printoptions(threshold=np.inf, linewidth=np.inf, suppress=True, formatter={'float': '{:6.1f}'.format, 'bool': '{:6}'.format}):\n",
    "    print(updated)\n",
    "    print(walker_chisqr)\n",
    "    print(walker_samples[:, p])\n",
    "    # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The corner plot shows all 1D and 2D projections of the posterior probabilities of your parameters.\n",
    "# This is useful because it quickly demonstrates all of the covariances between parameters. \n",
    "# Also, the way that you find the marginalized distribution for a parameter or set of parameters \n",
    "#   using the results of the MCMC chain is to project the samples into that plane and then make \n",
    "#   an N-dimensional histogram. \n",
    "# That means that the corner plot shows the marginalized distribution for each parameter independently \n",
    "#   in the histograms along the diagonal and then the marginalized two dimensional distributions \n",
    "#   in the other panels.\n",
    "all_flat_samples = chains.reshape(-1, ndim)\n",
    "\n",
    "#fig = corner.corner(all_flat_samples, labels=labels, range=param_ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the best N unique models (lowest chi squared)\n",
    "N = 3  # Number of best models to display\n",
    "log_probs = reader.get_log_prob(flat=True)\n",
    "all_flat_samples = reader.get_chain(flat=True)\n",
    "\n",
    "# Get indices of the top models with the highest log probabilities\n",
    "best_indices = np.argsort(log_probs)[::-1]\n",
    "\n",
    "shown = 0\n",
    "seen = set()\n",
    "for idx in best_indices:\n",
    "    # Convert to tuple for hashable comparison (rounded to avoid float precision issues)\n",
    "    fit_tuple = tuple(np.round(all_flat_samples[idx], 5))\n",
    "    if fit_tuple in seen:\n",
    "        continue\n",
    "    seen.add(fit_tuple)\n",
    "    best_fit = all_flat_samples[idx]\n",
    "    chi_squared = -2 * log_probs[idx]\n",
    "    print(f\"BEST MODEL {shown+1} (chi={chi_squared:.3f})\")\n",
    "\n",
    "    with np.printoptions(precision=5, suppress=True, formatter={'all': lambda x: f\"{x:.3f},\"}, linewidth=500):\n",
    "        print(best_fit)\n",
    "        pp.plot_parameters(best_fit)\n",
    "    #for i in range(len(labels)):\n",
    "    #    display(Latex(f'{labels[i]} = {best_fit[i]:.3f}'))\n",
    "    #print()\n",
    "    \n",
    "    shown += 1\n",
    "    if shown >= N:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then print means of the posteriors\n",
    "print(\"MEAN MODEL\")\n",
    "for i in range(ndim):\n",
    "    mcmc = np.percentile(flat_samples[:, i], [16, 50, 84])\n",
    "    q = np.diff(mcmc)\n",
    "    txt = f\"{labels[i]} = ${mcmc[1]:.3f}_{{-{q[0]:.3f}}}^{{{q[1]:.3f}}}$\"\n",
    "    display(Latex(txt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View Variances of fsat, LHMR from chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will save off a .npy file with the array of fsat values\n",
    "#save_from_log(PY_SRC_FOLDER + 'exec.out', overwrite=False)\n",
    "\n",
    "# TODO From Blobs to error estimates\n",
    "backends, folders = gc.get_backends()\n",
    "save_from_backend(backends, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsat_std, fsatr_std, fsatb_std, fsat_mean, fsatr_mean, fsatb_mean = fsat_variance_from_saved()\n",
    "#np.save(OUTPUT_FOLDER + 'std_fsat.npy', (fsat_std, fsatr_std, fsatb_std, fsat_mean, fsatr_mean, fsatb_mean))\n",
    "\n",
    "plt.figure()\n",
    "plt.errorbar(L_gal_bins, fsat_mean, yerr=fsat_std, fmt='.', color='k', label='All', capsize=3, alpha=0.7)\n",
    "plt.errorbar(L_gal_bins, fsatr_mean, yerr=fsatr_std, fmt='.', color='r', label='Quiescent', capsize=3, alpha=0.7)\n",
    "plt.errorbar(L_gal_bins, fsatb_mean, yerr=fsatb_std, fmt='.', color='b', label='Star-forming', capsize=3, alpha=0.7)\n",
    "plt.xlabel('$L_{\\mathrm{gal}}$')\n",
    "plt.ylabel(r'$\\langle f_{\\mathrm{sat}} \\rangle$')\n",
    "plt.legend()\n",
    "plt.xscale('log')\n",
    "plt.xlim(1E7, 2E11)\n",
    "plt.ylim(0.0, 1.0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lhmr_r_mean, lhmr_r_std, lhmr_r_scatter_mean, lhmr_r_scatter_std, lhmr_b_mean, lhmr_b_std, lhmr_b_scatter_mean, lhmr_b_scatter_std, lhmr_all_mean, lhmr_all_std, lhmr_all_scatter_mean, lhmr_all_scatter_std = lhmr_variance_from_saved()\n",
    "\n",
    "plt.figure()\n",
    "plt.errorbar(Mhalo_bins, lhmr_all_mean, yerr=lhmr_all_std, fmt='.', color='k', label='All', capsize=3, alpha=0.7)\n",
    "plt.errorbar(Mhalo_bins, lhmr_b_mean, yerr=lhmr_b_std, fmt='.', color='b', label='Star-forming', capsize=3, alpha=0.7)\n",
    "plt.errorbar(Mhalo_bins, lhmr_r_mean, yerr=lhmr_r_std, fmt='.', color='r', label='Quiescent', capsize=3, alpha=0.7)\n",
    "plt.xlabel('$log_{10}(M_{halo}~[M_\\\\odot]$')\n",
    "plt.ylabel(r'$\\langle L_{\\mathrm{cen}} \\rangle$')\n",
    "plt.title(\"Mean Central Luminosity vs. Halo Mass\")\n",
    "plt.legend()\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlim(1E10, 1E15)\n",
    "plt.ylim(1E7, 5E11)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.errorbar(Mhalo_bins, lhmr_all_scatter_mean, yerr=lhmr_all_scatter_std, fmt='.', color='k', label='All', capsize=3, alpha=0.7)\n",
    "plt.errorbar(Mhalo_bins, lhmr_b_scatter_mean, yerr=lhmr_b_scatter_std, fmt='.', color='b', label='Star-forming', capsize=3, alpha=0.7)\n",
    "plt.errorbar(Mhalo_bins, lhmr_r_scatter_mean, yerr=lhmr_r_scatter_std, fmt='.', color='r', label='Quiescent', capsize=3, alpha=0.7)\n",
    "plt.xlabel('$log_{10}(M_{halo}~[M_\\\\odot]$')\n",
    "plt.ylabel(r'$\\sigma_{{\\mathrm{log}}(L_{\\mathrm{cen}})}~$[dex]')\n",
    "plt.title(\"Central Luminosity Scatter vs. Halo Mass\")\n",
    "plt.legend()\n",
    "plt.xscale('log')\n",
    "plt.xlim(1E10, 1E15)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsat_r_mean, lsat_r_std, lsat_b_mean, lsat_b_std = lsat_variance_from_saved()\n",
    "\n",
    "data = np.loadtxt(LSAT_OBSERVATIONS_SDSS_FILE, skiprows=0, dtype='float')\n",
    "pp.lsat_compare_plot(data, lsat_r_mean, lsat_b_mean, lsat_r_std, lsat_b_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
