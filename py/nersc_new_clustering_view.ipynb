{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import fitsio\n",
    "from pycorr import TwoPointCorrelationFunction, TwoPointEstimator, project_to_multipoles, project_to_wp, utils, setup_logging\n",
    "from scipy.optimize import curve_fit\n",
    "from LSS.common_tools import mknz\n",
    "from astropy.table import Table\n",
    "import itertools\n",
    "\n",
    "from dataloc import *\n",
    "\n",
    "# MAKE ALL PLOTS TEXT BIGGER\n",
    "plt.rcParams.update({'font.size': 15})\n",
    "# But legend a bit smaller\n",
    "plt.rcParams.update({'legend.fontsize': 12})\n",
    "# Set DPI up a bit\n",
    "plt.rcParams.update({'figure.dpi': 150})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the prepared data\n",
    "#dir = os.path.join(CUSTOM_CLUSTERING_RESULTS_FOLDER, 'Y1', 'LSS', 'iron', 'LSScats', 'v1.5pip')\n",
    "dir = os.path.join(CUSTOM_CLUSTERING_RESULTS_FOLDER_NEW, 'Y1', 'LSS', 'iron', 'LSScats', 'v1.5pip', '19-20_Q_i1')\n",
    "tbl1 = Table(fitsio.read(os.path.join(dir, 'BGS_BRIGHT_clustering.dat.fits')))\n",
    "tbl2 = Table(fitsio.read(os.path.join(dir, 'BGS_BRIGHT_0_clustering.ran.fits')))\n",
    "print(len(tbl1), len(tbl2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tbl1.columns)\n",
    "print(tbl2.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_wp(weights, survey, verspec, ver, zmax, nran, njack, region, mag: int, imagingver, quiescent=None):\n",
    "    if quiescent:\n",
    "        folder = f'{np.abs(mag)}-{np.abs(mag)+1}_Q_{imagingver}'\n",
    "    elif quiescent is None:\n",
    "        folder = f'{np.abs(mag)}-{np.abs(mag)+1}_{imagingver}'\n",
    "    else:\n",
    "        folder = f'{np.abs(mag)}-{np.abs(mag)+1}_SF_{imagingver}'\n",
    "\n",
    "    dir = os.path.join(CUSTOM_CLUSTERING_RESULTS_FOLDER_NEW, survey, 'LSS', verspec, 'LSScats', ver, folder, 'rppi')\n",
    "\n",
    "    if not os.path.exists(dir):\n",
    "        print(\"WARNING: Directory does not exist: \", dir)\n",
    "        return None, None\n",
    "        \n",
    "    wp_fn = os.path.join(dir, f'wp_BGS_BRIGHT_{region}_0.001_{zmax}_{weights}_custom_njack{njack}_nran{nran}_split20.txt')\n",
    "\n",
    "    if not os.path.exists(wp_fn):\n",
    "        print(\"WARNING: File does not exist: \", wp_fn)\n",
    "        return None, None\n",
    "\n",
    "    try:\n",
    "        allcounts_fn = os.path.join(dir, f'allcounts_BGS_BRIGHT_{region}_0.001_{zmax}_{weights}_custom_njack{njack}_nran{nran}_split20.npy')\n",
    "        tpc = TwoPointEstimator.load(allcounts_fn)\n",
    "        s, xiell, cov = tpc.get_corr(return_sep=True, return_cov=True, mode='wp')\n",
    "    except Exception as e:\n",
    "        print(\"WARNING: Could not load TwoPointEstimator\")\n",
    "        cov = None\n",
    "\n",
    "    return np.loadtxt(wp_fn), cov\n",
    "\n",
    "def get_wp_for(tracer, weights, survey, verspec, ver, bins, zmax, nran, njack, region, quiescent=None):\n",
    "    dir = os.path.join(CUSTOM_CLUSTERING_RESULTS_FOLDER, survey, 'LSS', verspec, 'LSScats', ver, 'rppi')\n",
    "\n",
    "    if not os.path.exists(dir):\n",
    "        print(\"WARNING: Directory does not exist: \", dir)\n",
    "        return None\n",
    "\n",
    "    if quiescent is not None:\n",
    "        addon = '_QUIESCENT' + ('1' if quiescent else '0')\n",
    "    else:\n",
    "        addon = ''\n",
    "        \n",
    "    wp_fn = os.path.join(dir, f'wp_{tracer}_{region}_0.001_{zmax}_{weights}_{bins}_njack{njack}_nran{nran}_split20{addon}.txt')\n",
    "\n",
    "    if not os.path.exists(wp_fn):\n",
    "        print(\"WARNING: File does not exist: \", wp_fn)\n",
    "        return None\n",
    "\n",
    "    return np.loadtxt(wp_fn)\n",
    "\n",
    "def get_fn_for(weights, survey, verspec, ver, zmax, nran, njack, region, quiescent=None):\n",
    "    dir = os.path.join(CUSTOM_CLUSTERING_RESULTS_FOLDER, survey, 'LSS', verspec, 'LSScats', ver, 'rppi')\n",
    "\n",
    "    if not os.path.exists(dir):\n",
    "        print(\"WARNING: Directory does not exist: \", dir)\n",
    "        return None\n",
    "\n",
    "    if quiescent is not None:\n",
    "        addon = '_QUIESCENT' + ('1' if quiescent else '0')\n",
    "    else:\n",
    "        addon = ''\n",
    "        \n",
    "    allcounts_fn = os.path.join(dir, f'allcounts_BGS_BRIGHT_{region}_0.001_{zmax}_{weights}_custom_njack{njack}_nran{nran}_split20{addon}.npy')\n",
    "    return allcounts_fn\n",
    "\n",
    "\n",
    "def plot_wp(red_results, blue_results, all_results, zmaxes, magbins):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for i in range(len(red_results)):\n",
    "        plt.subplot(3, 3, i+1)\n",
    "        red_wp, red_cov = red_results[i]\n",
    "        blue_wp, blue_cov = blue_results[i]\n",
    "        #all_wp, all_cov = all_results[i]\n",
    "\n",
    "        plt.xscale('log')\n",
    "        plt.yscale('log')\n",
    "        plt.ylim(2, 4000)\n",
    "        plt.ylabel(r'$w_p(r_p)$')\n",
    "        plt.xlabel(r'$r_p$ [Mpc/h]')\n",
    "        plt.title(f'${magbins[i]} > M_r > {magbins[i+1]}$')\n",
    "        plt.text(0.75, 0.9, f'$z<{zmaxes[i]}$', transform=plt.gca().transAxes, ha='center', va='center', fontsize=10)\n",
    "\n",
    "        if red_wp is not None:\n",
    "            if red_wp.shape[1] > 3:\n",
    "                plt.errorbar(red_wp[:,0], red_wp[:,2], yerr=red_wp[:,3], label=f'Quiescent {zmaxes[i]}', fmt='r.', capsize=3)\n",
    "            else:\n",
    "                plt.plot(red_wp[:,0], red_wp[:,2], label=f'Quiescent {zmaxes[i]}', color='r')\n",
    "        if blue_wp is not None:\n",
    "            if blue_wp.shape[1] > 3:\n",
    "                plt.errorbar(blue_wp[:,0], blue_wp[:,2], yerr=blue_wp[:,3], label=f'Star-Forming {zmaxes[i]}', fmt='b.', capsize=3)\n",
    "            else:\n",
    "                plt.plot(blue_wp[:,0], blue_wp[:,2], label=f'Star-Forming {zmaxes[i]}', color='b')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Directory does not exist:  /global/cfs/cdirs/desi/users/ianw89/newclustering/Y1/LSS/iron/LSScats/v1.5pip/15-16_Q_i1/rppi\n",
      "WARNING: Directory does not exist:  /global/cfs/cdirs/desi/users/ianw89/newclustering/Y1/LSS/iron/LSScats/v1.5pip/15-16_SF_i1/rppi\n",
      "WARNING: Directory does not exist:  /global/cfs/cdirs/desi/users/ianw89/newclustering/Y1/LSS/iron/LSScats/v1.5pip/16-17_Q_i1/rppi\n",
      "WARNING: Directory does not exist:  /global/cfs/cdirs/desi/users/ianw89/newclustering/Y1/LSS/iron/LSScats/v1.5pip/16-17_SF_i1/rppi\n",
      "WARNING: Directory does not exist:  /global/cfs/cdirs/desi/users/ianw89/newclustering/Y1/LSS/iron/LSScats/v1.5pip/17-18_Q_i1/rppi\n",
      "WARNING: Directory does not exist:  /global/cfs/cdirs/desi/users/ianw89/newclustering/Y1/LSS/iron/LSScats/v1.5pip/17-18_SF_i1/rppi\n",
      "WARNING: Directory does not exist:  /global/cfs/cdirs/desi/users/ianw89/newclustering/Y1/LSS/iron/LSScats/v1.5pip/18-19_Q_i1/rppi\n",
      "WARNING: Directory does not exist:  /global/cfs/cdirs/desi/users/ianw89/newclustering/Y1/LSS/iron/LSScats/v1.5pip/18-19_SF_i1/rppi\n",
      "WARNING: Could not load TwoPointEstimator\n",
      "WARNING: Could not load TwoPointEstimator\n",
      "WARNING: Could not load TwoPointEstimator\n",
      "WARNING: Could not load TwoPointEstimator\n",
      "WARNING: Could not load TwoPointEstimator\n",
      "WARNING: Could not load TwoPointEstimator\n",
      "WARNING: Directory does not exist:  /global/cfs/cdirs/desi/users/ianw89/newclustering/Y1/LSS/iron/LSScats/v1.5pip/22-23_Q_i1/rppi\n",
      "WARNING: Directory does not exist:  /global/cfs/cdirs/desi/users/ianw89/newclustering/Y1/LSS/iron/LSScats/v1.5pip/22-23_SF_i1/rppi\n",
      "WARNING: Directory does not exist:  /global/cfs/cdirs/desi/users/ianw89/newclustering/Y1/LSS/iron/LSScats/v1.5pip/15-16_Q_i0/rppi\n",
      "WARNING: Directory does not exist:  /global/cfs/cdirs/desi/users/ianw89/newclustering/Y1/LSS/iron/LSScats/v1.5pip/15-16_SF_i0/rppi\n",
      "WARNING: Directory does not exist:  /global/cfs/cdirs/desi/users/ianw89/newclustering/Y1/LSS/iron/LSScats/v1.5pip/16-17_Q_i0/rppi\n",
      "WARNING: Directory does not exist:  /global/cfs/cdirs/desi/users/ianw89/newclustering/Y1/LSS/iron/LSScats/v1.5pip/16-17_SF_i0/rppi\n",
      "WARNING: Directory does not exist:  /global/cfs/cdirs/desi/users/ianw89/newclustering/Y1/LSS/iron/LSScats/v1.5pip/17-18_Q_i0/rppi\n",
      "WARNING: Directory does not exist:  /global/cfs/cdirs/desi/users/ianw89/newclustering/Y1/LSS/iron/LSScats/v1.5pip/17-18_SF_i0/rppi\n",
      "WARNING: Directory does not exist:  /global/cfs/cdirs/desi/users/ianw89/newclustering/Y1/LSS/iron/LSScats/v1.5pip/18-19_Q_i0/rppi\n",
      "WARNING: Directory does not exist:  /global/cfs/cdirs/desi/users/ianw89/newclustering/Y1/LSS/iron/LSScats/v1.5pip/18-19_SF_i0/rppi\n",
      "WARNING: Could not load TwoPointEstimator\n",
      "WARNING: Could not load TwoPointEstimator\n",
      "WARNING: Could not load TwoPointEstimator\n",
      "WARNING: Could not load TwoPointEstimator\n",
      "WARNING: Could not load TwoPointEstimator\n",
      "WARNING: Could not load TwoPointEstimator\n",
      "WARNING: Directory does not exist:  /global/cfs/cdirs/desi/users/ianw89/newclustering/Y1/LSS/iron/LSScats/v1.5pip/22-23_Q_i0/rppi\n",
      "WARNING: Directory does not exist:  /global/cfs/cdirs/desi/users/ianw89/newclustering/Y1/LSS/iron/LSScats/v1.5pip/22-23_SF_i0/rppi\n"
     ]
    }
   ],
   "source": [
    "zmaxes = [0.02595, 0.04067, 0.06336, 0.09792, 0.14977, 0.22620, 0.33694, 0.49523] \n",
    "magbins = [-15, -16, -17, -18, -19, -20, -21, -22, -23]\n",
    "tracer = \"BGS_BRIGHT\" \n",
    "jack_official = 0\n",
    "red_results_i1 = []\n",
    "blue_results_i1 = []\n",
    "main_all_results = []\n",
    "for m, z in zip(magbins[:-1], zmaxes):\n",
    "    for q in [True, False]:\n",
    "        wp, cov = get_new_wp('pip_angular_bitwise', 'Y1', 'iron', 'v1.5pip', z, 9, jack_official, 'GCcomb', m, \"i1\", quiescent=q)\n",
    "\n",
    "        if q is True:\n",
    "            red_results_i1.append((wp, cov))\n",
    "        elif q is False:\n",
    "            blue_results_i1.append((wp, cov))\n",
    "        else:\n",
    "            main_all_results.append((wp, cov))\n",
    "\n",
    "red_result_i0 = []\n",
    "blue_results_i0 = []\n",
    "main_all_results_i0 = []\n",
    "for m, z in zip(magbins[:-1], zmaxes):\n",
    "    for q in [True, False]:\n",
    "        wp, cov = get_new_wp('pip_angular_bitwise', 'Y1', 'iron', 'v1.5pip', z, 9, jack_official, 'GCcomb', m, \"i0\", quiescent=q)\n",
    "\n",
    "        if q is True:\n",
    "            red_result_i0.append((wp, cov))\n",
    "        elif q is False:\n",
    "            blue_results_i0.append((wp, cov))\n",
    "        else:\n",
    "            main_all_results_i0.append((wp, cov))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wp(red_results_i1, blue_results_i1, main_all_results, zmaxes, magbins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: File does not exist:  /global/cfs/cdirs/desi/users/ianw89/clustering/Y1/LSS/iron/LSScats/v1.5pip/rppi/wp_BGS_BRIGHT_GCcomb_0.001_0.02595_pip_angular_bitwise_custom_njack128_nran18_split20_QUIESCENT1.txt\n",
      "WARNING: File does not exist:  /global/cfs/cdirs/desi/users/ianw89/clustering/Y1/LSS/iron/LSScats/v1.5pip/rppi/wp_BGS_BRIGHT_GCcomb_0.001_0.02595_pip_angular_bitwise_custom_njack128_nran18_split20_QUIESCENT0.txt\n",
      "WARNING: File does not exist:  /global/cfs/cdirs/desi/users/ianw89/clustering/Y1/LSS/iron/LSScats/v1.5pip/rppi/wp_BGS_BRIGHT_GCcomb_0.001_0.04067_pip_angular_bitwise_custom_njack128_nran18_split20_QUIESCENT1.txt\n",
      "WARNING: File does not exist:  /global/cfs/cdirs/desi/users/ianw89/clustering/Y1/LSS/iron/LSScats/v1.5pip/rppi/wp_BGS_BRIGHT_GCcomb_0.001_0.06336_pip_angular_bitwise_custom_njack128_nran18_split20.txt\n",
      "WARNING: File does not exist:  /global/cfs/cdirs/desi/users/ianw89/clustering/Y1/LSS/iron/LSScats/v1.5pip/rppi/wp_BGS_BRIGHT_GCcomb_0.001_0.09792_pip_angular_bitwise_custom_njack128_nran18_split20.txt\n",
      "WARNING: File does not exist:  /global/cfs/cdirs/desi/users/ianw89/clustering/Y1/LSS/iron/LSScats/v1.5pip/rppi/wp_BGS_BRIGHT_GCcomb_0.001_0.14977_pip_angular_bitwise_custom_njack128_nran18_split20.txt\n",
      "WARNING: File does not exist:  /global/cfs/cdirs/desi/users/ianw89/clustering/Y1/LSS/iron/LSScats/v1.5pip/rppi/wp_BGS_BRIGHT_GCcomb_0.001_0.2262_pip_angular_bitwise_custom_njack128_nran18_split20.txt\n",
      "WARNING: File does not exist:  /global/cfs/cdirs/desi/users/ianw89/clustering/Y1/LSS/iron/LSScats/v1.5pip/rppi/wp_BGS_BRIGHT_GCcomb_0.001_0.33694_pip_angular_bitwise_custom_njack128_nran18_split20.txt\n",
      "WARNING: File does not exist:  /global/cfs/cdirs/desi/users/ianw89/clustering/Y1/LSS/iron/LSScats/v1.5pip/rppi/wp_BGS_BRIGHT_GCcomb_0.001_0.49523_pip_angular_bitwise_custom_njack128_nran18_split20.txt\n"
     ]
    }
   ],
   "source": [
    "# Load the equivalent old results from the other NERSC clustering view notebook\n",
    "# Assuming the old results are stored in a similar directory structure but under CUSTOM_CLUSTERING_RESULTS_FOLDER\n",
    "zmaxes = [0.02595, 0.04067, 0.06336, 0.09792, 0.14977, 0.22620, 0.33694, 0.49523] \n",
    "magbins = [-15, -16, -17, -18, -19, -20, -21, -22, -23]\n",
    "tracer = \"BGS_BRIGHT\" \n",
    "jack_official = 128\n",
    "old_red_results = []\n",
    "old_blue_results = []\n",
    "old_all_results = []\n",
    "for z, q in itertools.product(zmaxes, [True, False, None]):\n",
    "\n",
    "    wp = get_wp_for(tracer, 'pip_angular_bitwise', 'Y1', 'iron', 'v1.5pip', 'custom', z, 18, jack_official, 'GCcomb', quiescent=q)\n",
    "    cov = None\n",
    "    if wp is not None:\n",
    "        savedir = get_fn_for('pip_angular_bitwise', 'Y1', 'iron', 'v1.5pip', z, 18, jack_official, 'GCcomb', quiescent=q)\n",
    "        tpc = TwoPointEstimator.load(savedir)\n",
    "        s, xiell, cov = tpc.get_corr(return_sep=True, return_cov=True, mode='wp')\n",
    "\n",
    "    if q is True:\n",
    "        old_red_results.append((wp, cov))\n",
    "    elif q is False:\n",
    "        old_blue_results.append((wp, cov))\n",
    "    else:\n",
    "        old_all_results.append((wp, cov))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_new_vs_old_comparison(new_results, new_i0_results, old_results, zmaxes, magbins, title_suffix, color='r'):\n",
    "    \"\"\"\n",
    "    Plot comparison between new and old clustering results\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    new_results : list\n",
    "        List of (wp, cov) tuples for new results\n",
    "    new_i0_results : list  \n",
    "        List of (wp, cov) tuples for new i0 results\n",
    "    old_results : list\n",
    "        List of (wp, cov) tuples for old results\n",
    "    zmaxes : list\n",
    "        Redshift limits for each magnitude bin\n",
    "    magbins : list\n",
    "        Magnitude bin edges\n",
    "    title_suffix : str\n",
    "        Suffix to add to plot title (e.g., \"Red Galaxies\", \"Blue Galaxies\")\n",
    "    color : str\n",
    "        Color for new results plots\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for i in range(len(old_results)):\n",
    "        plt.subplot(3, 3, i+1)\n",
    "        new_wp, _ = new_results[i]\n",
    "        new_i0, _ = new_i0_results[i] if new_i0_results else (None, None)\n",
    "        old_wp, _ = old_results[i]\n",
    "\n",
    "        plt.xscale('log')\n",
    "        plt.yscale('log')\n",
    "        plt.ylim(2, 4000)\n",
    "        plt.ylabel(r'$w_p(r_p)$')\n",
    "        plt.xlabel(r'$r_p$ [Mpc/h]')\n",
    "        plt.title(f'${magbins[i]} > M_r > {magbins[i+1]}$')\n",
    "        plt.text(0.75, 0.9, f'$z<{zmaxes[i]}$', transform=plt.gca().transAxes, ha='center', va='center', fontsize=10)\n",
    "\n",
    "        if new_wp is not None:\n",
    "            if new_wp.shape[1] > 3:\n",
    "                plt.errorbar(new_wp[:,0], new_wp[:,2], yerr=new_wp[:,3], label='New i1', fmt=f'{color}.', capsize=3, alpha=0.7)\n",
    "            else:\n",
    "                plt.plot(new_wp[:,0], new_wp[:,2], label='New i1', color=color, alpha=0.7)\n",
    "        \n",
    "        if new_i0 is not None:\n",
    "            if new_i0.shape[1] > 3:\n",
    "                plt.errorbar(new_i0[:,0], new_i0[:,2], yerr=new_i0[:,3], label='New i0', fmt=f'{color}--', capsize=3)\n",
    "            else:\n",
    "                plt.plot(new_i0[:,0], new_i0[:,2], label='New i0', color=color, linestyle='--')\n",
    "        \n",
    "        if old_wp is not None:\n",
    "            if old_wp.shape[1] > 3:\n",
    "                plt.errorbar(old_wp[:,0], old_wp[:,2], yerr=old_wp[:,3], label='Old', fmt='k.', capsize=3, alpha=0.7)\n",
    "            else:\n",
    "                plt.plot(old_wp[:,0], old_wp[:,2], label='Old', color='k', alpha=0.7)\n",
    "\n",
    "        if i == 4:  # Add legend only to first subplot\n",
    "            plt.legend()\n",
    "\n",
    "    plt.suptitle(f'New vs Old Results - {title_suffix}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overlay new vs old results for red galaxies\n",
    "plot_new_vs_old_comparison(red_results_i1, red_result_i0, old_red_results, \n",
    "                          zmaxes, magbins, \"Red Galaxies (Quiescent)\", color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overlay new vs old results for blue galaxies  \n",
    "plot_new_vs_old_comparison(blue_results_i1, blue_results_i0, old_blue_results,\n",
    "                          zmaxes, magbins, \"Blue Galaxies (Star-Forming)\", color='b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_FSF(indata,fsf_cols,fsf_dir='/pscratch/sd/i/ioannis/fastspecfit/data/loa/catalogs/',prog='bright'):\n",
    "    from astropy.table import Table,join\n",
    "    fsl = []\n",
    "    for hp in range(0,12):\n",
    "        fsi = fitsio.read(fsf_dir+f'fastspec-iron-main-{prog}-nside1-hp'+str(hp).zfill(2)+'.fits',ext='SPECPHOT',columns = fsf_cols)\n",
    "        fsl.append(fsi)\n",
    "    fs = np.concatenate(fsl)\n",
    "    del fsl\n",
    "    ol = len(indata)\n",
    "    # Print fs columns names\n",
    "    indata = join(indata,fs,keys=['TARGETID']) #note, anything missing from fastspecfit will now be missing\n",
    "    del fs\n",
    "    return indata\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It appears with new method i0 vs i1 does not matter\n",
    "# BUT Old method vs this is totally different\n",
    "# Let's inspect samples\n",
    "# Look at the prepared data\n",
    "olddir = os.path.join(CUSTOM_CLUSTERING_RESULTS_FOLDER, 'Y1', 'LSS', 'iron', 'LSScats', 'v1.5pip')\n",
    "dir = os.path.join(CUSTOM_CLUSTERING_RESULTS_FOLDER_NEW, 'Y1', 'LSS', 'iron', 'LSScats', 'v1.5pip', '19-20_Q_i0')\n",
    "df_old= Table(fitsio.read(os.path.join(olddir, 'BGS_BRIGHT_NGC_clustering.dat.fits')))\n",
    "df_new = Table(fitsio.read(os.path.join(dir, 'BGS_BRIGHT_NGC_clustering.dat.fits')))\n",
    "df_new = get_FSF(df_new, ['TARGETID', 'ABSMAG01_SDSS_R'], fsf_dir='/global/cfs/cdirs/desi/public/dr1/vac/dr1/fastspecfit/iron/v3.0/catalogs/', prog='bright')\n",
    "\n",
    "# Drop BITWEIGHT column and convert to pandas\n",
    "df_old.remove_columns(['BITWEIGHTS'])\n",
    "df_new.remove_columns(['BITWEIGHTS'])\n",
    "df_old = df_old.to_pandas()\n",
    "df_new = df_new.to_pandas()\n",
    "\n",
    "df_old.set_index('TARGETID', inplace=True)\n",
    "df_new.set_index('TARGETID', inplace=True)\n",
    "\n",
    "#df_old.add_index('TARGETID')\n",
    "#df_new.add_index('TARGETID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old size: 2,909,876, New size: 206,125\n",
      "Filtered Old size: 100,097, Filtered New size: 100,073\n"
     ]
    }
   ],
   "source": [
    "# filter them to same z and abs mag range\n",
    "# 0.001 < z < 0.14977\n",
    "# -20 < M_r < -19\n",
    "old_size = len(df_old)\n",
    "new_size = len(df_new)\n",
    "df_old = df_old.loc[(df_old['Z'] > 0.001) & (df_old['Z'] < 0.14977) & (df_old['ABSMAG_R'] < -19) & (df_old['ABSMAG_R'] > -20) & df_old['QUIESCENT'] == 1]\n",
    "#df_old = df_old.loc[(df_old['Z'] > 0.001) & (df_old['Z'] < 0.14977) & df_old['QUIESCENT'] == 1]\n",
    "df_new = df_new.loc[(df_new['Z'] > 0.001) & (df_new['Z'] < 0.14977)]\n",
    "print(f\"Old size: {old_size:,}, New size: {new_size:,}\")\n",
    "print(f\"Filtered Old size: {len(df_old):,}, Filtered New size: {len(df_new):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_old: rows = 100097 columns = 20\n",
      "df_new: rows = 100073 columns = 20\n",
      "\n",
      "df_old columns: Index(['Z', 'NTILE', 'RA', 'DEC', 'PHOTSYS', 'FRAC_TLOBS_TILES',\n",
      "       'WEIGHT_ZFAIL', 'PROB_OBS', 'WEIGHT', 'WEIGHT_COMP', 'WEIGHT_SYS',\n",
      "       'flux_g_dered', 'flux_r_dered', 'flux_z_dered', 'flux_w1_dered',\n",
      "       'flux_w2_dered', 'NX', 'WEIGHT_FKP', 'ABSMAG_R', 'QUIESCENT'],\n",
      "      dtype='object')\n",
      "df_new columns: Index(['TILEID', 'Z', 'NTILE', 'RA', 'DEC', 'PHOTSYS', 'FRAC_TLOBS_TILES',\n",
      "       'WEIGHT_ZFAIL', 'PROB_OBS', 'WEIGHT', 'WEIGHT_COMP', 'WEIGHT_SYS',\n",
      "       'flux_g_dered', 'flux_r_dered', 'flux_z_dered', 'flux_w1_dered',\n",
      "       'flux_w2_dered', 'NX', 'WEIGHT_FKP', 'ABSMAG01_SDSS_R'],\n",
      "      dtype='object')\n",
      "\n",
      "Columns in df_old but not df_new: {'ABSMAG_R', 'QUIESCENT'}\n",
      "Columns in df_new but not df_old: {'ABSMAG01_SDSS_R', 'TILEID'}\n",
      "\n",
      "Column: RA\n",
      "  df_old: mean = 194.590712081077 std = 47.17363451738618\n",
      "  df_new: mean = 194.59979895560846 std = 47.17771299389695\n",
      "\n",
      "Column: WEIGHT\n",
      "  df_old: mean = 1.055331605200858 std = 0.6281498728931229\n",
      "  df_new: mean = 0.9947900973936997 std = 0.5846467649509172\n",
      "\n",
      "Column: WEIGHT_COMP\n",
      "  df_old: mean = 1.61596032128948 std = 1.264106008015736\n",
      "  df_new: mean = 1.616235597303464 std = 1.2649863894349365\n",
      "\n",
      "Column: WEIGHT_ZFAIL\n",
      "  df_old: mean = 1.0000602717056464 std = 0.000793509635480785\n",
      "  df_new: mean = 1.0000599657379126 std = 0.0007906916587987173\n",
      "\n",
      "Column: WEIGHT_SYS\n",
      "  df_old: mean = 1.0 std = 0.0\n",
      "  df_new: mean = 1.0 std = 0.0\n",
      "\n",
      "Column: Z\n",
      "  df_old: mean = 0.10922108183451137 std = 0.030091285461617382\n",
      "  df_new: mean = 0.10920855474150493 std = 0.030110950850680954\n",
      "\n",
      "Column: PROB_OBS\n",
      "  df_old: mean = 0.7677381845859517 std = 0.26114310405732366\n",
      "  df_new: mean = 0.7677368177480439 std = 0.26119594618989256\n",
      "\n",
      "Column: flux_z_dered\n",
      "  df_old: mean = 198.9062 std = 430.87366\n",
      "  df_new: mean = 200.50113 std = 463.39627\n",
      "\n",
      "Column: flux_r_dered\n",
      "  df_old: mean = 111.62415 std = 252.86676\n",
      "  df_new: mean = 112.59858 std = 273.076\n",
      "\n",
      "Column: DEC\n",
      "  df_old: mean = 24.366231344553427 std = 23.524920378784227\n",
      "  df_new: mean = 24.371940213875817 std = 23.527728080683904\n",
      "\n",
      "Column: FRAC_TLOBS_TILES\n",
      "  df_old: mean = 0.9724073461974642 std = 0.03178854966905005\n",
      "  df_new: mean = 0.9724131702229164 std = 0.03178719795965778\n",
      "\n",
      "Column: flux_g_dered\n",
      "  df_old: mean = 50.277058 std = 123.447014\n",
      "  df_new: mean = 50.776947 std = 134.48956\n",
      "\n",
      "Column: flux_w1_dered\n",
      "  df_old: mean = 160.97366 std = 312.7012\n",
      "  df_new: mean = 161.89624 std = 329.96304\n",
      "\n",
      "Column: NTILE\n",
      "  df_old: mean = 1.9699791202533543 std = 0.9070821277572261\n",
      "  df_new: mean = 1.9702217381311642 std = 0.907149439354111\n",
      "\n",
      "Column: flux_w2_dered\n",
      "  df_old: mean = 100.06244 std = 189.39835\n",
      "  df_new: mean = 100.61153 std = 199.99547\n",
      "\n",
      "Column: WEIGHT_FKP\n",
      "  df_old: mean = 0.0076796982099044024 std = 0.0032370742561009095\n",
      "  df_new: mean = 0.058548402082820875 std = 0.016421718153734142\n",
      "\n",
      "Column: NX\n",
      "  df_old: mean = 0.0229973696088027 std = 0.013505767427410439\n",
      "  df_new: mean = 0.0024885398971079115 std = 0.0006996758917310499\n"
     ]
    }
   ],
   "source": [
    "# Compare df_old and df_new in several ways\n",
    "\n",
    "# 1. Print basic info\n",
    "print(\"df_old: rows =\", len(df_old), \"columns =\", len(df_old.columns))\n",
    "print(\"df_new: rows =\", len(df_new), \"columns =\", len(df_new.columns))\n",
    "\n",
    "# 2. Print column names\n",
    "print(\"\\ndf_old columns:\", df_old.columns)\n",
    "print(\"df_new columns:\", df_new.columns)\n",
    "\n",
    "# 3. Find columns in one but not the other\n",
    "print(\"\\nColumns in df_old but not df_new:\", set(df_old.columns) - set(df_new.columns))\n",
    "print(\"Columns in df_new but not df_old:\", set(df_new.columns) - set(df_old.columns))\n",
    "\n",
    "# 4. Compare summary statistics for columns in common\n",
    "common_cols = set(df_old.columns) & set(df_new.columns)\n",
    "for col in common_cols:\n",
    "    if df_old[col].dtype.kind in 'if' and df_new[col].dtype.kind in 'if':\n",
    "        print(f\"\\nColumn: {col}\")\n",
    "        print(\"  df_old: mean =\", df_old[col].mean(), \"std =\", df_old[col].std())\n",
    "        print(\"  df_new: mean =\", df_new[col].mean(), \"std =\", df_new[col].std())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows in df_old but not in df_new: 230\n",
      "Rows in df_new but not in df_old: 206\n",
      "Number of matched TARGETIDs: 99867\n",
      "\n",
      "Summary of differences:\n",
      "ABSMAG_R: Median = -0.000, 1σ = 0.021, 2σ = 0.042, 3σ = 0.064\n",
      "WEIGHT: Median = -5.462, 1σ = 1.654, 2σ = 3.309, 3σ = 4.963\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Find rows in df_old but not in df_new, and vice versa\n",
    "missing_in_new = df_old.loc[~df_old.index.isin(df_new.index)]\n",
    "missing_in_old = df_new.loc[~df_new.index.isin(df_old.index)]\n",
    "print(f\"Rows in df_old but not in df_new: {len(missing_in_new)}\")\n",
    "print(f\"Rows in df_new but not in df_old: {len(missing_in_old)}\")\n",
    "\n",
    "# Find common TARGETIDs (intersection of indexes)\n",
    "common_ids = df_old.index.intersection(df_new.index)\n",
    "print(f\"Number of matched TARGETIDs: {len(common_ids)}\")\n",
    "\n",
    "# Prepare arrays for old and new values (fill unmatched with np.nan)\n",
    "old_absmag = df_old['ABSMAG_R'].reindex(common_ids).values\n",
    "old_weight = df_old['WEIGHT'].reindex(common_ids).values\n",
    "new_absmag = df_new['ABSMAG01_SDSS_R'].reindex(common_ids).values\n",
    "new_weight = df_new['WEIGHT'].reindex(common_ids).values\n",
    "\n",
    "# Percent difference calculation\n",
    "absmag_diff = 100 * (new_absmag - old_absmag) / old_absmag\n",
    "weight_diff = 100 * (new_weight - old_weight) / old_weight\n",
    "\n",
    "# Store differences in a DataFrame for summary\n",
    "diff_df = pd.DataFrame({\n",
    "    'old_ABSMAG_R': old_absmag,\n",
    "    'new_ABSMAG01_SDSS_R': new_absmag,\n",
    "    'ABSMAG_R_diff': absmag_diff,\n",
    "    'old_WEIGHT': old_weight,\n",
    "    'new_WEIGHT': new_weight,\n",
    "    'WEIGHT_diff': weight_diff\n",
    "}, index=common_ids)\n",
    "\n",
    "# Summarize the differences\n",
    "mean_diff_absmag = diff_df['ABSMAG_R_diff'].median()\n",
    "std_diff_absmag = diff_df['ABSMAG_R_diff'].std()\n",
    "mean_diff_weight = diff_df['WEIGHT_diff'].median()\n",
    "std_diff_weight = diff_df['WEIGHT_diff'].std()\n",
    "\n",
    "print(\"\\nSummary of differences:\")\n",
    "print(f\"ABSMAG_R: Median = {mean_diff_absmag:.3f}, 1σ = {std_diff_absmag:.3f}, 2σ = {2*std_diff_absmag:.3f}, 3σ = {3*std_diff_absmag:.3f}\")\n",
    "print(f\"WEIGHT: Median = {mean_diff_weight:.3f}, 1σ = {std_diff_weight:.3f}, 2σ = {2*std_diff_weight:.3f}, 3σ = {3*std_diff_weight:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug the weight differences\n",
    "print(\"Debugging weight differences...\")\n",
    "print(f\"Number of common IDs: {len(common_ids)}\")\n",
    "print(f\"Number of non-NaN weight differences: {(~np.isnan(weight_diff)).sum()}\")\n",
    "print(f\"Number of NaN weight differences: {np.isnan(weight_diff).sum()}\")\n",
    "\n",
    "# Check for extreme outliers in weight differences\n",
    "print(f\"\\nWeight difference percentiles:\")\n",
    "print(f\"  5th percentile: {np.nanpercentile(weight_diff, 5):.3f}%\")\n",
    "print(f\"  25th percentile: {np.nanpercentile(weight_diff, 25):.3f}%\")\n",
    "print(f\"  50th percentile (median): {np.nanpercentile(weight_diff, 50):.3f}%\")\n",
    "print(f\"  75th percentile: {np.nanpercentile(weight_diff, 75):.3f}%\")\n",
    "print(f\"  95th percentile: {np.nanpercentile(weight_diff, 95):.3f}%\")\n",
    "\n",
    "# Show histogram of weight differences\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "valid_weight_diff = weight_diff[~np.isnan(weight_diff)]\n",
    "plt.hist(valid_weight_diff, bins=50, alpha=0.7, edgecolor='black')\n",
    "plt.xlabel('Weight % Difference')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Weight Differences')\n",
    "plt.axvline(np.median(valid_weight_diff), color='red', linestyle='--', label=f'Median: {np.median(valid_weight_diff):.1f}%')\n",
    "plt.legend()\n",
    "\n",
    "# Zoom in on the central region\n",
    "plt.subplot(1, 2, 2)\n",
    "central_range = np.abs(valid_weight_diff) < 50  # Focus on differences within ±50%\n",
    "if central_range.sum() > 0:\n",
    "    plt.hist(valid_weight_diff[central_range], bins=30, alpha=0.7, edgecolor='black')\n",
    "    plt.xlabel('Weight % Difference')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Weight Differences (±50% range)')\n",
    "    plt.axvline(np.median(valid_weight_diff), color='red', linestyle='--', label=f'Median: {np.median(valid_weight_diff):.1f}%')\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Check if there are systematic issues with the reindex operation\n",
    "print(f\"\\nChecking reindex operation...\")\n",
    "print(f\"Old weight array has {np.isnan(old_weight).sum()} NaN values\")\n",
    "print(f\"New weight array has {np.isnan(new_weight).sum()} NaN values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative approach: merge dataframes to avoid reindex issues\n",
    "print(\"Alternative analysis using merge...\")\n",
    "\n",
    "# Create temporary dataframes with just the columns we need\n",
    "df_old_temp = df_old.reset_index()[['TARGETID', 'WEIGHT', 'ABSMAG_R']].copy()\n",
    "df_new_temp = df_new.reset_index()[['TARGETID', 'WEIGHT', 'ABSMAG01_SDSS_R']].copy()\n",
    "\n",
    "# Merge on TARGETID\n",
    "merged_df = df_old_temp.merge(df_new_temp, on='TARGETID', suffixes=('_old', '_new'))\n",
    "print(f\"Successfully merged {len(merged_df)} rows\")\n",
    "\n",
    "# Calculate differences\n",
    "merged_df['weight_diff_pct'] = 100 * (merged_df['WEIGHT_new'] - merged_df['WEIGHT_old']) / merged_df['WEIGHT_old']\n",
    "merged_df['absmag_diff_pct'] = 100 * (merged_df['ABSMAG01_SDSS_R'] - merged_df['ABSMAG_R']) / merged_df['ABSMAG_R']\n",
    "\n",
    "# Print summary statistics\n",
    "print(f\"\\nMerged dataframe statistics:\")\n",
    "print(f\"Weight differences - Mean: {merged_df['weight_diff_pct'].mean():.3f}%, Median: {merged_df['weight_diff_pct'].median():.3f}%, Std: {merged_df['weight_diff_pct'].std():.3f}%\")\n",
    "print(f\"AbsMag differences - Mean: {merged_df['absmag_diff_pct'].mean():.3f}%, Median: {merged_df['absmag_diff_pct'].median():.3f}%, Std: {merged_df['absmag_diff_pct'].std():.3f}%\")\n",
    "\n",
    "# Check for extreme outliers\n",
    "extreme_weight_outliers = np.abs(merged_df['weight_diff_pct']) > 100\n",
    "print(f\"\\nNumber of extreme weight outliers (>100% difference): {extreme_weight_outliers.sum()}\")\n",
    "\n",
    "if extreme_weight_outliers.sum() > 0:\n",
    "    print(\"Examples of extreme weight outliers:\")\n",
    "    outlier_examples = merged_df[extreme_weight_outliers].head(5)\n",
    "    for _, row in outlier_examples.iterrows():\n",
    "        print(f\"  TARGETID {row['TARGETID']}: Old weight {row['WEIGHT_old']:.3f}, New weight {row['WEIGHT_new']:.3f}, Diff: {row['weight_diff_pct']:.1f}%\")\n",
    "\n",
    "# Show random sample from merged data\n",
    "print(f\"\\nRandom sample from merged data:\")\n",
    "sample_merged = merged_df.sample(min(10, len(merged_df)))\n",
    "for _, row in sample_merged.iterrows():\n",
    "    print(f\"TARGETID {row['TARGETID']}: Weight diff: {row['weight_diff_pct']:.1f}%, AbsMag diff: {row['absmag_diff_pct']:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TARGETID: New (ABSMAG01_SDSS_R and WEIGHT) vs Old (ABSMAG_R and WEIGHT)\n",
      "39627643326301868: New: (-19.713, 0.451), Old: (-19.714, 0.488)\n",
      "39627945723039818: New: (-19.675, 0.925), Old: (-19.675, 0.962)\n",
      "39627637152288046: New: (-19.098, 0.860), Old: (-19.098, 0.895)\n",
      "39627745189170797: New: (-19.863, 1.077), Old: (-19.862, 1.166)\n",
      "39633531260898442: New: (-19.187, 1.662), Old: (-19.189, 1.799)\n",
      "39628406454747551: New: (-19.137, 1.385), Old: (-19.137, 1.499)\n",
      "39627654256656977: New: (-19.414, 0.881), Old: (-19.416, 0.916)\n",
      "39633538068253443: New: (-19.667, 0.895), Old: (-19.667, 0.968)\n",
      "39627745046561759: New: (-19.584, 0.895), Old: (-19.582, 0.931)\n",
      "39627703044806340: New: (-19.216, 0.991), Old: (-19.216, 1.030)\n",
      "39627782552030456: New: (-19.093, 0.874), Old: (-19.090, 0.909)\n",
      "39633182366107009: New: (-19.386, 1.711), Old: (-19.388, 1.852)\n",
      "39627860050185016: New: (-19.105, 0.860), Old: (-19.094, 0.895)\n",
      "39633148711012077: New: (-19.128, 1.058), Old: (-19.124, 1.145)\n",
      "39633099574740464: New: (-19.378, 0.489), Old: (-19.378, 0.529)\n"
     ]
    }
   ],
   "source": [
    "# Take some TARGETIDs from df_new and print off the ABSMAG_R from the match in old\n",
    "print(\"TARGETID: New (ABSMAG01_SDSS_R and WEIGHT) vs Old (ABSMAG_R and WEIGHT)\")\n",
    "sample_ids = pd.Index(df_new.index).intersection(df_old.index)\n",
    "if len(sample_ids) > 0:\n",
    "    target_ids = np.random.choice(sample_ids, size=min(15, len(sample_ids)), replace=False)\n",
    "    for target_id in target_ids:\n",
    "        new_row = df_new.loc[[target_id]]\n",
    "        old_row = df_old.loc[[target_id]]\n",
    "        print(f\"{target_id}: New: ({new_row['ABSMAG01_SDSS_R'].values[0]:.3f}, {new_row['WEIGHT'].values[0]:.3f}), Old: ({old_row['ABSMAG_R'].values[0]:.3f}, {old_row['WEIGHT'].values[0]:.3f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "from astropy.coordinates import SkyCoord\n",
    "\n",
    "import astropy.units as u\n",
    "\n",
    "# Create SkyCoord objects for both dataframes\n",
    "coords_old = SkyCoord(ra=df_old['RA'].values * u.deg, dec=df_old['DEC'].values * u.deg)\n",
    "coords_new = SkyCoord(ra=df_new['RA'].values * u.deg, dec=df_new['DEC'].values * u.deg)\n",
    "\n",
    "# Match old to new\n",
    "idx, d2d, d3d = coords_old.match_to_catalog_sky(coords_new)\n",
    "\n",
    "# Add columns to df_old for matched new TARGETID and separation in arcsec\n",
    "df_old['matched_new_TARGETID'] = df_new.iloc[idx]['TARGETID'].values\n",
    "df_old['sep_arcsec'] = d2d.arcsec\n",
    "\n",
    "matched = d2d.to(u.arcsec) < 1.0 * u.arcsec  # 1 arcsec match threshold\n",
    "df_old['matched'] = matched\n",
    "\n",
    "# Show a summary of the matching\n",
    "print(f\"Matched {matched.sum()} out of {len(df_old)} old entries to new entries within 1 arcsec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View Imaging Systematics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO look at imaging systematics\n",
    "#path = 'BGS_BRIGHT_N_0.0010.5_linclusimsysfit.png'\n",
    "\n",
    "for fn in ['BGS_BRIGHT_N_0.0010.5_linclusimsysfit.png']:\n",
    "    path = os.path.join(CUSTOM_CLUSTERING_RESULTS_FOLDER_NEW, 'Y1', 'LSS', 'iron', 'LSScats', 'v1.5pip', '19-20_Q_i1', fn)\n",
    "    if os.path.exists(path):\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        img = plt.imread(path)\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.title(fn)\n",
    "        plt.show()\n",
    "        \n",
    "        # Also print off the .txt file. Same thing but _linfitparam.txt\n",
    "        txt_fn = path.replace('_linclusimsysfit.png', '_linfitparam.txt')\n",
    "        if os.path.exists(txt_fn):\n",
    "            with open(txt_fn, 'r') as f:\n",
    "                print(f.read())\n",
    "        else:\n",
    "            print(\"No text file found for\", txt_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
