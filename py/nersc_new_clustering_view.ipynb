{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import fitsio\n",
    "from pycorr import TwoPointCorrelationFunction, TwoPointEstimator, project_to_multipoles, project_to_wp, utils, setup_logging\n",
    "from scipy.optimize import curve_fit\n",
    "from LSS.common_tools import mknz\n",
    "from astropy.table import Table\n",
    "import itertools\n",
    "\n",
    "from dataloc import *\n",
    "\n",
    "# MAKE ALL PLOTS TEXT BIGGER\n",
    "plt.rcParams.update({'font.size': 15})\n",
    "# But legend a bit smaller\n",
    "plt.rcParams.update({'legend.fontsize': 12})\n",
    "# Set DPI up a bit\n",
    "plt.rcParams.update({'figure.dpi': 150})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zmaxes = [0.02595, 0.04067, 0.06336, 0.09792, 0.14977, 0.22620, 0.33694, 0.49523] \n",
    "magbins = [-15, -16, -17, -18, -19, -20, -21, -22, -23]\n",
    "NRAND_SF=[1, 1, 2, 5, 9, 12, 6, 1] \n",
    "NRAND_Q=[1, 1, 1, 1, 5, 12, 12, 3] \n",
    "NRAND_ALL=[1, 1, 1, 1, 5, 12, 12, 3] \n",
    "tracer = \"BGS_BRIGHT\" \n",
    "\n",
    "\n",
    "def get_new_wp(weights, survey, verspec, ver, zmax, nran, njack, region, mag: int, imagingver, quiescent=None, modifiedtracer=True):\n",
    "    if quiescent:\n",
    "        folder = f'{np.abs(mag)}-{np.abs(mag)+1}_Q_{imagingver}'\n",
    "        string = f'_R-{np.abs(mag)}-{np.abs(mag)+1}_Q'\n",
    "    elif quiescent is None:\n",
    "        folder = f'{np.abs(mag)}-{np.abs(mag)+1}_ALL_{imagingver}'\n",
    "        string = f'_R-{np.abs(mag)}-{np.abs(mag)+1}'\n",
    "    else:\n",
    "        folder = f'{np.abs(mag)}-{np.abs(mag)+1}_SF_{imagingver}'\n",
    "        string = f'_R-{np.abs(mag)}-{np.abs(mag)+1}_SF'\n",
    "        \n",
    "    if not modifiedtracer:\n",
    "        string = ''\n",
    "\n",
    "    dir = os.path.join(CUSTOM_CLUSTERING_RESULTS_FOLDER_NEW, survey, 'LSS', verspec, 'LSScats', ver, folder, 'rppi')\n",
    "\n",
    "    if not os.path.exists(dir):\n",
    "        print(\"WARNING: Directory does not exist: \", dir)\n",
    "        return None, None\n",
    "        \n",
    "    wp_fn = os.path.join(dir, f'wp_BGS_BRIGHT{string}_{region}_0.001_{zmax}_{weights}_custom_njack{njack}_nran{nran}_split20.txt')\n",
    "\n",
    "    if not os.path.exists(wp_fn):\n",
    "        print(\"WARNING: File does not exist: \", wp_fn)\n",
    "        return None, None\n",
    "\n",
    "    try:\n",
    "        allcounts_fn = os.path.join(dir, f'allcounts_BGS_BRIGHT_{region}_0.001_{zmax}_{weights}_custom_njack{njack}_nran{nran}_split20.npy')\n",
    "        tpc = TwoPointEstimator.load(allcounts_fn)\n",
    "        s, xiell, cov = tpc.get_corr(return_sep=True, return_cov=True, mode='wp')\n",
    "    except Exception as e:\n",
    "        print(\"WARNING: Could not load TwoPointEstimator\")\n",
    "        cov = None\n",
    "\n",
    "    return np.loadtxt(wp_fn), cov\n",
    "\n",
    "def get_wp_for(tracer, weights, survey, verspec, ver, bins, zmax, nran, njack, region, quiescent=None):\n",
    "    dir = os.path.join(CUSTOM_CLUSTERING_RESULTS_FOLDER, survey, 'LSS', verspec, 'LSScats', ver, 'rppi')\n",
    "\n",
    "    if not os.path.exists(dir):\n",
    "        print(\"WARNING: Directory does not exist: \", dir)\n",
    "        return None\n",
    "\n",
    "    if quiescent is not None:\n",
    "        addon = '_QUIESCENT' + ('1' if quiescent else '0')\n",
    "    else:\n",
    "        addon = ''\n",
    "        \n",
    "    wp_fn = os.path.join(dir, f'wp_{tracer}_{region}_0.001_{zmax}_{weights}_{bins}_njack{njack}_nran{nran}_split20{addon}.txt')\n",
    "\n",
    "    if not os.path.exists(wp_fn):\n",
    "        print(\"WARNING: File does not exist: \", wp_fn)\n",
    "        return None\n",
    "\n",
    "    return np.loadtxt(wp_fn)\n",
    "\n",
    "def get_fn_for(weights, survey, verspec, ver, zmax, nran, njack, region, quiescent=None):\n",
    "    dir = os.path.join(CUSTOM_CLUSTERING_RESULTS_FOLDER, survey, 'LSS', verspec, 'LSScats', ver, 'rppi')\n",
    "\n",
    "    if not os.path.exists(dir):\n",
    "        print(\"WARNING: Directory does not exist: \", dir)\n",
    "        return None\n",
    "\n",
    "    if quiescent is not None:\n",
    "        addon = '_QUIESCENT' + ('1' if quiescent else '0')\n",
    "    else:\n",
    "        addon = ''\n",
    "        \n",
    "    allcounts_fn = os.path.join(dir, f'allcounts_BGS_BRIGHT_{region}_0.001_{zmax}_{weights}_custom_njack{njack}_nran{nran}_split20{addon}.npy')\n",
    "    return allcounts_fn\n",
    "\n",
    "\n",
    "def plot_wp(red_results, blue_results, all_results, zmaxes, magbins):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for i in range(len(red_results)):\n",
    "        plt.subplot(3, 3, i+1)\n",
    "        red_wp, red_cov = red_results[i]\n",
    "        blue_wp, blue_cov = blue_results[i]\n",
    "        #all_wp, all_cov = all_results[i]\n",
    "\n",
    "        plt.xscale('log')\n",
    "        plt.yscale('log')\n",
    "        plt.ylim(2, 4000)\n",
    "        plt.ylabel(r'$w_p(r_p)$')\n",
    "        plt.xlabel(r'$r_p$ [Mpc/h]')\n",
    "        plt.title(f'${magbins[i]} > M_r > {magbins[i+1]}$')\n",
    "        plt.text(0.75, 0.9, f'$z<{zmaxes[i]}$', transform=plt.gca().transAxes, ha='center', va='center', fontsize=10)\n",
    "\n",
    "        if red_wp is not None:\n",
    "            if red_wp.shape[1] > 3:\n",
    "                plt.errorbar(red_wp[:,0], red_wp[:,2], yerr=red_wp[:,3], label=f'Quiescent {zmaxes[i]}', fmt='r.', capsize=3)\n",
    "            else:\n",
    "                plt.plot(red_wp[:,0], red_wp[:,2], label=f'Quiescent {zmaxes[i]}', color='r')\n",
    "        if blue_wp is not None:\n",
    "            if blue_wp.shape[1] > 3:\n",
    "                plt.errorbar(blue_wp[:,0], blue_wp[:,2], yerr=blue_wp[:,3], label=f'Star-Forming {zmaxes[i]}', fmt='b.', capsize=3)\n",
    "            else:\n",
    "                plt.plot(blue_wp[:,0], blue_wp[:,2], label=f'Star-Forming {zmaxes[i]}', color='b')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red_result_i0 = []\n",
    "blue_results_i0 = []\n",
    "all_results_i0 = []\n",
    "for m, z, rsf, rq in zip(magbins[:-1], zmaxes, NRAND_SF, NRAND_Q):\n",
    "    for q in [True, False, None]:\n",
    "        r = rq if q is True else rsf\n",
    "        wp, cov = get_new_wp('pip_angular_bitwise', 'Y1', 'iron', 'v1.5pip', z, r, 0, 'GCcomb', m, \"i0\", quiescent=q)\n",
    "        if q is True:\n",
    "            red_result_i0.append((wp, cov))\n",
    "        elif q is False:\n",
    "            blue_results_i0.append((wp, cov))\n",
    "        else:\n",
    "            all_results_i0.append((wp, cov))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red_results_i1 = []\n",
    "blue_results_i1 = []\n",
    "all_results_i1 = []\n",
    "for m, z, rsf, rq in zip(magbins[:-1], zmaxes, NRAND_SF, NRAND_Q):\n",
    "    for q in [True, False, None]:\n",
    "        r = rq if q is True else rsf\n",
    "        wp, cov = get_new_wp('pip_angular_bitwise', 'Y1', 'iron', 'v1.5pip', z, r, 128, 'GCcomb', m, \"i1\", quiescent=q)\n",
    "\n",
    "        if q is True:\n",
    "            red_results_i1.append((wp, cov))\n",
    "        elif q is False:\n",
    "            blue_results_i1.append((wp, cov))\n",
    "        else:\n",
    "            all_results_i1.append((wp, cov))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red_i2_results = []\n",
    "blue_i2_results = []\n",
    "all_i2_results = []\n",
    "for m, z, rsf, rq in zip(magbins[:-1], zmaxes, NRAND_SF, NRAND_Q):\n",
    "    for q in [True, False, None]:\n",
    "        r = rq if q is True else rsf\n",
    "        wp = get_new_wp('pip_bitwise', 'Y1', 'iron', 'v1.5pip', z, r, 0, 'GCcomb', m, \"i2\", quiescent=q, modifiedtracer=False)\n",
    "\n",
    "        if q is True:\n",
    "            red_i2_results.append(wp)\n",
    "        elif q is False:\n",
    "            blue_i2_results.append(wp)\n",
    "        else:\n",
    "            all_i2_results.append(wp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_wp(red_results_i1, blue_results_i1, main_all_results, zmaxes, magbins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the equivalent old results from the other NERSC clustering view notebook\n",
    "# Assuming the old results are stored in a similar directory structure but under CUSTOM_CLUSTERING_RESULTS_FOLDER\n",
    "#zmaxes = [0.02595, 0.04067, 0.06336, 0.09792, 0.14977, 0.22620, 0.33694, 0.49523] \n",
    "#magbins = [-15, -16, -17, -18, -19, -20, -21, -22, -23]\n",
    "tracer = \"BGS_BRIGHT\" \n",
    "jack_official = 128\n",
    "old_red_results = []\n",
    "old_blue_results = []\n",
    "old_all_results = []\n",
    "for z, q in itertools.product(zmaxes, [True, False, None]):\n",
    "\n",
    "    wp = get_wp_for(tracer, 'pip_angular_bitwise', 'Y1', 'iron', 'v1.5pip', 'custom', z, 18, jack_official, 'GCcomb', quiescent=q)\n",
    "    cov = None\n",
    "    if wp is not None:\n",
    "        savedir = get_fn_for('pip_angular_bitwise', 'Y1', 'iron', 'v1.5pip', z, 18, jack_official, 'GCcomb', quiescent=q)\n",
    "        tpc = TwoPointEstimator.load(savedir)\n",
    "        s, xiell, cov = tpc.get_corr(return_sep=True, return_cov=True, mode='wp')\n",
    "\n",
    "    if q is True:\n",
    "        old_red_results.append((wp, cov))\n",
    "    elif q is False:\n",
    "        old_blue_results.append((wp, cov))\n",
    "    else:\n",
    "        old_all_results.append((wp, cov))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_new_vs_old_comparison(i1_results, i0_results, old_results, i2_results, zmaxes, magbins, title_suffix, color='r'):\n",
    "    \"\"\"\n",
    "    Plot comparison between new and old clustering results\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    new_results : list\n",
    "        List of (wp, cov) tuples for new results\n",
    "    new_i0_results : list  \n",
    "        List of (wp, cov) tuples for new i0 results\n",
    "    old_results : list\n",
    "        List of (wp, cov) tuples for old results\n",
    "    zmaxes : list\n",
    "        Redshift limits for each magnitude bin\n",
    "    magbins : list\n",
    "        Magnitude bin edges\n",
    "    title_suffix : str\n",
    "        Suffix to add to plot title (e.g., \"Red Galaxies\", \"Blue Galaxies\")\n",
    "    color : str\n",
    "        Color for new results plots\n",
    "    \"\"\"\n",
    "\n",
    "    # assert all lengths are the same\n",
    "    assert len(i1_results) == len(old_results) == len(i0_results) == len(i2_results)\n",
    "\n",
    "    plt.figure(figsize=(11, 11))\n",
    "    for i in range(len(old_results)):\n",
    "        plt.subplot(3, 3, i+1)\n",
    "        i1, _ = i1_results[i] if i1_results else (None, None)\n",
    "        i0, _ = i0_results[i] if i0_results else (None, None)\n",
    "        old_wp, _ = old_results[i]\n",
    "        i2, _ = i2_results[i] if i2_results else (None, None)\n",
    "        #noang, _ = noangular[i] if noangular else (None, None)\n",
    "\n",
    "        plt.xscale('log')\n",
    "        plt.yscale('log')\n",
    "        plt.ylim(1, 4000)\n",
    "        plt.ylabel(r'$w_p(r_p)$')\n",
    "        plt.xlabel(r'$r_p$ [Mpc/h]')\n",
    "        plt.title(f'${magbins[i]} > M_r > {magbins[i+1]}$')\n",
    "        plt.text(0.2, 0.1, f'$z<{zmaxes[i]}$', transform=plt.gca().transAxes, ha='center', va='center', fontsize=10)\n",
    "\n",
    "        if old_wp is not None:\n",
    "            if old_wp.shape[1] > 3:\n",
    "                plt.errorbar(old_wp[:,0], old_wp[:,2], yerr=old_wp[:,3], label='Old ang', fmt='k.', capsize=3)\n",
    "            else:\n",
    "                plt.plot(old_wp[:,0], old_wp[:,2], label='Old ang', color='k')\n",
    "        \n",
    "        if i1 is not None:\n",
    "            if i1.shape[1] > 3:\n",
    "                plt.errorbar(i1[:,0], i1[:,2], yerr=i1[:,3], label='ang i1', fmt=f'{color}.', capsize=3, alpha=0.7)\n",
    "            else:\n",
    "                plt.plot(i1[:,0], i1[:,2], label='ang i1', color=color, alpha=0.7)\n",
    "        \n",
    "        if i0 is not None:\n",
    "            if i0.shape[1] > 3:\n",
    "                plt.errorbar(i0[:,0], i0[:,2], yerr=i0[:,3], label='ang i0', fmt=f'purple--', capsize=3)\n",
    "            else:\n",
    "                plt.plot(i0[:,0], i0[:,2], label='ang i0', color='purple', linestyle='--')   \n",
    "\n",
    "        if i2 is not None:\n",
    "            if i2.shape[1] > 3:\n",
    "                plt.errorbar(i2[:,0], i2[:,2], yerr=i2[:,3], label='3p i1', fmt='g:', capsize=3)\n",
    "            else:\n",
    "                plt.plot(i2[:,0], i2[:,2], label='3p i1', color='g', linestyle=':')\n",
    "\n",
    "        plt.legend()\n",
    "\n",
    "    plt.suptitle(f'Clustering Comparison - {title_suffix}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# wp plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overlay new vs old results for red galaxies\n",
    "plot_new_vs_old_comparison(red_results_i1, red_result_i0, old_red_results, red_i2_results,\n",
    "                          zmaxes, magbins, \"Quiescent\", color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overlay new vs old results for blue galaxies  \n",
    "plot_new_vs_old_comparison(blue_results_i1, blue_results_i0, old_blue_results, blue_i2_results,\n",
    "                          zmaxes, magbins, \"Star-Forming\", color='b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overlay new vs old results for all galaxies  \n",
    "plot_new_vs_old_comparison(all_results_i1, all_results_i0, old_all_results, all_i2_results,\n",
    "                          zmaxes, magbins, \"All\", color='m')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_FSF(indata,fsf_cols,fsf_dir='/pscratch/sd/i/ioannis/fastspecfit/data/loa/catalogs/',prog='bright'):\n",
    "    from astropy.table import Table,join\n",
    "    fsl = []\n",
    "    for hp in range(0,12):\n",
    "        fsi = fitsio.read(fsf_dir+f'fastspec-iron-main-{prog}-nside1-hp'+str(hp).zfill(2)+'.fits',ext='SPECPHOT',columns = fsf_cols)\n",
    "        fsl.append(fsi)\n",
    "    fs = np.concatenate(fsl)\n",
    "    del fsl\n",
    "    ol = len(indata)\n",
    "    # Print fs columns names\n",
    "    indata = join(indata,fs,keys=['TARGETID']) #note, anything missing from fastspecfit will now be missing\n",
    "    del fs\n",
    "    return indata\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the prepared data\n",
    "dirorig = os.path.join('/dvs_ro/cfs/cdirs/desi/survey/catalogs/', 'Y1', 'LSS', 'iron', 'LSScats', 'v1.5pip')\n",
    "dirold = os.path.join(CUSTOM_CLUSTERING_RESULTS_FOLDER, 'Y1', 'LSS', 'iron', 'LSScats', 'v1.5pip')\n",
    "diri0 = os.path.join(CUSTOM_CLUSTERING_RESULTS_FOLDER_NEW, 'Y1', 'LSS', 'iron', 'LSScats', 'v1.5pip', '19-20_SF_i0')\n",
    "diri0_fixed = os.path.join(CUSTOM_CLUSTERING_RESULTS_FOLDER_NEW, 'Y1', 'LSS', 'iron', 'LSScats', 'v1.5pip', '19-20_SF_i0angfixed')\n",
    "tbl1 = Table(fitsio.read(os.path.join(dirorig, 'BGS_BRIGHT_NGC_clustering.dat.fits')))\n",
    "tbl2 = Table(fitsio.read(os.path.join(dirold, 'BGS_BRIGHT_NGC_clustering.dat.fits')))\n",
    "tbl3 = Table(fitsio.read(os.path.join(diri0, 'BGS_BRIGHT_NGC_clustering.dat.fits')))\n",
    "tbl4 = Table(fitsio.read(os.path.join(diri0_fixed, 'BGS_BRIGHT_R-19-20_SF_NGC_clustering.dat.fits')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tbl1 = get_FSF(tbl1, ['TARGETID', 'ABSMAG01_SDSS_R'], fsf_dir='/global/cfs/cdirs/desi/public/dr1/vac/dr1/fastspecfit/iron/v3.0/catalogs/', prog='bright')\n",
    "tbl3 = get_FSF(tbl3, ['TARGETID', 'ABSMAG01_SDSS_R'], fsf_dir='/global/cfs/cdirs/desi/public/dr1/vac/dr1/fastspecfit/iron/v3.0/catalogs/', prog='bright')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# It appears with new method i0 vs i1 does not matter\n",
    "# BUT Old method vs this is totally different\n",
    "df_orig = tbl1\n",
    "df_old = tbl2\n",
    "df_new = tbl3\n",
    "df_i0_fixed = tbl4\n",
    "\n",
    "# Drop BITWEIGHT column and convert to pandas\n",
    "# Only keep columns in df_orig that are in df_old or df_new\n",
    "if 'BITWEIGHTS' in df_orig.columns:\n",
    "    df_orig.remove_columns(['BITWEIGHTS'])\n",
    "if 'BITWEIGHTS' in df_old.columns:\n",
    "    df_old.remove_columns(['BITWEIGHTS'])\n",
    "if 'BITWEIGHTS' in df_new.columns:\n",
    "    df_new.remove_columns(['BITWEIGHTS'])\n",
    "if 'BITWEIGHTS' in df_i0_fixed.columns:\n",
    "    df_i0_fixed.remove_columns(['BITWEIGHTS'])\n",
    "\n",
    "df_orig: pd.DataFrame = df_orig.to_pandas()\n",
    "df_old: pd.DataFrame = df_old.to_pandas()\n",
    "df_new: pd.DataFrame = df_new.to_pandas()\n",
    "df_i0_fixed: pd.DataFrame = df_i0_fixed.to_pandas()\n",
    "df_orig.set_index('TARGETID', inplace=True)\n",
    "df_old.set_index('TARGETID', inplace=True)\n",
    "df_new.set_index('TARGETID', inplace=True)\n",
    "df_i0_fixed.set_index('TARGETID', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter them to same z and abs mag range\n",
    "# 0.001 < z < 0.14977\n",
    "# -20 < M_r < -19\n",
    "df_orig = df_orig.loc[(df_orig['Z'] > 0.001) & (df_orig['Z'] < 0.14977)]\n",
    "df_old = df_old.loc[(df_old['Z'] > 0.001) & (df_old['Z'] < 0.14977)]\n",
    "df_new = df_new.loc[(df_new['Z'] > 0.001) & (df_new['Z'] < 0.14977)]\n",
    "df_i0_fixed = df_i0_fixed.loc[(df_i0_fixed['Z'] > 0.001) & (df_i0_fixed['Z'] < 0.14977)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare df_old and df_new in several ways\n",
    "\n",
    "# 1. Print basic info\n",
    "print(\"df_orig: rows =\", len(df_orig), \"columns =\", len(df_orig.columns))\n",
    "print(\"df_old: rows =\", len(df_old), \"columns =\", len(df_old.columns))\n",
    "print(\"df_new: rows =\", len(df_new), \"columns =\", len(df_new.columns))\n",
    "print(\"df_new_i1: rows =\", len(df_i0_fixed), \"columns =\", len(df_i0_fixed.columns))\n",
    "\n",
    "# 2. Find columns in one but not the other\n",
    "print(\"\\nColumns in df_old but not df_new:\", set(df_old.columns) - set(df_new.columns))\n",
    "print(\"Columns in df_new but not df_old:\", set(df_new.columns) - set(df_old.columns))\n",
    "\n",
    "# 3. Compare summary statistics for columns in common\n",
    "common_cols = set(df_old.columns) & set(df_new.columns)\n",
    "for col in common_cols:\n",
    "    if df_old[col].dtype.kind in 'if' and df_new[col].dtype.kind in 'if':\n",
    "        print(f\"\\nColumn: {col}\")\n",
    "        print(\"  df_old: mean =\", df_old[col].mean(), \"std =\", df_old[col].std())\n",
    "        print(\"  df_new: mean =\", df_new[col].mean(), \"std =\", df_new[col].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temporary dataframes with just the columns we need\n",
    "df_orig_temp = df_orig.reset_index()[['TARGETID', 'WEIGHT', 'WEIGHT_COMP', 'WEIGHT_SYS', 'WEIGHT_ZFAIL', 'WEIGHT_FKP']].copy()\n",
    "df_old_temp = df_old.reset_index()[['TARGETID', 'WEIGHT', 'WEIGHT_COMP', 'WEIGHT_SYS', 'WEIGHT_ZFAIL', 'WEIGHT_FKP']].copy()\n",
    "df_new_temp = df_new.reset_index()[['TARGETID', 'WEIGHT', 'WEIGHT_COMP', 'WEIGHT_SYS', 'WEIGHT_ZFAIL', 'WEIGHT_FKP']].copy()\n",
    "df_i0_fixed_temp = df_i0_fixed.reset_index()[['TARGETID', 'WEIGHT', 'WEIGHT_COMP', 'WEIGHT_SYS', 'WEIGHT_ZFAIL', 'WEIGHT_FKP']].copy()\n",
    "\n",
    "# Merge on TARGETID\n",
    "merged_df = df_old_temp.merge(df_new_temp, on='TARGETID', suffixes=('_old', '_new'))\n",
    "merged_df = merged_df.merge(df_orig_temp, on='TARGETID', suffixes=('', '_orig'))\n",
    "merged_df = merged_df.merge(df_i0_fixed_temp, on='TARGETID', suffixes=('', '_new_i1'))\n",
    "print(f\"Successfully merged {len(merged_df)} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take some TARGETIDs from df_new and print off the ABSMAG_R from the match in old\n",
    "print(\"TARGETID: New (WEIGHT) vs Old (WEIGHT) vs Orig (WEIGHT)\")\n",
    "sample_ids = pd.Index(df_new.index).intersection(df_old.index)\n",
    "if len(sample_ids) > 0:\n",
    "    target_ids = np.random.choice(sample_ids, size=min(10, len(sample_ids)), replace=False)\n",
    "    for target_id in target_ids:\n",
    "        new_row = df_new.loc[[target_id]]\n",
    "        old_row = df_old.loc[[target_id]]\n",
    "        orig_row = df_orig.loc[[target_id]]\n",
    "        print(f\"{target_id}: {new_row['WEIGHT'].values[0]:.5f} vs {old_row['WEIGHT'].values[0]:.5f} vs {orig_row['WEIGHT'].values[0]:.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate percent change in all WEIGHT* columns from ORIG to OLD and ORIG to NEW\n",
    "weight_cols = ['WEIGHT', 'WEIGHT_COMP', 'WEIGHT_SYS', 'WEIGHT_ZFAIL', 'WEIGHT_FKP']\n",
    "\n",
    "for col in weight_cols:\n",
    "    merged_df[f'{col.lower()}_diff_pct_orig_to_old'] = 100 * (merged_df[f'{col}_old'] - merged_df[col]) / merged_df[col]\n",
    "    merged_df[f'{col.lower()}_diff_pct_orig_to_new_i0'] = 100 * (merged_df[f'{col}_new'] - merged_df[col]) / merged_df[col]\n",
    "    merged_df[f'{col.lower()}_diff_pct_orig_to_new_i1'] = 100 * (merged_df[f'{col}_new_i1'] - merged_df[col]) / merged_df[col]\n",
    "\n",
    "for col in weight_cols:\n",
    "    print(f\"\\nSummary of {col} percent change (ORIG to OLD):\")\n",
    "    print(f\"Mean: {merged_df[f'{col.lower()}_diff_pct_orig_to_old'].mean():.3f}%, Median: {merged_df[f'{col.lower()}_diff_pct_orig_to_old'].median():.3f}%, Std: {merged_df[f'{col.lower()}_diff_pct_orig_to_old'].std():.3f}%\")\n",
    "    print(f\"Summary of {col} percent change (ORIG to i0):\")\n",
    "    print(f\"Mean: {merged_df[f'{col.lower()}_diff_pct_orig_to_new_i0'].mean():.3f}%, Median: {merged_df[f'{col.lower()}_diff_pct_orig_to_new_i0'].median():.3f}%, Std: {merged_df[f'{col.lower()}_diff_pct_orig_to_new_i0'].std():.3f}%\")\n",
    "    print(f\"Summary of {col} percent change (ORIG to i0 fixed):\")\n",
    "    print(f\"Mean: {merged_df[f'{col.lower()}_diff_pct_orig_to_new_i1'].mean():.3f}%, Median: {merged_df[f'{col.lower()}_diff_pct_orig_to_new_i1'].median():.3f}%, Std: {merged_df[f'{col.lower()}_diff_pct_orig_to_new_i1'].std():.3f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate differences\n",
    "merged_df['weight_diff_pct'] = 100 * (merged_df['WEIGHT_new'] - merged_df['WEIGHT_old']) / merged_df['WEIGHT_old']\n",
    "merged_df['absmag_diff_pct'] = 100 * (merged_df['ABSMAG01_SDSS_R'] - merged_df['ABSMAG_R']) / merged_df['ABSMAG_R']\n",
    "\n",
    "# Print summary statistics\n",
    "print(f\"\\nMerged dataframe statistics:\")\n",
    "print(f\"Weight differences - Mean: {merged_df['weight_diff_pct'].mean():.3f}%, Median: {merged_df['weight_diff_pct'].median():.3f}%, Std: {merged_df['weight_diff_pct'].std():.3f}%\")\n",
    "print(f\"AbsMag differences - Mean: {merged_df['absmag_diff_pct'].mean():.3f}%, Median: {merged_df['absmag_diff_pct'].median():.3f}%, Std: {merged_df['absmag_diff_pct'].std():.3f}%\")\n",
    "\n",
    "# Check for extreme outliers\n",
    "extreme_weight_outliers = np.abs(merged_df['weight_diff_pct']) > 100\n",
    "print(f\"\\nNumber of extreme weight outliers (>100% difference): {extreme_weight_outliers.sum()}\")\n",
    "\n",
    "if extreme_weight_outliers.sum() > 0:\n",
    "    print(\"Examples of extreme weight outliers:\")\n",
    "    outlier_examples = merged_df[extreme_weight_outliers].head(5)\n",
    "    for _, row in outlier_examples.iterrows():\n",
    "        print(f\"  TARGETID {row['TARGETID']}: Old weight {row['WEIGHT_old']:.3f}, New weight {row['WEIGHT_new']:.3f}, Diff: {row['weight_diff_pct']:.1f}%\")\n",
    "\n",
    "# Show random sample from merged data\n",
    "print(f\"\\nRandom sample from merged data:\")\n",
    "sample_merged = merged_df.sample(min(10, len(merged_df)))\n",
    "for _, row in sample_merged.iterrows():\n",
    "    print(f\"TARGETID {row['TARGETID']}: Weight diff: {row['weight_diff_pct']:.1f}%, AbsMag diff: {row['absmag_diff_pct']:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View Imaging Systematics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = f'BGS_BRIGHT_R-19-20_SF_clustering.dat.fits'\n",
    "path = os.path.join(CUSTOM_CLUSTERING_RESULTS_FOLDER_NEW, 'Y1', 'LSS', 'iron', 'LSScats', 'v1.5pip', '19-20_SF_i1', fn)\n",
    "tbl = Table(fitsio.read(path))\n",
    "print(tbl.colnames)\n",
    "junk=plt.hist(tbl['WEIGHT_SYS'], bins=100, range=(0.5,1.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mag in magbins[:-1]: \n",
    "    for q in ['Q', 'SF']:\n",
    "        for region in ['N', 'S']:\n",
    "            fn = f'BGS_BRIGHT_R-{abs(mag)}-{abs(mag)+1}_{q}_{region}_0.0010.5_linclusimsysfit.png'\n",
    "            path = os.path.join(CUSTOM_CLUSTERING_RESULTS_FOLDER_NEW, 'Y1', 'LSS', 'iron', 'LSScats', 'v1.5pip', f'{abs(mag)}-{abs(mag)+1}_{q}_i1', fn)\n",
    "            if os.path.exists(path):\n",
    "                plt.figure(figsize=(10, 10))\n",
    "                img = plt.imread(path)\n",
    "                plt.imshow(img)\n",
    "                plt.axis('off') \n",
    "                plt.title(f'{abs(mag)}-{abs(mag)+1}mag {q} Imaging Sys Fit ({region})')\n",
    "                plt.show()\n",
    "                \n",
    "                # Also print off the .txt file. Same thing but _linfitparam.txt\n",
    "                if (False):\n",
    "                    txt_fn = path.replace('_linclusimsysfit.png', '_linfitparam.txt')\n",
    "                    if os.path.exists(txt_fn):\n",
    "                        with open(txt_fn, 'r') as f:\n",
    "                            print(f.read())\n",
    "                    else:\n",
    "                        print(\"No text file found for\", txt_fn)\n",
    "\n",
    "            # Load catalog and compare imaging systematics to old ones\n",
    "            fn = f'BGS_BRIGHT_R-{abs(mag)}-{abs(mag)+1}_{q}_{region}GC_clustering.dat.fits'\n",
    "            path = os.path.join(CUSTOM_CLUSTERING_RESULTS_FOLDER_NEW, 'Y1', 'LSS', 'iron', 'LSScats', 'v1.5pip', f'{abs(mag)}-{abs(mag)+1}_{q}_i1', fn)\n",
    "            if os.path.exists(path):\n",
    "                tbl = Table(fitsio.read(path))\n",
    "                tbl.remove_column('BITWEIGHTS')\n",
    "                tbl = tbl.to_pandas()\n",
    "                print(f\"Loaded catalog {abs(mag)}-{abs(mag)+1} {q} {region}GC\")\n",
    "                tbl['SYS_CHANGE'] = (tbl['WEIGHT_SYS_OLD'] - tbl['WEIGHT_SYS']) * 100 / tbl['WEIGHT_SYS']\n",
    "                plt.figure(figsize=(10, 5))\n",
    "                plt.hist(tbl['WEIGHT_SYS_OLD'], bins=100, alpha=0.5, label='Old WEIGHT_SYS', range=(0.5, 1.5))\n",
    "                plt.hist(tbl['WEIGHT_SYS'], bins=100, alpha=0.5, label='New WEIGHT_SYS', range=(0.5, 1.5))\n",
    "                plt.yscale('log')\n",
    "                plt.xlabel('Weight')\n",
    "                plt.ylabel('Count')\n",
    "                plt.title(f'Weight Distribution: {abs(mag)}-{abs(mag)+1} {q} {region}GC')\n",
    "                plt.legend()\n",
    "                plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "re.sub(r'_[A-Z]-[0-9]+-[0-9]+_[a-zA-Z]+_', '_', 'BGS_BRIGHT_R-19-20_Q_full_HPmapcut.dat.fits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "baseclone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
