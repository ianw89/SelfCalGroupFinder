{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import astropy.coordinates as coord\n",
    "import astropy.units as u\n",
    "import emcee\n",
    "import sys\n",
    "from scipy.special import factorial\n",
    "from astropy.table import Table\n",
    "from numpy.polynomial.polynomial import Polynomial\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "if './SelfCalGroupFinder/py/' not in sys.path:\n",
    "    sys.path.append('./SelfCalGroupFinder/py/')\n",
    "from pyutils import *\n",
    "import plotting as pp\n",
    "from dataloc import *\n",
    "from bgs_helpers import *\n",
    "import catalog_definitions as cat\n",
    "from groupcatalog import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the group finder is run, this notebook is used to postprocess the results, generating plots and such for analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading existing datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mxxl_all=deserialize(cat.mxxl_all)\n",
    "#mxxl_fiberonly=deserialize(cat.mxxl_fiberonly)\n",
    "#mxxl_nn=deserialize(cat.mxxl_nn)\n",
    "#mxxl_simple_4=deserialize(cat.mxxl_simple_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdss_vanilla_v2 = deserialize(cat.sdss_vanilla_v2)\n",
    "sdss_colors_v2 = deserialize(cat.sdss_colors_v2)\n",
    "sdss_colors_chi_v2 = deserialize(cat.sdss_colors_chi_v2)\n",
    "sdss_vanilla_v1 = deserialize(cat.sdss_vanilla)\n",
    "sdss_colors_v1 = deserialize(cat.sdss_colors)\n",
    "sdss_colors_chi_v1 = deserialize(cat.sdss_colors_chi)\n",
    "\n",
    "sdss_bgscut = deserialize(cat.sdss_bgscut)\n",
    "\n",
    "cat.sdss_published.postprocess()\n",
    "sdss_published = cat.sdss_published # It really is ~exactly sdss_colors_chi, which is great news for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bgs_y1_pzp_2_4 = deserialize(cat.bgs_y1_pzp_2_4)\n",
    "bgs_y1_pzp_2_4_c2 = deserialize(cat.bgs_y1_pzp_2_4_c2)\n",
    "bgs_y1_pzp_2_4_c2_serial = deserialize(cat.bgs_y1_pzp_2_4_c2_serial)\n",
    "bgs_y1_pzp_2_4_c2_withrc = deserialize(cat.bgs_y1_pzp_2_4_c2_withrc)\n",
    "#bgs_y1_pzp_2_4_c2_flm2 = deserialize(cat.bgs_y1_pzp_2_4_c2_flm2) # old fluxlim correction model\n",
    "#bgs_y1_pzp_2_4_c2_flm0 = deserialize(cat.bgs_y1_pzp_2_4_c2_flm0) # no fluxlim correction model\n",
    "#bgs_y1_pzp_2_4_c3 = deserialize(cat.bgs_y1_pzp_2_4_c3)\n",
    "#bgs_y1mini_pzp_2_4_c1 = deserialize(cat.bgs_y1mini_pzp_2_4_c1)\n",
    "#bgs_y1_hybrid8_mcmc = deserialize(cat.bgs_y1_hybrid8_mcmc)\n",
    "#bgs_y3_pzp_2_4 = deserialize(cat.bgs_y3_pzp_2_4)\n",
    "#bgs_y3_pzp_2_4_c2 = deserialize(cat.bgs_y3_pzp_2_4_c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our best source of BGS Truth\n",
    "bgs_sv3_pz_2_4_10p = deserialize(cat.bgs_sv3_pz_2_4_10p) \n",
    "bgs_sv3_pz_2_4_10p_c1 = deserialize(cat.bgs_sv3_pz_2_4_10p_c1) \n",
    "bgs_sv3_pz_2_4_10p_c2 = deserialize(cat.bgs_sv3_pz_2_4_10p_c2) \n",
    "bgs_sv3_fiberonly_10p = deserialize(cat.bgs_sv3_fiberonly_10p)\n",
    "#bgs_sv3_nn_10p = deserialize(cat.bgs_sv3_nn_10p)\n",
    "\n",
    "# These ones are the best way to compare main survey to SV3 for fiber incompleteness study\n",
    "bgs_y3_like_sv3_fiberonly = deserialize(cat.bgs_y3_like_sv3_fiberonly)\n",
    "bgs_y3_like_sv3_pz_2_4 = deserialize(cat.bgs_y3_like_sv3_pz_2_4)\n",
    "bgs_y3_like_sv3_pz_2_4_c1 = deserialize(cat.bgs_y3_like_sv3_pz_2_4_c1)\n",
    "bgs_y3_like_sv3_pz_2_4_c2 = deserialize(cat.bgs_y3_like_sv3_pz_2_4_c2)\n",
    "bgs_y3_like_sv3_pz_2_0 = deserialize(cat.bgs_y3_like_sv3_pz_2_0)\n",
    "bgs_y3_like_sv3_nn = deserialize(cat.bgs_y3_like_sv3_nn)\n",
    "\n",
    "#bgs_sv3_10p_mcmc = deserialize(cat.bgs_sv3_10p_mcmc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat.bgs_y3_like_sv3_pz_2_6.run_group_finder()\n",
    "cat.bgs_y3_like_sv3_pz_2_6.postprocess()\n",
    "cat.bgs_y3_like_sv3_pz_2_6.dump()\n",
    "bgs_y3_like_sv3_pz_2_6 = deserialize(cat.bgs_y3_like_sv3_pz_2_6)                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat.bgs_y3_like_sv3_pz_4_0.run_group_finder()\n",
    "cat.bgs_y3_like_sv3_pz_4_0.postprocess()\n",
    "cat.bgs_y3_like_sv3_pz_4_0.dump()\n",
    "bgs_y3_like_sv3_pz_4_0 = deserialize(cat.bgs_y3_like_sv3_pz_4_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_u = deserialize(cat.uchuu_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.cosmology import FlatLambdaCDM\n",
    "\n",
    "df = bgs_y1_pzp_2_4_c3.all_data\n",
    "\n",
    "# Create a dataframe with central galaxy properties, indexed by IGRP\n",
    "centrals = df.loc[~df['IS_SAT']].set_index('IGRP')\n",
    "cen_props = centrals[['RA', 'DEC', 'Z', 'M_HALO']].rename(columns=lambda x: x + '_cen')\n",
    "\n",
    "# Merge central properties into the main dataframe based on IGRP\n",
    "df = df.merge(cen_props, left_on='IGRP', right_index=True, how='left')\n",
    "\n",
    "sats = df.loc[df['IS_SAT']].copy()\n",
    "\n",
    "cosmo = FlatLambdaCDM(H0=100, Om0=0.3) # Using h=1 units consistent with M_HALO\n",
    "cen_dist = cosmo.comoving_distance(sats['Z_cen'].values)\n",
    "\n",
    "# Create SkyCoord objects\n",
    "sat_coords = coord.SkyCoord(ra=sats['RA'].values*u.deg, dec=sats['DEC'].values*u.deg)\n",
    "cen_coords = coord.SkyCoord(ra=sats['RA_cen'].values*u.deg, dec=sats['DEC_cen'].values*u.deg)\n",
    "\n",
    "# Calculate angular separation\n",
    "angular_sep = sat_coords.separation(cen_coords)\n",
    "\n",
    "# Calculate projected separation and store it in the main dataframe\n",
    "# For small angles, projected distance is angular_separation_in_radians * comoving_distance\n",
    "df.loc[df['IS_SAT'], 'dist_to_cen_mpc'] = (angular_sep.to(u.rad).value * cen_dist).to(u.Mpc).value\n",
    "\n",
    "# Calculate R200m for each central\n",
    "# R200m = (3 * M_halo / (4 * pi * 200 * rho_crit))^(1/3)\n",
    "# Using h=1 cosmology, rho_crit is in (M_sun/h) / (Mpc/h)^3\n",
    "rho_crit = cosmo.critical_density(0).to(u.Msun / u.Mpc**3).value # Value is now in (M_sun/h) / (Mpc/h)^3\n",
    "df['R200m'] = (3 * df['M_HALO_cen'] / (4 * np.pi * 200 * rho_crit))**(1/3)\n",
    "\n",
    "# Calculate the normalized distance for satellites\n",
    "df['dist_norm'] = df['dist_to_cen_mpc'] / df['R200m']\n",
    "\n",
    "# Plot the distribution of normalized satellite distances\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(df.loc[df['IS_SAT'], 'dist_norm'].dropna(), bins=100, range=(0, 3), histtype='step', linewidth=2)\n",
    "plt.xlabel('Satellite Projected Distance / $R_{200m}$')\n",
    "plt.ylabel('Number of Satellites')\n",
    "plt.yscale('log')\n",
    "plt.title('Distribution of Normalized Satellite Projected Distances')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MCMC of z assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best preprocess photo-z-plus v2 result for SV3\n",
    "path = f'/mount/sirocco1/imw2293/GROUP_CAT/mcmc13_m4_2_4.h5'\n",
    "reader = emcee.backends.HDFBackend(path, read_only=True)\n",
    "bgs_sv3_pz2_mcmcbest = BGSGroupCatalog.from_ZASSIGN_MCMC(reader, Mode.PHOTOZ_PLUS_v2)\n",
    "bgs_sv3_pz2_mcmcbest.run_group_finder(popmock=False)\n",
    "bgs_sv3_pz2_mcmcbest.postprocess()\n",
    "serialize(bgs_sv3_pz2_mcmcbest)\n",
    "\n",
    "# 64.94% neighbor\n",
    "#[6.32102907 1.59813654 1.49851461 3.18966963 0.83098626 2.83411711 3.26649451 1.75386219 1.95862571 2.56928697 0.91387857 1.70360255 3.6181996 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best preprocess photo-z-plus v3 result for SV3\n",
    "path = f'/mount/sirocco1/imw2293/GROUP_CAT/mcmc13_m4_3_1.h5'\n",
    "reader = emcee.backends.HDFBackend(path, read_only=True)\n",
    "bgs_sv3_pz3_mcmcbest = BGSGroupCatalog.from_ZASSIGN_MCMC(reader, Mode.PHOTOZ_PLUS_v3)\n",
    "bgs_sv3_pz3_mcmcbest.run_group_finder(popmock=False)\n",
    "bgs_sv3_pz3_mcmcbest.postprocess()\n",
    "serialize(bgs_sv3_pz3_mcmcbest)\n",
    "\n",
    "# 37% Neighbor for this, not bad\n",
    "#[3.68776334 1.03045493 1.00947751 2.79354858 0.88263756 1.15014321 2.71197235 0.63517952 1.44684275 2.63171751 1.16820625 0.96790557 3.02351026]\n",
    "\n",
    "# This one is great at 63.13% Neighbor.\n",
    "# [8.26010114 1.29383299 1.54671643 3.01349293 1.2229046  0.86286149 2.58828658 0.87067123 0.61260216 2.44470607 1.11635435 1.29386183 3.16506802]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best preprocess photo-z-plus v2 result for SV3\n",
    "# TODO DOESN'T WORK\n",
    "#path = f'/mount/sirocco1/imw2293/GROUP_CAT/mcmc13_m4_2_6.h5'\n",
    "#reader = emcee.backends.HDFBackend(path, read_only=True)\n",
    "#bgs_sv3_new_mcmcbest = BGSGroupCatalog.from_MCMC(reader, Mode.PHOTOZ_PLUS_v2)\n",
    "#bgs_sv3_new_mcmcbest.run_group_finder(popmock=False)\n",
    "#bgs_sv3_new_mcmcbest.postprocess()\n",
    "#serialize(bgs_sv3_new_mcmcbest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case MCMC has been dumb, check similar parameter values\n",
    "import copy\n",
    "bb, rb, br, rr = bgs_sv3_pz3_mcmcbest.extra_params[1:13].reshape(4, 3)\n",
    "\n",
    "params = [bb, rb, br, rr]\n",
    "colors = [[0, 0, 1.0], [1.0, 0, 0.4], [0.2, 0.7, 0.2], [1.0, 0.0, 0.0]]\n",
    "variants = []\n",
    "\n",
    "for i, (param, color) in enumerate(zip(params, colors), start=1):\n",
    "    variant = BGSGroupCatalog(\n",
    "        f\"PZP 3 Variant {i}\",\n",
    "        bgs_sv3_pz3_mcmcbest.mode,\n",
    "        bgs_sv3_pz3_mcmcbest.mag_cut,\n",
    "        bgs_sv3_pz3_mcmcbest.catalog_mag_cut,\n",
    "        bgs_sv3_pz3_mcmcbest.sdss_fill,\n",
    "        bgs_sv3_pz3_mcmcbest.num_passes,\n",
    "        bgs_sv3_pz3_mcmcbest.drop_passes,\n",
    "        bgs_sv3_pz3_mcmcbest.data_cut,\n",
    "        bgs_sv3_pz3_mcmcbest.extra_params\n",
    "    )\n",
    "    variant.extra_params = [bgs_sv3_pz3_mcmcbest.extra_params[0], param, param, param, param]\n",
    "    variant.color = color\n",
    "    variant.preprocess()\n",
    "    variant.run_group_finder(popmock=False)\n",
    "    variant.postprocess()\n",
    "    variants.append(variant)\n",
    "\n",
    "bgs_sv3_pz3_mcmcbest_var1, bgs_sv3_pz3_mcmcbest_var2, bgs_sv3_pz3_mcmcbest_var3, bgs_sv3_pz3_mcmcbest_var4 = variants\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Release Candidate Catalogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sdss_published.sanity_tests() # The published catalog has some issues\n",
    "sdss_vanilla_v2.sanity_tests()\n",
    "sdss_colors_v2.sanity_tests()\n",
    "#sdss_colors_chi_v2.sanity_tests()\n",
    "\n",
    "sdss_vanilla_v1.sanity_tests()\n",
    "sdss_colors_v1.sanity_tests()\n",
    "#sdss_colors_chi_v1.sanity_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bgs_sv3_pz_2_4_10p_c2.basic_stats()\n",
    "bgs_sv3_pz_2_4_10p_c2.sanity_tests()\n",
    "bgs_y3_like_sv3_pz_2_4_c2.basic_stats()\n",
    "bgs_y3_like_sv3_pz_2_4_c2.sanity_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bgs_y1_pzp_2_4_c2.basic_stats()\n",
    "bgs_y1_pzp_2_4_c2.sanity_tests()\n",
    "bgs_y1_pzp_2_4_c2_serial.basic_stats()\n",
    "bgs_y1_pzp_2_4_c2_serial.sanity_tests()\n",
    "bgs_y1_pzp_2_4_c2_withrc.basic_stats()\n",
    "bgs_y1_pzp_2_4_c2_withrc.sanity_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bgs_y1_pzp_2_4_c2.basic_stats()\n",
    "bgs_y1_pzp_2_4_c2.sanity_tests()\n",
    "bgs_y3_pzp_2_4_c2.basic_stats()\n",
    "bgs_y3_pzp_2_4_c2.sanity_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bgs_y1_pzp_2_4.color = 'k'\n",
    "bgs_y1_pzp_2_4.name = 'BGS Y1 Basic'\n",
    "bgs_y1_hybrid8_mcmc.marker = '-'\n",
    "bgs_y1_hybrid8_mcmc.name = 'BGS Y1 Self-Calibrated'\n",
    "pp.plots(bgs_y1_pzp_2_4, bgs_y1_hybrid8_mcmc)\n",
    "pp.proj_clustering_plot(bgs_y1_pzp_2_4)\n",
    "pp.lsat_data_compare_plot(bgs_y1_pzp_2_4)\n",
    "pp.proj_clustering_plot(bgs_y1_hybrid8_mcmc)\n",
    "pp.lsat_data_compare_plot(bgs_y1_hybrid8_mcmc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.single_plots(bgs_y1_hybrid8_mcmc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bgs_y1_pzp_2_4_c1.name = \"BGS Y1\"\n",
    "bgs_y1_pzp_2_4_c1.color = 'k'\n",
    "bgs_y1_pzp_2_4_c1.marker = '-'\n",
    "sdss_published.name = \"SDSS\"\n",
    "pp.plots(bgs_y1_pzp_2_4_c1, sdss_published)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.qf_cen_plot(bgs_sv3_pz_2_4_10p, test_methods=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bgs_y1_pzp_2_4.all_data.sort_values('L_GAL', ascending=False).loc[:, ['RA', 'DEC', 'Z', 'Z_PHOT', 'Z_ASSIGNED_FLAG']].head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.single_plots(bgs_y3_pzp_2_4_c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.plots(bgs_y3_pzp_2_4_c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.plots(sdss_colors_chi_v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fiber Incompleteness Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SV3 'Truth' Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bgs_sv3_pz_2_4_10p.calculate_projected_clustering(with_extra_randoms=True)\n",
    "#bgs_sv3_pz_2_4_10p.calculate_projected_clustering_in_magbins(with_extra_randoms=True)\n",
    "\n",
    "# TODO run this again\n",
    "#bgs_sv3_pz_2_4_10p.add_jackknife_err_to_proj_clustering(with_extra_randoms=True, for_mag_bins=False)\n",
    "#bgs_sv3_pz_2_4_10p.add_jackknife_err_to_proj_clustering(with_extra_randoms=False, for_mag_bins=True) # BUG Broken\n",
    "#serialize(bgs_sv3_pz_2_4_10p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.printoptions(precision=2, suppress=True, linewidth=200):\n",
    "    print(bgs_sv3_pz_2_4_10p.wp_all_extra[1])\n",
    "    print(bgs_sv3_pz_2_4_10p.wp_err)\n",
    "\n",
    "    std_devs = np.sqrt(np.diag(bgs_sv3_pz_2_4_10p.wp_cov))\n",
    "    print(std_devs)\n",
    "\n",
    "    print(bgs_sv3_pz_2_4_10p.wp_cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question - does using the small set of randoms vs the full set of randoms make a difference?\n",
    "percent_diff = np.abs(bgs_sv3_pz_2_4_10p.wp_all[1] - bgs_sv3_pz_2_4_10p.wp_all_extra[1]) / bgs_sv3_pz_2_4_10p.wp_all[1] * 100\n",
    "print(percent_diff)\n",
    "red_p_diff = np.abs(bgs_sv3_pz_2_4_10p.wp_all[2] - bgs_sv3_pz_2_4_10p.wp_all_extra[2]) / bgs_sv3_pz_2_4_10p.wp_all[2] * 100\n",
    "print(red_p_diff)\n",
    "blue_p_diff = np.abs(bgs_sv3_pz_2_4_10p.wp_all[3] - bgs_sv3_pz_2_4_10p.wp_all_extra[3]) / bgs_sv3_pz_2_4_10p.wp_all[3] * 100\n",
    "print(blue_p_diff)\n",
    "# Answer - Less than 1% generally\n",
    "\n",
    "colors = ['k', 'r', 'b']\n",
    "f = bgs_sv3_pz_2_4_10p\n",
    "plt.figure(figsize=(5, 5))\n",
    "if f.wp_err is not None:\n",
    "    plt.errorbar(f.wp_all[0][:-1], f.wp_all[1], yerr=f.wp_err, marker='o', linestyle='-', label='All', color=colors[0], alpha=0.5)\n",
    "    plt.errorbar(f.wp_all[0][:-1], f.wp_all[2], yerr=f.wp_r_err, marker='o', linestyle='-', label='Red', color=colors[1], alpha=0.5)\n",
    "    plt.errorbar(f.wp_all[0][:-1], f.wp_all[3], yerr=f.wp_b_err, marker='o', linestyle='-', label='Blue', color=colors[2], alpha=0.5)\n",
    "else:\n",
    "    plt.plot(f.wp_all[0][:-1], f.wp_all[1], marker='o', linestyle='-', label='All', color=colors[0])\n",
    "    plt.plot(f.wp_all[0][:-1], f.wp_all[2], marker='o', linestyle='-', label='Red', color=colors[1])\n",
    "    plt.plot(f.wp_all[0][:-1], f.wp_all[3], marker='o', linestyle='-', label='Blue', color=colors[2])\n",
    "\n",
    "plt.plot(f.wp_all_extra[0][:-1], f.wp_all_extra[1], marker='x', linestyle='--', label='All Extra Rands', color=colors[0])\n",
    "plt.plot(f.wp_all_extra[0][:-1], f.wp_all_extra[2], marker='x', linestyle='--', label='Red Extra Rands', color=colors[1])\n",
    "plt.plot(f.wp_all_extra[0][:-1], f.wp_all_extra[3], marker='x', linestyle='--', label='Blue Extra Rands', color=colors[2])\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.ylim(8, 2000)\n",
    "plt.yscale('log')\n",
    "plt.xlabel(r'$r_p$ [Mpc/h]')\n",
    "plt.ylabel(r'$w_p(r_p)$')\n",
    "plt.legend()\n",
    "plt.title('Full Sample $w_p(r_p)$ ')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sets = [bgs_y3_like_sv3_fiberonly, bgs_y3_like_sv3_nn, bgs_y3_like_sv3_pz_2_0, bgs_y3_like_sv3_pz_2_4] \n",
    "#sets = [bgs_y3_like_sv3_pz_2_4, bgs_y3_like_sv3_pz_2_6] \n",
    "#sets = [bgs_y3_like_sv3_pz_2_4_c1]\n",
    "\n",
    "#bgs_y3_like_sv3_pz_4_0.name = \"PZP Lorentzian\"\n",
    "#bgs_y3_like_sv3_pz_2_6.name = \"New Parameters\"\n",
    "bgs_y3_like_sv3_pz_2_4.name = \"New Technique\"\n",
    "bgs_y3_like_sv3_pz_2_4_c1.name = \"New Technique\"\n",
    "bgs_y3_like_sv3_nn.name = \"Nearest Neighbor\"\n",
    "bgs_y3_like_sv3_pz_2_0.name = \"Photo-z\"\n",
    "bgs_y3_like_sv3_fiberonly.name = \"Drop Lost Galaxies\"\n",
    "bgs_sv3_pz_2_4_10p.name = \"SV3 ~Truth\"\n",
    "bgs_sv3_pz_2_4_10p_c1.name = \"SV3 ~Truth C1\"\n",
    "bgs_sv3_pz_2_4_10p.color = 'k'\n",
    "bgs_sv3_pz_2_4_10p_c1.color = 'k'\n",
    "\n",
    "#for s in [bgs_sv3_pz_2_4_10p_c1, *sets]:\n",
    "#    print(f\"--- {s.name} ---\")\n",
    "#    print(f\"Has extra randoms: {s.wp_all_extra is not None}\")\n",
    "#    # Check if s.wp_slices_extra is None or amy it's slices are None\n",
    "#    flag = s.wp_slices_extra !=  np.repeat(None, len(s.wp_slices_extra))\n",
    "#    print(f\"Has extra randoms for slices: {flag}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.plots(*sets, show_err=bgs_sv3_pz_2_4_10p)\n",
    "#pp.LHMR_plots_logerr(*sets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth_catalog = bgs_sv3_pz_2_4_10p\n",
    "truth_df = truth_catalog.all_data\n",
    "for s in sets:\n",
    "    print(s.name)\n",
    "    s.get_true_z_from(truth_df)\n",
    "    s.refresh_df_views()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO debug all this\n",
    "pp.Lfunc_compare(bgs_y3_like_sv3_fiberonly, bgs_sv3_pz_2_4_10p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.correct_redshifts_assigned_plot(bgs_y3_like_sv3_nn, bgs_y3_like_sv3_pz_2_0, bgs_y3_like_sv3_pz_2_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.luminosity_function_plots(bgs_y3_like_sv3_nn, bgs_y3_like_sv3_pz_2_0, bgs_y3_like_sv3_pz_2_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.single_plots(bgs_y3_like_sv3_pz_2_4)\n",
    "#pp.single_plots(bgs_y3_like_sv3_pz_2_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in sets:\n",
    "    data = s.all_data.loc[z_flag_is_not_spectro_z(s.all_data['Z_ASSIGNED_FLAG'])]\n",
    "    delta_red = data['Z'] - data['Z_T'] # I used to do z_obs for SV3 dropping passes... TODO\n",
    "    plt.hist(delta_red, bins=100, range=(-0.05, 0.05), histtype='step', label=s.name, color=s.color)\n",
    "    plt.yscale('log')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Targets in the 'truth' catalog that are NOT IN the other one aren't accounted for in the purity and completeness calculations.\n",
    "# This becomes obvious issue for the fiberonly runs, so don't use those here.\n",
    "psets = [bgs_y3_like_sv3_nn, bgs_y3_like_sv3_pz_2_0, bgs_y3_like_sv3_pz_2_4] \n",
    "pp.test_purity_and_completeness(*psets, truth_catalog=truth_catalog, lost_only=False)\n",
    "pp.purity_complete_plots(*psets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a bar plot of the z_assigned_flag values for each set\n",
    "for s in sets:\n",
    "    j=plt.hist(s.all_data['Z_ASSIGNED_FLAG'], bins=[-3,-2,-1,0,1,2,3,4,5,6,7,8,9,10,11,12], histtype='step', label=s.name)\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.ylim(0, 100000)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in sets:\n",
    "    pp.proj_clustering_plot(s)\n",
    "\n",
    "for s in sets:\n",
    "    pp.lsat_data_compare_plot(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv3_pip_bad = pickle.load(open(OUTPUT_FOLDER + 'sv3_pip_clustering.pkl', 'rb'))\n",
    "sv3_pip_clustering = pickle.load(open(OUTPUT_FOLDER + 'sv3_pip_clustering_proper.pkl', 'rb'))\n",
    "y3_likesv3_clustering = pickle.load(open(OUTPUT_FOLDER + 'y3_likesv3_pip_clustering_proper.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Clustering from processed ones to ~Truth\n",
    "for s in sets:\n",
    "    pp.wp_rp(s)\n",
    "    pp.compare_wp_rp(s, bgs_sv3_pz_2_4_10p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.wp_rp(bgs_sv3_pz_2_4_10p)\n",
    "pp.wp_rp(sv3_pip_bad)\n",
    "pp.wp_rp(sv3_pip_clustering)\n",
    "pp.wp_rp(y3_likesv3_clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load up SV3 PIP Clustering as created by the LS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does using randoms from full vs clustering with weights matter?\n",
    "pp.compare_wp_rp(sv3_pip_bad, sv3_pip_clustering)\n",
    "# BUG how is overall not average of red and blue?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This plot really should be close to 0% difference\n",
    "# It's SV3 10p, so 2% filled with my method, compared to the PIP method\n",
    "pp.compare_wp_rp(bgs_sv3_pz_2_4_10p, sv3_pip_clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If LSS team is calculating PIP weights right and I'm doing nothing wrong,\n",
    "# then these two should also largely agree\n",
    "pp.compare_wp_rp(y3_likesv3_clustering, sv3_pip_clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rough check that the footprints all match up\n",
    "for s in sets:\n",
    "    fig=pp.make_map(s.all_data['RA'].to_numpy(), s.all_data['DEC'].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fluxlim Correction Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sets = [bgs_y1_pzp_2_4_c2_flm0, bgs_y1_pzp_2_4_c2_flm2, bgs_y1_pzp_2_4_c2]\n",
    "bgs_y1_pzp_2_4_c2_flm2.name = \"Weak Correction\"\n",
    "bgs_y1_pzp_2_4_c2_flm2.color = 'orange'\n",
    "bgs_y1_pzp_2_4_c2_flm0.name = \"No Correction\"\n",
    "bgs_y1_pzp_2_4_c2_flm0.color = 'red'\n",
    "bgs_y1_pzp_2_4_c2.name = \"Full Correction\"\n",
    "bgs_y1_pzp_2_4_c2.color = 'black'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define redshift and luminosity bins for the analysis\n",
    "z_bins = np.linspace(0.001, 0.5, 12)\n",
    "z_mids = (z_bins[:-1] + z_bins[1:]) / 2\n",
    "\n",
    "# Select a few luminosity bins to examine, using indices from LogLgal_labels\n",
    "l_gal_indices = [22,24,26,27,28,29,30,31] \n",
    "l_gal_labels_to_plot = [LogLgal_labels[i] for i in l_gal_indices]\n",
    "\n",
    "# Create a figure with subplots for each luminosity bin\n",
    "fig, axes = plt.subplots(2, len(l_gal_indices)//2, figsize=(24, 12))\n",
    "\n",
    "# Iterate over the selected luminosity bins and corresponding axes\n",
    "for i, (l_gal_idx, ax) in enumerate(zip(l_gal_indices, axes.flatten())):\n",
    "    l_gal_val = L_gal_labels[l_gal_idx]\n",
    "    \n",
    "    # Iterate over each dataset (fluxlim model)\n",
    "    for s in sets:\n",
    "        df = s.centrals.loc[z_flag_is_spectro_z(s.centrals['Z_ASSIGNED_FLAG'])]\n",
    "        mean_mhalo_values = []\n",
    "        \n",
    "        # Filter for central galaxies in the current luminosity bin\n",
    "        centrals_in_lbin = df[df['LGAL_BIN'] == l_gal_val]\n",
    "\n",
    "        # Calculate the mean halo mass in each redshift bin\n",
    "        for j in range(len(z_bins) - 1):\n",
    "            z_low, z_high = z_bins[j], z_bins[j+1]\n",
    "            \n",
    "            # Filter by redshift\n",
    "            z_mask = (centrals_in_lbin['Z'] >= z_low) & (centrals_in_lbin['Z'] < z_high)\n",
    "            \n",
    "            if np.sum(z_mask) > 19: # Require a minimum number of galaxies\n",
    "                mean_mhalo = centrals_in_lbin.loc[z_mask, 'M_HALO'].mean()\n",
    "                mean_mhalo_values.append(mean_mhalo)\n",
    "            else:\n",
    "                mean_mhalo_values.append(np.nan)\n",
    "        \n",
    "        # Plot M_halo vs. z for the current luminosity and model on the specific subplot\n",
    "        ax.plot(z_mids, mean_mhalo_values, marker='o', color=s.color, linestyle='-', label=s.name, alpha=0.7)\n",
    "\n",
    "    ax.set_yscale('log')\n",
    "    ax.set_xlabel('z')\n",
    "    ax.set_title(f'log(L)={l_gal_labels_to_plot[i]:.2f}')\n",
    "    ax.grid(True, which=\"both\", ls=\"--\")\n",
    "    ax.legend()\n",
    "\n",
    "axes.flatten()[0].set_ylabel('<$M_h$> [$M_\\odot$/h]')\n",
    "axes.flatten()[4].set_ylabel('<$M_h$> [$M_\\odot$/h]')\n",
    "\n",
    "# Adjust layout to prevent titles from overlapping\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Race Condition and Parallel Bugs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all galaxies who do not have same CEN/SAT assignment in bgs_y1_pzp_2_4_c2 vs bgs_y1_pzp_2_4_c2_serial\n",
    "df1 = bgs_y1_pzp_2_4_c2.all_data.set_index('TARGETID')\n",
    "df2 = bgs_y1_pzp_2_4_c2_serial.all_data.set_index('TARGETID') # Best one\n",
    "df3 = bgs_y1_pzp_2_4_c2_withrc.all_data.set_index('TARGETID')\n",
    "diff_cen_sat = df2['IS_SAT'] != df1['IS_SAT']\n",
    "diff_cen_sat_galaxies = df2[diff_cen_sat]\n",
    "diff_cen_sat_withrc = df2['IS_SAT'] != df3['IS_SAT']\n",
    "diff_cen_sat_withrc_galaxies = df2[diff_cen_sat_withrc]\n",
    "diff_cen_sat_parparwrc = df3['IS_SAT'] != df1['IS_SAT']\n",
    "diff_cen_sat_parparwrc_galaxies = df3[diff_cen_sat_parparwrc]\n",
    "\n",
    "print(f\"Number of galaxies with different CEN/SAT assignment: {diff_cen_sat.sum()} of {len(df1)} total galaxies\")\n",
    "print(f\"Number of galaxies with different CEN/SAT assignment (with rc): {diff_cen_sat_withrc.sum()} of {len(df1)} total galaxies\")\n",
    "print(f\"Number of galaxies with different CEN/SAT assignment (par vs with rc): {diff_cen_sat_parparwrc.sum()} of {len(df1)} total galaxies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.proj_clustering_plot(bgs_y1_pzp_2_4_c2)\n",
    "pp.proj_clustering_plot(bgs_y1_pzp_2_4_c2_serial)\n",
    "pp.proj_clustering_plot(bgs_y1_pzp_2_4_c2_withrc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### App mag limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.cosmology import FlatLambdaCDM\n",
    "\n",
    "# Cosmology parameters (example values, adjust as needed)\n",
    "cosmo = FlatLambdaCDM(H0=70, Om0=0.3)\n",
    "\n",
    "flux_limit = 24.0  # apparent magnitude limit\n",
    "limit2 = 23.4\n",
    "limit3 = 22.0\n",
    "z_vals = np.linspace(0.01, 0.5, 200)\n",
    "\n",
    "mag_limits = app_mag_to_abs_mag(flux_limit, z_vals)\n",
    "mag_limits2 = app_mag_to_abs_mag(limit2, z_vals)\n",
    "mag_limits3 = app_mag_to_abs_mag(limit3, z_vals)\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.plot(z_vals, mag_limits, label=f'Flux limit = {flux_limit}')\n",
    "plt.plot(z_vals, mag_limits2, label=f'Flux limit = {limit2}', linestyle='--')\n",
    "plt.plot(z_vals, mag_limits3, label=f'Flux limit = {limit3}', linestyle=':')    \n",
    "# flip y axis direction\n",
    "plt.gca().invert_yaxis()\n",
    "plt.yticks(np.arange(-20, -7, 2))\n",
    "plt.xlabel('Redshift (z)')\n",
    "plt.ylabel('M_r - 5log10(h)')\n",
    "plt.title('Magnitude Limit vs Redshift')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convergence of Number of Bootstrap Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Number of Bootstrap Iterations\n",
    "# For non vmax weighted fsat I find 300 is converged to 10% w.r.t. 3000.\n",
    "# For non vmax weighted fsat I find 1000 is converged to 5% w.r.t. 3000.\n",
    "x = [30,100,300,1000,3000,10000]\n",
    "fsat_r_std = []\n",
    "fsat_b_std = []\n",
    "\n",
    "for n in x:\n",
    "    print(f\"n={n}\")\n",
    "    bgs_y1_pzp_2_4_c2.add_bootstrapped_f_sat(N_ITERATIONS=n)\n",
    "    fsat_r_std.append(np.copy(bgs_y1_pzp_2_4_c2.fsat_q_bootstrap_err))\n",
    "    fsat_b_std.append(np.copy(bgs_y1_pzp_2_4_c2.fsat_sf_bootstrap_err))\n",
    "    \n",
    "fsat_r_std=np.array(fsat_r_std)\n",
    "fsat_b_std=np.array(fsat_b_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "for i in [10,15,20,25,28,31]:\n",
    "    plt.figure()\n",
    "    plt.plot(x, fsat_r_std[:,i], color='r', marker='o', label='Quiescent')\n",
    "    plt.plot(x, fsat_b_std[:,i], color='b', marker='o', label='Star-forming')\n",
    "    plt.xscale('log')\n",
    "    plt.xlabel('Number of Bootstrap Iterations')\n",
    "    plt.ylabel('Standard Deviation of $f_{sat}$')\n",
    "    plt.title(f'Bin {i+1}: {LogLgal_labels[i]:.2f} $L_{{\\odot}}/h^2$')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stability check: compute the ratio of standard deviation between the largest and smallest number of bootstrap iterations for each bin\n",
    "\n",
    "# x is the list of bootstrap iteration counts, fsat_r_std and fsat_b_std are arrays of shape (len(x), n_bins)\n",
    "# We'll check the ratio of std at max(x) to min(x) for each bin\n",
    "\n",
    "def stability_check(fsat_std, x):\n",
    "    # fsat_std: shape (len(x), n_bins)\n",
    "    ratios = []\n",
    "    for bin_idx in range(fsat_std.shape[1]):\n",
    "        min_std = np.min(fsat_std[:, bin_idx])\n",
    "        max_std = np.max(fsat_std[:, bin_idx])\n",
    "        # Avoid division by zero or nan\n",
    "        if np.isfinite(min_std) and min_std > 0 and np.isfinite(max_std):\n",
    "            ratio = max_std / min_std\n",
    "        else:\n",
    "            ratio = np.nan\n",
    "        ratios.append(ratio)\n",
    "    return np.array(ratios)\n",
    "\n",
    "# Change the index here to see stabiltiy for different numbers of iterations\n",
    "fsat_r_stability = stability_check(fsat_r_std[2:], x)\n",
    "fsat_b_stability = stability_check(fsat_b_std[2:], x)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(LogLgal_labels, fsat_r_stability, 'ro-', label='Quiescent')\n",
    "plt.plot(LogLgal_labels, fsat_b_stability, 'bo-', label='Star-forming')\n",
    "plt.xlabel('log$(L_{\\mathrm{gal}}~/~[L_{\\odot}~/h^2])$')\n",
    "plt.ylabel('Stability Ratio (max std / min std)')\n",
    "plt.title('Stability of $f_{sat}$ Bootstrap Std by Bin')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is LHMR Scatter LogNormal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No it really isn't, but this is a known by-product of the algorithm.\n",
    "# At fixed halo mass, the abundance matching has put centrals with no satellites at a certain L into those halos,\n",
    "# and then groups whose TOTAL L matches that into those halos, so the centrals in those groups have to have lower L (and a spread).\n",
    "from scipy.stats import norm, skewnorm\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "f = bgs_y1_pzp_2_4_c2\n",
    "fine_bins = np.logspace(9, 15.5, 2*(156-90))\n",
    "f.centrals['Mh_bin2'] = pd.cut(x = f.centrals['M_HALO'], bins = fine_bins, labels = fine_bins[:-1], include_lowest=True)\n",
    "\n",
    "speccentrals = f.centrals.loc[z_flag_is_spectro_z(f.centrals['Z_ASSIGNED_FLAG'])]\n",
    "r_g = speccentrals.loc[speccentrals['QUIESCENT']].groupby('Mh_bin', observed=False)\n",
    "b_g = speccentrals.loc[~speccentrals['QUIESCENT']].groupby('Mh_bin', observed=False)\n",
    "\n",
    "def examine_scatter(sample_groupby, color):\n",
    "\n",
    "    for bin_name, group in sample_groupby:\n",
    "        log_lgal = np.log10(group['L_GAL'].to_numpy())\n",
    "\n",
    "        if len(log_lgal) < 100:\n",
    "            continue  # skip bins with too few galaxies\n",
    "\n",
    "        plt.figure(figsize=(7, 5))\n",
    "\n",
    "        # Histogram\n",
    "        bounds = np.percentile(log_lgal, [1, 99])\n",
    "        bounds = log_lgal.min(), log_lgal.max()\n",
    "        bins = np.linspace(bounds[0]*.98, bounds[1]*1.02, 50)\n",
    "        hist_counts, _, _ = plt.hist(log_lgal, bins=bins, alpha=0.5, color=color, edgecolor='black')\n",
    "        \n",
    "        x = np.linspace(bins[0], bins[-1], 500)\n",
    "\n",
    "        # Fit and plot Gaussian for each\n",
    "        #mu_r, sigma_r = np.mean(log_lgal), np.std(log_lgal)\n",
    "        #plt.plot(x, norm.pdf(x, mu_r, sigma_r) * len(log_lgal) * (bins[1] - bins[0]), 'k-', label='LogNormal')\n",
    "        #plt.text(log_lgal.min(), plt.ylim()[1]*0.7, f'$\\mu_{{red}}$={mu_r:.2f}\\n$\\sigma_{{red}}$={sigma_r:.2f}', color='red')\n",
    "        \n",
    "        # --- Skew-Normal Fit ---\n",
    "        #a_r, loc_r, scale_r = skewnorm.fit(log_lgal)\n",
    "        #skewnorm_pdf = skewnorm.pdf(x, a_r, loc_r, scale_r) * len(log_lgal) * (bins[1] - bins[0])\n",
    "        #plt.plot(x, skewnorm_pdf, 'g--', label='Skew-LogNormal')\n",
    "        #plt.text(log_lgal.min(), plt.ylim()[1]*0.5, f'Skew-Normal Fit:\\n$a$={a_r:.2f}\\nloc={loc_r:.2f}\\nscale={scale_r:.2f}', \n",
    "        #        color='green', transform=plt.gca().transAxes, va='top', ha='right')\n",
    "\n",
    "        # 2-part GMM Fit\n",
    "\n",
    "        # Reshape data for sklearn\n",
    "        log_lgal_reshape = log_lgal.reshape(-1, 1)\n",
    "        gmm = GaussianMixture(n_components=2, covariance_type='full', random_state=857)\n",
    "        gmm.fit(log_lgal_reshape)\n",
    "        weights = gmm.weights_\n",
    "        means = gmm.means_.flatten()\n",
    "        covars = gmm.covariances_.flatten()\n",
    "        stds = np.sqrt(covars)\n",
    "\n",
    "        # Sort components by mean for consistent coloring\n",
    "        idx = np.argsort(means)\n",
    "        means = means[idx]\n",
    "        stds = stds[idx]\n",
    "        weights = weights[idx]\n",
    "\n",
    "        gmm_pdf = (\n",
    "            weights[0] * norm.pdf(x, means[0], stds[0]) +\n",
    "            weights[1] * norm.pdf(x, means[1], stds[1])\n",
    "        ) * len(log_lgal) * (bins[1] - bins[0])\n",
    "        plt.plot(x, gmm_pdf, 'b-.', label='2-Gaussian Mixture')\n",
    "\n",
    "        # Optionally, plot the individual components\n",
    "        plt.plot(x, weights[0] * norm.pdf(x, means[0], stds[0]) * len(log_lgal) * (bins[1] - bins[0]),\n",
    "             'b:', alpha=0.5)\n",
    "        plt.plot(x, weights[1] * norm.pdf(x, means[1], stds[1]) * len(log_lgal) * (bins[1] - bins[0]),\n",
    "             'b:', alpha=0.5)\n",
    "\n",
    "        plt.xlabel('log($L_{GAL}$)')\n",
    "        plt.ylabel('Number of Centrals')\n",
    "        plt.title(f'L Distribution for $M_h$~{bin_name:.1e}')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        \n",
    "        # Print the log_lgal.max() as text in the plot\n",
    "        plt.text(plt.xlim()[0], 0.85*plt.ylim()[1], f'Max log(L_gal)={log_lgal.max():.2f}', \n",
    "                horizontalalignment='right', verticalalignment='top', fontsize=10)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "#examine_scatter(r_g, 'red')\n",
    "examine_scatter(b_g, 'blue')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is the distribution of satellites Poissonian?\n",
    "\n",
    "# Construct a volume limited sample\n",
    "for maglim in [-17.0, -18.0, -19.0]:\n",
    "    vl_df, zmax = bgs_y3_pzp_2_4_c1.get_volume_limited_sample(maglim)\n",
    "\n",
    "    mmins = [3e12, 1e13, 5e13]\n",
    "    colors = ['red', 'blue', 'green']\n",
    "    #cmax = 50 + 17*5 - int(abs(maglim)*5)\n",
    "    cmax = 50\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    offset = 0.0\n",
    "    ymax = 1000\n",
    "    for mmin in mmins:\n",
    "        color = colors[mmins.index(mmin)]\n",
    "        mmax = mmin * 2\n",
    "        df = vl_df[vl_df['M_HALO'] > mmin]\n",
    "        df = df[df['M_HALO'] < mmax]\n",
    "\n",
    "        nsat = df.groupby('IGRP').size() - 1 # -1 to remove the central galaxy from the count\n",
    "        nsat_minus1 = nsat - 1\n",
    "\n",
    "        bins = np.linspace(0, cmax, cmax)\n",
    "        j=plt.hist(nsat, bins=bins, histtype='step', label='Satellites per Group', color=color)\n",
    "        plt.yscale('log')\n",
    "        ymax = max(j[0].max() * 1.5, ymax)\n",
    "        plt.ylim(1, ymax)\n",
    "        plt.xlabel('$N_{sat}$')\n",
    "        plt.ylabel('Number of Groups')\n",
    "        plt.title('$R^{{0.1}}$ < {maglim} ($z$ < {zmax:.4f})'.format(maglim=maglim, zmax=zmax))\n",
    "\n",
    "        # Fit this to a Poisson distribution\n",
    "        mean = nsat.mean()\n",
    "        #print(f'Mean number of satellites: {mean:.2f}')\n",
    "        x = np.arange(0, cmax-1)\n",
    "        poisson_dist = len(nsat) * np.power(mean, x) * np.exp(-mean) / factorial(x)\n",
    "        plt.plot(x+0.5, poisson_dist, color=color, marker='.', label='Data')\n",
    "\n",
    "        # Compute the mean squared error between the histogram and the Poisson distribution\n",
    "        # maxbin to use in the comparison is the last one that the poisson is above 1\n",
    "        maxbin = np.where(poisson_dist >= 1)[0][-1]\n",
    "        # But also make sure we don't go beyond the histogram\n",
    "        maxbin = min(maxbin, len(j[0]))\n",
    "\n",
    "        try:\n",
    "            mse = mean_squared_error(np.log10(j[0][:maxbin]), np.log10(poisson_dist[:maxbin]))\n",
    "            plt.text(0.4, 0.96 - offset, f'{np.log10(mmin):.1f} < log($M_h$) < {np.log10(mmax):.1f}. MSE: {mse:.2f} dex', transform=plt.gca().transAxes, fontsize=12, verticalalignment='top', color=color)\n",
    "            offset += 0.06\n",
    "        except ValueError as e:\n",
    "            pass\n",
    "        \n",
    "        # < Nsat Nsat-1 > = < Nsat >^2 for Poisson. What is it for the sample?\n",
    "        print(f\"Groups with {np.log10(mmin):.2f} < log(M_h) < {np.log10(mmax):.2f}\")\n",
    "        print(f'  < Nsat Nsat-1 > = {np.mean(nsat * nsat_minus1):.2f}')\n",
    "        print(f'  < Nsat > ^2     = {np.mean(nsat)**2:.2f}')\n",
    "        print(f'  Ratio: {np.mean(nsat * nsat_minus1) / np.mean(nsat)**2:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdss_colors_chi_v2.Mr_gal_labels = Mr_gal_labels[15:]\n",
    "sdss_colors_chi_v1.Mr_gal_labels = Mr_gal_labels[15:]\n",
    "sdss_colors_chi_v2.color = 'darkred'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.plots(sdss_colors_chi_v2, sdss_colors_chi_v1, deserialize(cat.bgs_sv3_10p_mcmc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sets = [bgs_y3_like_sv3_fiberonly, bgs_y3_like_sv3_nn, bgs_y3_like_sv3_pz_2_0, bgs_y3_like_sv3_pz_2_4] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Targets in SV3 region observed in main survey got new redshift measurements\n",
    "# Q: How different are those z's compared to SV3 z's? \n",
    "# A: They are similar, but not identical. The difference is less than 0.001 for 99.7% so it's OK for us I think.\n",
    "#    (Subdominant to v_peculiar)\n",
    "\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astropy import units as u\n",
    "from astropy.coordinates import match_coordinates_sky\n",
    "\n",
    "def find_unique_and_matched_objects(cat1, cat2):\n",
    "    df1 = cat1.all_data.loc[z_flag_is_spectro_z(cat1.all_data['Z_ASSIGNED_FLAG'])].reset_index()\n",
    "    df2 = cat2.all_data.loc[z_flag_is_spectro_z(cat2.all_data['Z_ASSIGNED_FLAG'])].reset_index()\n",
    "                                                 \n",
    "    # Extract RA and Dec from the catalogs\n",
    "    ra1, dec1 = df1['RA'].to_numpy(), df1['DEC'].to_numpy()\n",
    "    ra2, dec2 = df2['RA'].to_numpy(), df2['DEC'].to_numpy()\n",
    "        \n",
    "    # Create SkyCoord objects\n",
    "    coords1 = SkyCoord(ra=ra1*u.degree, dec=dec1*u.degree)\n",
    "    coords2 = SkyCoord(ra=ra2*u.degree, dec=dec2*u.degree)\n",
    "    \n",
    "    # Match coordinates\n",
    "    idx, d2d, _ = match_coordinates_sky(coords1, coords2)\n",
    "\n",
    "    df1['FID'] = idx\n",
    "    df2['FID'] = df2.index\n",
    "    \n",
    "    # Find objects in df1 that are not in df2\n",
    "    unique_mask = d2d > 1*u.arcsec  # You can adjust the threshold as needed\n",
    "\n",
    "    # join with df2 for matched_objects on the FID\n",
    "    matched_objects = df1.join(df2.set_index('FID'), on='FID', rsuffix='_2')\n",
    "    matched_objects = matched_objects[~unique_mask]\n",
    "\n",
    "    print(f\"Total spectroscopic galaxies in cat1: {len(df1)}, cat2: {len(df2)}\")\n",
    "    print(f'Unique objects in cat1: {unique_mask.sum()}, Matched objects in cat1: {len(matched_objects)}')\n",
    "    \n",
    "    return df1[unique_mask], matched_objects\n",
    "\n",
    "# Example usage\n",
    "unique_objects, matched_objects = find_unique_and_matched_objects(bgs_sv3_pz_2_4_10p, bgs_y3_like_sv3_pz_2_4)\n",
    "\n",
    "print(np.isclose(matched_objects['Z'], matched_objects['z_2'], atol=0.001, rtol=0).sum() / len(matched_objects))\n",
    "#fig=pp.make_map(unique_objects.RA.to_numpy(), unique_objects['DEC'].to_numpy())\n",
    "\n",
    "plt.hist(matched_objects['Z'] - matched_objects['z_2'], bins=np.linspace(-0.005, 0.005, 100))\n",
    "plt.yscale('log')\n",
    "\n",
    "# Draw verticle line at 0.005\n",
    "plt.axvline(x=0.005, color='r', linestyle='--')\n",
    "plt.axvline(x=-0.005, color='r', linestyle='--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SV3 10p and SDSS BGS-cut are very similar!\n",
    "bgs_sv3_pz_2_4_10p.color = 'k'\n",
    "pp.plots(bgs_sv3_pz_2_4_10p, sdss_bgscut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cutting SDSS to remove regions with poor BGS overlap barely improves the completeness\n",
    "pp.plots(sdss_bgscut, sdss_vanilla_v2)\n",
    "print(f\"{spectroscopic_complete_percent(sdss_bgscut.all_data['Z_ASSIGNED_FLAG']):.2f}% spectroscopic complete for BGS cut\")\n",
    "print(f\"{spectroscopic_complete_percent(sdss_vanilla_v2.all_data['Z_ASSIGNED_FLAG']):.2f}% spectroscopic complete for Vanilla\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdss_vanilla_v2.marker = '--'\n",
    "sdss_colors_mine.color = 'navy'\n",
    "sdss_colors_mine.marker = '--'\n",
    "sdss_colors_chi_v2.color = 'deeppink'\n",
    "sdss_colors_chi_v2.marker = '--'\n",
    "bgs_y1_pzp_2_4.color = 'brown'\n",
    "bgs_y1_pzp_2_4.marker = '-'\n",
    "pp.plots(bgs_y1_pzp_2_4, bgs_sv3_pz_2_4_10p, sdss_vanilla_v2, sdss_colors_mine, sdss_colors_chi_v2)\n",
    "#pp.plots(cat.sdss_published, sdss_colors_chi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why doesn't mstar missing % exactly match z_assigned_flag? \n",
    "# Probably redshift failures. Still have a spectra so still have mstar\n",
    "print(np.sum(np.isnan(bgs_y1_pzp_2_4.all_data['MSTAR'])) / len(bgs_y1_pzp_2_4.all_data['MSTAR']))\n",
    "print(np.sum(bgs_y1_pzp_2_4.all_data['Z_ASSIGNED_FLAG'] != 0) / len(bgs_y1_pzp_2_4.all_data['Z_ASSIGNED_FLAG']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.compare_fsat_color_split(sdss_vanilla_v1, sdss_vanilla_v2, project_percent=0.52)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.compare_fsat_color_split(bgs_sv3_pz_2_4_10p, sdss_vanilla_v2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.qf.centered_plot(bgs_y1_pzp_2_4)\n",
    "pp.qf.centered_plot(sdss_published)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.fsat_by_z_bins(bgs_y1_pzp_2_4, z_bins=np.array([0.0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.5, 1.0]), show_plots=True)\n",
    "#pp.fsat_by_z_bins(mxxl_simple_4, z_bins=np.array([0.0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.5, 1.0]), show_plots=False, aggregation=pp.fsat_truth_vmax_weighted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(bgs_y1_pzp_2_4.all_data['MSTAR'].dropna(), np.logspace(6, 13, 100))\n",
    "plt.xlabel('Stellar Mass')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Stellar Masses for bgs_y1_pzp_2_4.all_data')\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bgs_y1_pzp_2_4.all_data['Mstar_bin'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bgs_y1_pz_2_4.postprocess()\n",
    "#bgs_y1_pz_2_4.all_data['Mstar_bin'] = pd.cut(x = bgs_y1_pz_2_4.all_data['MSTAR'], bins = mstar_bins, labels = mstar_labels, include_lowest = True)\n",
    "pp.qf_cen_plot(bgs_y1_pzp_2_4, mstar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.fsat_by_z_bins(bgs_y1_pzp_2_4, z_bins=np.array([0.0, 0.2, 1.0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out biggest group size\n",
    "for dataset in [bgs_y1_pzp_2_4, sdss_vanilla_v2]:\n",
    "    print(dataset.name)\n",
    "    print(dataset.all_data.groupby('IGRP').size().max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SDSS Examine Bimodality\n",
    "\n",
    "z=sdss_vanilla_v2.all_data['Z']\n",
    "gmr=sdss_vanilla_v2.all_data['MAG_G'] - sdss_vanilla_v2.all_data['MAG_R']\n",
    "junk=plt.hist(gmr, bins=np.linspace(-1,3,300), alpha=0.4)\n",
    "#junk=plt.hist(k_correct(sdss_vanilla.all_data['MAG_G'], z, gmr, band='g')  - k_correct(sdss_vanilla.all_data['MAG_R'], z, gmr, band='r'), bins=500, alpha=0.4)\n",
    "junk=plt.hist(sdss_vanilla_v2.all_data['DN4000'], bins=np.linspace(0,4,300), alpha=0.4)\n",
    "plt.xlim(-1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate changes in halo mass function from wcen\n",
    "m1=np.log10(sdss_vanilla_v2.all_data['M_HALO'])\n",
    "m2=np.log10(sdss_colors_v2.all_data['M_HALO'])\n",
    "m3=np.log10(sdss_colors_chi_v2.all_data['M_HALO'])\n",
    "\n",
    "# bin m1,m2,m3 the same way\n",
    "n_bins = 20\n",
    "bins = np.linspace(10.8, 15.0, n_bins)\n",
    "d1 = np.digitize(m1, bins)\n",
    "d2 = np.digitize(m2, bins)\n",
    "d3 = np.digitize(m3, bins)\n",
    "\n",
    "# count the number of galaxies in each bin\n",
    "n1 = np.array([np.sum(d1==i) for i in range(1, n_bins+1)])\n",
    "n2 = np.array([np.sum(d2==i) for i in range(1, n_bins+1)])\n",
    "n3 = np.array([np.sum(d3==i) for i in range(1, n_bins+1)])\n",
    "\n",
    "# Do the same but for log10(counts)\n",
    "n1 = np.log10(n1)\n",
    "n2 = np.log10(n2)\n",
    "n3 = np.log10(n3)\n",
    "print(n1,n2,3)\n",
    "\n",
    "# Log difference\n",
    "p1 = np.abs(n1-n2)\n",
    "p2 = np.abs(n1-n3)\n",
    "\n",
    "plt.plot(bins, p1, label='SDSS Colors vs Vanilla')\n",
    "plt.plot(bins, p2, label='SDSS Colors+Chi vs Vanilla')\n",
    "\n",
    "plt.xlabel('log10(M_halo)')\n",
    "plt.ylabel('Log10 Difference in Counts')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make single group CSV for legacysurvey.org/viewer visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcat = deserialize(cat.bgs_y1_hybrid8_mcmc)\n",
    "gcat.all_data.set_index('TARGETID', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View TARGETID 39627582685054090\n",
    "gcat.all_data.loc[39627582685054090]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "large_halos = gcat.centrals['M_HALO'] > 1e13\n",
    "large_halos = gcat.centrals['N_SAT'] > 20\n",
    "\n",
    "# Print RA, DEC of 5 large halos\n",
    "for i in [4]:\n",
    "    print(f\"Large Halo {i+1}:\")\n",
    "    print(f\"RA: {gcat.centrals.loc[large_halos].iloc[i]['RA']}, DEC: {gcat.centrals.loc[large_halos].iloc[i]['DEC']}\")\n",
    "    print(f\"M_HALO: {gcat.centrals.loc[large_halos].iloc[i]['M_HALO']:.1e}\")\n",
    "    print(f\"Z: {gcat.centrals.loc[large_halos].iloc[i]['Z']}\")\n",
    "\n",
    "    # Print off a file of RA, DEC, TARGETID of the group members\n",
    "    group_members = gcat.all_data.loc[gcat.all_data['IGRP'] == gcat.centrals.loc[large_halos].iloc[i]['IGRP']]\n",
    "    with open(f'large_halo_{i+1}_members.txt', 'w') as f:\n",
    "        f.write(\"RA,DEC,TARGETID\\n\")\n",
    "        for index, row in group_members.iterrows():\n",
    "            f.write(f\"{row['RA']},{row['DEC']},{row['TARGETID']}\\n\")\n",
    "\n",
    "    print(group_members.loc[:, ['Z', 'Z_ASSIGNED_FLAG']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in group_ids[0:10]:\n",
    "for i in [1644058, 1644051]:\n",
    "    #df.loc[df['IGRP'] == i, ['RA', 'DEC']].to_csv(OUTPUT_FOLDER + f'group{i}.csv', index=False)\n",
    "    print(df.loc[df['IGRP'] == i, ['RA', 'DEC', 'Z', 'Z_ASSIGNED_FLAG']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Study z_phot vs z_spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = bgs_sv3_pz_2_4_10p.all_data\n",
    "\n",
    "low_cut = 0.0001 # not a dramatic shift when moving from here to 0.1\n",
    "quality = (df['Z_PHOT'] != NO_PHOTO_Z) & z_flag_is_spectro_z(df['Z_ASSIGNED_FLAG']) & (df['Z_OBS'] > low_cut) & (df['L_GAL'] > 3E10)\n",
    "#quality = (df['Z_PHOT'] != NO_PHOTO_Z) & z_flag_is_spectro_z(df['Z_ASSIGNED_FLAG']) & (df['Z_OBS'] > low_cut)\n",
    "\n",
    "# Investigate the photo-z error distribution for red and blue galaxies\n",
    "# Blue exhibits and offset and a less peaked distribution than red\n",
    "# Red does have some skew\n",
    "data = df.loc[np.logical_and(df['QUIESCENT'], quality)]\n",
    "blue = df.loc[np.logical_and(~df['QUIESCENT'], quality)]\n",
    "delta_red = data['Z_PHOT'] - data['Z_OBS']\n",
    "delta_blue = blue['Z_PHOT'] - blue['Z_OBS']\n",
    "delta_all = df.loc[quality, 'Z_PHOT'] - df.loc[quality, 'Z_OBS']\n",
    "\n",
    "x = 0.1\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(delta_red, bins=50, range=(-x, x), histtype='step', color='red', label='Red')\n",
    "plt.hist(delta_blue, bins=50, range=(-x, x), histtype='step', color='blue', label='Blue')\n",
    "plt.hist(delta_all, bins=50, range=(-x, x), histtype='step', color='k', label='All')\n",
    "#plt.yscale('log')\n",
    "plt.xlabel('Photo-z - Spectro-z')\n",
    "plt.legend()\n",
    "\n",
    "# draw a vertical line at 0\n",
    "plt.axvline(0, color='black', lw=1)\n",
    "plt.axvline(-SIM_Z_THRESH, color='green')\n",
    "plt.axvline(SIM_Z_THRESH, color='green')\n",
    "\n",
    "percentiles = np.percentile(delta_all, [16, 50, 84])\n",
    "print(f\"Median delta z: {percentiles[1]:.4f}, 16th percentile: {percentiles[0]:.4f}, 84th percentile: {percentiles[2]:.4f}\")\n",
    "# add bars for the percentiles\n",
    "#plt.axvline(percentiles[0], color='green')\n",
    "#plt.axvline(percentiles[2], color='green')\n",
    "\n",
    "# What % fall within 0.005 of the true redshift?\n",
    "within_5_milli = np.abs(delta_all) < SIM_Z_THRESH\n",
    "print(f\"{np.sum(within_5_milli) / len(delta_all) * 100:.2f}% of galaxies have a photometric redshift within {SIM_Z_THRESH} of the spectroscopic redshift.\")\n",
    "print(f\"For red: {np.sum(np.abs(delta_red) < SIM_Z_THRESH) / len(delta_red) * 100:.2f}%\")\n",
    "print(f\"For blue: {np.sum(np.abs(delta_blue) < SIM_Z_THRESH) / len(delta_blue) * 100:.2f}%\")\n",
    "\n",
    "# Find the +/- that gives 95% of the data\n",
    "percentiles = np.percentile(delta_all, [2.5, 97.5])\n",
    "print(f\"2.5th percentile: {percentiles[0]:.4f}, 97.5th percentile: {percentiles[1]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BGS and SDSS Target Overlap Analysis\n",
    "\n",
    "TODO: SDSS magnitudes have e-corrections of them that fastspecfit on BGS does not have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sloan BGS overlap - are their abs mag's the same? \n",
    "# Control for different z fits\n",
    "\n",
    "bgs_to_use = bgs_y1_pzp_2_4.all_data\n",
    "#lost_bgs = bgs_to_use.loc[z_flag_is_not_spectro_z(bgs_to_use['Z_ASSIGNED_FLAG'])]\n",
    "sdss_obs = sdss_vanilla_v1.all_data.loc[sdss_vanilla_v1.all_data['Z_ASSIGNED_FLAG'] == AssignedRedshiftFlag.SDSS_SPEC.value]\n",
    "bgs_obs = bgs_to_use.loc[bgs_to_use['Z_ASSIGNED_FLAG'] == AssignedRedshiftFlag.DESI_SPEC.value].reset_index()\n",
    "\n",
    "sdss_obs = sdss_obs.loc[sdss_obs['Z'] > 0.1]\n",
    "bgs_obs = bgs_obs.loc[bgs_obs['Z'] > 0.1]\n",
    "#sdss_obs = sdss_obs.loc[sdss_obs['Z'] < 0.25]\n",
    "#bgs_obs = bgs_obs.loc[bgs_obs['Z'] < 0.25]\n",
    "\n",
    "catalog = coord.SkyCoord(ra=sdss_obs.RA.to_numpy()*u.degree, dec=sdss_obs['DEC'].to_numpy()*u.degree, frame='icrs')\n",
    "to_match = coord.SkyCoord(ra=bgs_obs.RA.to_numpy()*u.degree, dec=bgs_obs['DEC'].to_numpy()*u.degree, frame='icrs')\n",
    "\n",
    "idx, d2d, d3d = coord.match_coordinates_sky(to_match, catalog, nthneighbor=1, storekdtree=False)\n",
    "matched = d2d < 1*u.arcsec\n",
    "\n",
    "bgs_obs['SDSS_LOGLGAL'] = np.where(matched, sdss_obs['LOGLGAL'].to_numpy()[idx], np.nan)\n",
    "bgs_obs['SDSS_Z'] = np.where(matched, sdss_obs['Z'].to_numpy()[idx], np.nan)\n",
    "bgs_obs = bgs_obs.loc[matched]\n",
    "print(f\"Matched {len(bgs_obs)} out of {len(bgs_to_use)}\")\n",
    "\n",
    "bgs_obs['M_R'] = log_solar_L_to_abs_mag_r(bgs_obs['LOGLGAL'])\n",
    "#Q=1.6\n",
    "#e_corr = Q*(bgs_obs['SDSS_Z'] - 0.1)\n",
    "e_corr = 0.0\n",
    "bgs_obs['SDSS_M_R'] = log_solar_L_to_abs_mag_r(bgs_obs['SDSS_LOGLGAL']) - e_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# difference in abs mag as a function of BGS mag\n",
    "delta_z = bgs_obs['Z'] - bgs_obs['SDSS_Z']\n",
    "zagreed = np.abs(delta_z) < 0.0001\n",
    "bgs_obs_zagreed = bgs_obs.loc[zagreed]\n",
    "\n",
    "# make abs mag bins\n",
    "bins = np.linspace(-23.5, -16.5, 29)\n",
    "bins = np.linspace(-22.5, -19.5, 19)\n",
    "bgs_obs_zagreed['M_R_BIN'] = pd.cut(bgs_obs_zagreed['M_R'], bins=bins, labels=bins[:-1], include_lowest=True)\n",
    "bgs_obs_zagreed['M_R_DIFF'] = bgs_obs_zagreed['M_R'] - bgs_obs_zagreed['SDSS_M_R']\n",
    "bgs_obs_zagreed['M_R_ABS_DIFF'] = np.abs(bgs_obs_zagreed['M_R'] - bgs_obs_zagreed['SDSS_M_R'])\n",
    "\n",
    "# Print off counts in each bin\n",
    "counts = bgs_obs_zagreed.groupby('M_R_BIN').size()\n",
    "#print(counts)\n",
    "\n",
    "# Within each bin, calculate the median and 2sigma differences\n",
    "binned = bgs_obs_zagreed.groupby('M_R_BIN')['M_R_DIFF'].apply(lambda x: np.percentile(x, [2.5, 50, 97.5]))\n",
    "binned = pd.DataFrame(binned.tolist(), index=binned.index, columns=['2.5', 'median', '97.5'])\n",
    "\n",
    "plt.figure()\n",
    "plt.errorbar(binned.index, binned['median'], yerr=[binned['median'] - binned['2.5'], binned['97.5'] - binned['median']], fmt='o', capsize=3)\n",
    "plt.xlabel('$M_r^{BGS}$')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('95% Interval of $M_r^{BGS} - M_r^{SDSS}$')\n",
    "plt.ylim(-1.0, 1.0)\n",
    "plt.title('BGS vs SDSS Difference in $M_r$ (z > 0.1)')\n",
    "plt.axhline(0, color='red', linestyle='--')\n",
    "plt.tight_layout()\n",
    "plt.grid()\n",
    "plt.plot()\n",
    "\n",
    "# Within each bin, calculate the median and 2sigma differences\n",
    "#binned = bgs_obs.groupby('M_R_BIN')['M_R_ABS_DIFF'].agg(['median', 'std'])\n",
    "# Don't use std, we want asymmetric errors\n",
    "#binned = bgs_obs_zagreed.groupby('M_R_BIN')['M_R_ABS_DIFF'].apply(lambda x: np.percentile(x, [2.5, 50, 97.5]))\n",
    "#binned = pd.DataFrame(binned.tolist(), index=binned.index, columns=['2.5', 'median', '97.5'])\n",
    "\n",
    "\"\"\"\n",
    "plt.figure()\n",
    "plt.errorbar(binned.index, binned['median'], yerr=[binned['median'] - binned['2.5'], binned['97.5'] - binned['median']], fmt='o', capsize=3)\n",
    "plt.xlabel('$M_r^{BGS}$')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('95% Interval of |$M_r^{BGS} - M_r^{SDSS}$|')\n",
    "plt.ylim(-0.05, 1.0)\n",
    "plt.title('BGS vs SDSS Absolute Difference in $M_r$')\n",
    "plt.axhline(0, color='red', linestyle='--')\n",
    "plt.tight_layout()\n",
    "plt.grid()\n",
    "plt.plot()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract BGS M_R and the difference (M_R_DIFF) between BGS and SDSS M_R\n",
    "x = binned.index.to_numpy()\n",
    "y = binned['median'].to_numpy()\n",
    "\n",
    "# Fit a polynomial function (e.g., degree 2)\n",
    "degree = 2\n",
    "coeffs = Polynomial.fit(x, y, degree).convert().coef\n",
    "print(f\"Polynomial coefficients: {coeffs}\")\n",
    "\n",
    "# Generate fitted values\n",
    "x_fit = np.linspace(x.min() - 1, x.max() + 2, 100)\n",
    "\n",
    "y_fit = Polynomial(coeffs)(x_fit)\n",
    "# Calculate the mean squared error\n",
    "mse = mean_squared_error(y, Polynomial(coeffs)(x))\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "\n",
    "# Plot the data and the fitted polynomial\n",
    "plt.figure()\n",
    "plt.errorbar(binned.index, binned['median'], yerr=[binned['median'] - binned['2.5'], binned['97.5'] - binned['median']], fmt='o', capsize=3)\n",
    "plt.plot(x_fit, y_fit, color='orange', label=f'Polynomial Fit (degree {degree})')\n",
    "# Plot saved result too\n",
    "plt.plot(x_fit, bgs_mag_to_sdsslike_mag(x_fit)-x_fit, color='green', label='Saved Result')\n",
    "plt.xlabel('$M_r^{BGS}$')\n",
    "plt.ylabel('$M_r^{SDSS} - M_r^{BGS}$')\n",
    "plt.title('Polynomial Fit to BGS vs SDSS $M_r$')\n",
    "plt.legend()\n",
    "plt.ylim(-0.8, 0.8)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's fit a function of BGS M_R so we can 'correct' BGS Luminosities to look like SDSS ones\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Info for clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bgs_sv3_pz_2_4_10p.all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bgs_sv3_pz_2_4_10p.all_data\n",
    "\n",
    "tbl = Table.read(IAN_BGS_SV3_MERGED_NOY3_FILE)\n",
    "tbl.keep_columns(['TARGETID', 'ABSMAG01_SDSS_R'])\n",
    "df = tbl.to_pandas()\n",
    "\n",
    "df = bgs_sv3_pz_2_4_10p.all_data.join(df.set_index('TARGETID'), on='TARGETID')\n",
    "\n",
    "df['R'] = log_solar_L_to_abs_mag_r(df['LOGLGAL'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why are some observed redshifts higher than my theoretical max? \n",
    "# Peculiar velocity of course. Since it should be symmetric in the redshift direction, it's okay to use the theoretical max.\n",
    "mags = np.array([-14, -15, -16, -17, -18, -19, -20, -21, -22, -23])\n",
    "\n",
    "LIM = 17.6\n",
    "print(\"SDSS FLUXLIM\")\n",
    "print(\"DIM\")\n",
    "for m in mags:\n",
    "    print(f\"M_R={m}: THEORY MAX={get_max_observable_z(m, LIM).value:.5f}\")\n",
    "    #print(f\"M_R={m}: THEORY MAX={get_max_observable_z_m30(m, LIM).value:.5f}\")\n",
    "print(\"BRIGHT\")\n",
    "\n",
    "LIM = 19.5\n",
    "print(\"BGS BRIGHT FLUXLIM\")\n",
    "print(\"DIM\")\n",
    "for m in mags:\n",
    "    print(f\"M_R={m}: THEORY MAX={get_max_observable_z(m, LIM).value:.5f}\")\n",
    "print(\"BRIGHT\")\n",
    "\n",
    "#LIM = 20.0\n",
    "#print(\"BGS FAINT FLUXLIM\")\n",
    "#print(\"DIM\")\n",
    "#for m in mags:\n",
    "#    print(f\"M_R={m}: THEORY MAX={get_max_observable_z(m, LIM).value:.5f}\")\n",
    "#print(\"BRIGHT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure out abs mag bins redshift maxes to use\n",
    "#df = bgs_sv3_pz_2_4_10p.all_data\n",
    "#df['MAG'] = log_solar_L_to_abs_mag_r(np.log10(df['L_GAL']))\n",
    "\n",
    "df = df.loc[df['Z_ASSIGNED_FLAG'] == AssignedRedshiftFlag.DESI_SPEC.value]\n",
    "\n",
    "mags = np.array([-14, -15, -16, -17, -18, -19, -20, -21, -22, -23])\n",
    "\n",
    "# Why are some observed redshifts higher than my theoretical max? \n",
    "# Peculiar velocity of course. Since it should be symmetric in the redshift direction, it's okay to use the theoretical max.\n",
    "print(\"DIM\")\n",
    "for m in mags:\n",
    "    # This column is k-corr to 0.1 and uses h=1.0\n",
    "    # Not sure why OBS MAX is so high for some of them (beyond vpec...)\n",
    "    print(f\"ABSMAG01_SDSS_R > {m}: THEORY MAX={get_max_observable_z(m, 19.5).value:.5f}  OBS MAX={df.loc[df['ABSMAG01_SDSS_R'] > m, 'Z'].max()}\")\n",
    "print(\"BRIGHT\")\n",
    "\n",
    "print(\"DIM\")\n",
    "for m in mags:\n",
    "    # This is k-corr to 0.1 as well using my method I think\n",
    "    print(f\"R^0.1 > {m}: THEORY MAX={get_max_observable_z(m, 19.5).value:.5f}  OBS MAX={df.loc[df['R'] > m, 'Z'].max()}\")\n",
    "print(\"BRIGHT\")\n",
    "\n",
    "# For noncumulative mag ranges now, using the theory values\n",
    "print(\"\\n Generic Cosmology\")\n",
    "mags = np.array([-14, -15, -16, -17, -18, -19, -20, -21, -22, -23])\n",
    "zmaxes = np.array([get_max_observable_z(m, 19.5).value for m in mags])\n",
    "zmins = np.array([get_max_observable_z(m+1, 19.5).value for m in mags])\n",
    "for m, zmin, zmax in zip(mags, zmins, zmaxes):\n",
    "    print(f\"{m} < Mag-5log(h) <= {m+1}:  zmin={zmin:.5f}  zmax={zmax:.5f}\")\n",
    "\n",
    "print(\"\\n MXXL Cosmology\")\n",
    "zmaxes = np.array([get_max_observable_z_mxxlcosmo(m, 19.5).value for m in mags])\n",
    "zmins = np.array([get_max_observable_z_mxxlcosmo(m+1, 19.5).value for m in mags])\n",
    "for m, zmin, zmax in zip(mags, zmins, zmaxes):\n",
    "    print(f\"{m} < Mag-{5*np.log10(0.7):.2f} <= {m+1}:  zmin={zmin:.5f}  zmax={zmax:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bgs_sv3_pz_2_4_10p.calculate_projected_clustering()\n",
    "# why is n=0? # BUG\n",
    "pp.wp_rp(bgs_sv3_pz_2_4_10p.wp_all[0], bgs_sv3_pz_2_4_10p.wp_all[1])#, bgs_sv3_pz_2_4_10p.wp_all[2], bgs_sv3_pz_2_4_10p.wp_all[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Co-moving Dist:  {get_cosmology().comoving_distance([0.01, 0.1, 0.2, 0.4]).value}\") # / Mpc/h\n",
    "print(f\"Co-moving Dist:  {get_cosmology().luminosity_distance([0.01, 0.1, 0.2, 0.4]).value / np.array([1.01, 1.1, 1.2, 1.4])}\") # / Mpc/h\n",
    "print(f\"Luminosity Dist: {get_cosmology().luminosity_distance([0.01, 0.1, 0.2, 0.4]).value}\") # / Mpc/h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SDSS Tutorial of Corrfunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mock catalog (SDSS-North) supplied with Corrfunc\n",
    "mock_catalog = pjoin(dirname(abspath(Corrfunc.__file__)), \"../mocks/tests/data/\", \"Mr19_mock_northonly.rdcz.ff\")\n",
    "RA, DEC, CZ = read_catalog(mock_catalog)\n",
    "\n",
    "# Randoms catalog (SDSS-North) supplied with Corrfunc\n",
    "randoms_catalog = pjoin(dirname(abspath(Corrfunc.__file__)), \"../mocks/tests/data/\", \"Mr19_randoms_northonly.rdcz.ff\")\n",
    "RAND_RA, RAND_DEC, RAND_CZ = read_catalog(randoms_catalog)\n",
    "\n",
    "rbins, wp = calculate_wp(RA, DEC, CZ, RAND_RA, RAND_DEC, RAND_CZ)\n",
    "\n",
    "pp.wp_rp(rbins, wp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j=plt.hist(RAND_CZ, bins=100, histtype='step', density=True)\n",
    "j=plt.hist(CZ, bins=100, histtype='step', density=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code to support others projects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bernie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MASS_CUT = 10**15.0\n",
    "df = bgs_y3_pzp_2_4.all_data[bgs_y3_pzp_2_4.all_data['M_HALO'] > MASS_CUT]\n",
    "df = df[df['DEC'] < 30.0] # Rough for ACT footprint matching\n",
    "\n",
    "print(\"\\nBGS Y3\")\n",
    "#print(df['Z_ASSIGNED_FLAG'].value_counts())    \n",
    "print(df.sort_values('L_GAL', ascending=False).loc[:, ['RA', 'DEC', 'L_GAL', 'Z', 'Z_PHOT', 'Z_OBS', 'Z_ASSIGNED_FLAG']].head(10))\n",
    "\n",
    "centrals = df[df['IGRP'] == df.index]\n",
    "clusters = centrals[centrals['N_SAT'] >= 14]\n",
    "print(f\"Clusters with 15+ members: {len(clusters)}\")\n",
    "\n",
    "x = df['IGRP'].value_counts() > 14\n",
    "print(f\"Clusters with 15+ members: {np.sum(x)}\")\n",
    "\n",
    "# Why do the above 2 not agree?\n",
    "\n",
    "df_spec = df[z_flag_is_spectro_z(df['Z_ASSIGNED_FLAG'])]\n",
    "y = df_spec['IGRP'].value_counts() > 14\n",
    "print(f\"Clusters with 15+ spectroscopic members: {np.sum(y)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viraj Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viraj Compare\n",
    "path = DATA_FOLDER + 'VIRAJ/jura_bgs_bright_catalog_for_ian.fits'\n",
    "table = Table.read(path)\n",
    "viraj_df = table.to_pandas()\n",
    "viraj_df.set_index('TARGETID', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_viraj_ian(viraj_df: pd.DataFrame, gc : GroupCatalog):\n",
    "    ian_df = gc.all_data.set_index('TARGETID').loc[:, ['Z', 'L_GAL', 'VMAX', 'P_SAT', 'M_HALO', 'N_SAT', 'L_TOT',\n",
    "       'IGRP', 'WEIGHT', 'app_mag', 'Z_ASSIGNED_FLAG', 'G_R', 'Z_PHOT', 'IS_SAT', 'QUIESCENT']]\n",
    "    print(ian_df['IGRP'].dtype)\n",
    "    #ian_df['QUIESCENT'] = ian_df['QUIESCENT'].astype(float)\n",
    "    ian_df['N_SAT'] = ian_df['N_SAT'].astype(int)\n",
    "    together = viraj_df.join(ian_df, how='inner', validate='one_to_one')\n",
    "    print(together['IGRP'].dtype)\n",
    "    print(f\"Viraj targets: {len(viraj_df):,}, Ian {gc.name} Catalog: {len(ian_df):,}, # of Viraj Targets found in Ian's: {(~np.isnan(together['IS_SAT'])).sum():,}\")\n",
    "    return together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "together1 = merge_viraj_ian(viraj_df, bgs_simple_4_1pass)\n",
    "together2 = merge_viraj_ian(viraj_df, simple4_BGS)\n",
    "together3 = merge_viraj_ian(viraj_df, bgs_y3_simple_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing=together3.loc[np.isnan(together3['IS_SAT'])]\n",
    "missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bgs_y3_simple_5.all_data['IGRP'].dtype)\n",
    "print(together3['IGRP'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_write = Table.from_pandas(together3, index=True)\n",
    "to_write.write(DATA_FOLDER + 'VIRAJ/jura_bgs_bright_catalog_for_ian_matched.fits', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = Table.read(DATA_FOLDER + 'VIRAJ/jura_bgs_bright_catalog_for_ian_matched.fits', format='fits')\n",
    "df = combined.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mock and SV3 Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UCHUU Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(all_u.all_data['M_HALO'], bins=pp.Mhalo_bins, alpha=0.4)\n",
    "plt.hist(all_u.all_data['uchuu_halo_mass']*10**10, bins=pp.Mhalo_bins, alpha=0.4)\n",
    "plt.loglog()\n",
    "\n",
    "# TODO do we expect the mass distribution of halos to be so different from the UCHUU SHAM catalog and our assigned halo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 1 / VMax corrections do odd thing to UCHUU Truth. Why?\n",
    "pp.hod_plots(all_u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What effect does Fiber Assignment have on the luminosity function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pp.group_finder_centrals_halo_masses_plots(mxxl_all, [mxxl_fiberonly, mxxl_simple_4])\n",
    "pp.group_finder_centrals_halo_masses_plots(bgs_sv3_pz_2_4_10p, [bgs_sv3_pz_1_7p, bgs_sv3_simple_5_7p])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare halos to truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.assigned_halo_analysis(mxxl_simple_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare assigned implied abs mags to truth from MXXL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_unobs_counts = mxxl_all.all_data[mxxl_all.all_data['Z_ASSIGNED_FLAG'] != 0].groupby('LGAL_BIN').RA.count()\n",
    "simple_4_ubobs_counts = mxxl_simple_4.all_data.groupby('LGAL_BIN').RA.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.L_func_plot([mxxl_all, mxxl_simple_4], [all_unobs_counts, simple_4_ubobs_counts])\n",
    "\n",
    "\n",
    "\n",
    "#pp.L_func_plot([all, simple_4], [all.all_data.L_gal[all.all_data['Z_ASSIGNED_FLAG'] == 0], simple_4.all_data.L_gal[simple_4.all_data['Z_ASSIGNED_FLAG'] == 0]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SV3 Edge Effects Quantification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc = bgs_sv3_pz_2_4_10p\n",
    "inner_galaxies = filter_SV3_to_avoid_edges(gc)\n",
    "inner_galaxies.color = 'r'\n",
    "inner_galaxies.name = 'SV3 Inner Galaxies'\n",
    "pp.plots(inner_galaxies, gc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = pp.make_map(gc.all_data.RA.to_numpy(), gc.all_data['DEC'].to_numpy())\n",
    "fig = pp.make_map(inner_galaxies.all_data.RA.to_numpy(), inner_galaxies.all_data['DEC'].to_numpy(), fig=fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.all_data.groupby('LGAL_BIN')['Z'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centering_versions = [\n",
    "    filter_SV3_to_avoid_edges(gc, 1.5),\n",
    "    filter_SV3_to_avoid_edges(gc, 1.4),\n",
    "    filter_SV3_to_avoid_edges(gc, 1.3),\n",
    "    filter_SV3_to_avoid_edges(gc, 1.2),\n",
    "    filter_SV3_to_avoid_edges(gc, 1.1),\n",
    "    filter_SV3_to_avoid_edges(gc, 1.0),\n",
    "    filter_SV3_to_avoid_edges(gc, 0.9),\n",
    "]\n",
    "pickle.dump(centering_versions, open('centering_versions.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "lowz = gc.all_data.loc[gc.all_data.z < 0.03]\n",
    "lowz_gc = copy.deepcopy(gc)\n",
    "lowz_gc.all_data = lowz\n",
    "lowz_gc.refresh_df_views()\n",
    "centering_versions_lowz = [\n",
    "    filter_SV3_to_avoid_edges(lowz_gc, 1.5),\n",
    "    filter_SV3_to_avoid_edges(lowz_gc, 1.4),\n",
    "    filter_SV3_to_avoid_edges(lowz_gc, 1.3),\n",
    "    filter_SV3_to_avoid_edges(lowz_gc, 1.2),\n",
    "    filter_SV3_to_avoid_edges(lowz_gc, 1.1),\n",
    "    filter_SV3_to_avoid_edges(lowz_gc, 1.0),\n",
    "    filter_SV3_to_avoid_edges(lowz_gc, 0.9),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.load(open('centering_versions.pkl', 'rb'))\n",
    "\n",
    "for i, d in enumerate(centering_versions):\n",
    "    d.color = [0, i/len(centering_versions), 0]\n",
    "    d.name = f'SV3 10p, {1.5-i*0.1:.1f} deg center cut'\n",
    "\n",
    "pp.LEGENDS_ON = False\n",
    "gc.color = 'blue'\n",
    "pp.fsat_by_zbins(gc, *centering_versions, z_bins=np.array([0.0, 0.03, 1.0]))\n",
    "pp.single_plots(gc)\n",
    "pp.single_plots(centering_versions[2])\n",
    "pp.single_plots(centering_versions[4])\n",
    "pp.single_plots(centering_versions[6])\n",
    "pp.LEGENDS_ON = True\n",
    "\n",
    "#pp.fsat_by_z_bins(gc, z_bins=np.array([0.0, 0.03, 1.0]))\n",
    "#for d in centering_versions:\n",
    "#    pp.fsat_by_z_bins(d, z_bins=np.array([0.0, 0.03, 1.0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowz_gc.color = 'blue'\n",
    "pp.single_plots(lowz_gc)\n",
    "pp.single_plots(centering_versions_lowz[2])\n",
    "pp.single_plots(centering_versions_lowz[4])\n",
    "pp.single_plots(centering_versions_lowz[6])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig = pp.make_map(gc.all_data.RA.to_numpy(), gc.all_data['DEC'].to_numpy())\n",
    "\n",
    "#for i, gc in enumerate(centering_versions):\n",
    "#    fig = pp.make_map(gc.all_data.RA.to_numpy(), gc.all_data['DEC'].to_numpy(), fig=fig)\n",
    "\n",
    "#plot_positions(gc, *centering_versions, tiles_df=None, split=False, DEG_LONG=7, ra_min = 186.5, dec_min = 60)\n",
    "# BUG pass in all_data, not the GroupCatalog object\n",
    "plot_positions(gc.all_data, *centering_versions, tiles_df=None, split=False, DEG_LONG=6, ra_min = 147, dec_min = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lost Galaxy Luminosity Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a cut of SV3 whose completeness is similar to Y1 BGS.\n",
    "\n",
    "Question: is the luminosity function of lost galaxies (that were later observed) is different from the luminosity function observed galaxies?\n",
    "\n",
    "They seem similar; perhaps a mild slant. Overall it seems that trying to match the observed luminosity function with the lost ones is ok.\n",
    "\n",
    "Now for lost galaxies in 6pass that we have later got redshifts for.\n",
    "\n",
    "Question: What did our processing do to the luminosity function for lost galaxies?\n",
    "\n",
    "Our processing squeezes the luminosity function. We move galaxies from the wings towards the middle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Galaxy Neighborhood Examiner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_bsat_column(catalog: GroupCatalog):\n",
    "    bprob = 10\n",
    "    if 'beta0q' in catalog.GF_props:\n",
    "        beta0q = catalog.GF_props['beta0q']\n",
    "        beta0sf = catalog.GF_props['beta0sf']\n",
    "        betaLq = catalog.GF_props['betaLq']\n",
    "        betaLsf = catalog.GF_props['betaLsf']\n",
    "        bprob = np.zeros(len(catalog.all_data))\n",
    "        bprob = np.where(catalog.all_data['QUIESCENT'], beta0q + betaLq*(catalog.all_data['LOGLGAL']-9.5), beta0sf + betaLsf*(catalog.all_data['LOGLGAL']-9.5))\n",
    "        bprob = np.where(bprob < 0.001, 0.001, bprob)\n",
    "\n",
    "    catalog.all_data['BSAT'] = bprob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bgs_y1_hybrid8_mcmc = deserialize(cat.bgs_y1_hybrid8_mcmc)\n",
    "add_halo_columns(bgs_y3_pzp_2_4_c2)\n",
    "add_bsat_column(bgs_y3_pzp_2_4_c2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = bgs_y3_pzp_2_4_c2.all_data\n",
    "groups = bgs_y3_pzp_2_4_c2.centrals.loc[df['N_SAT'] > -1]\n",
    "groups = groups.loc[df['Z'] < 0.115]\n",
    "groups = groups.loc[df['Z'] > 0.095]\n",
    "groups = groups.loc[groups['RA'] > 185.4]\n",
    "groups = groups.loc[groups['RA'] < 185.55]\n",
    "groups = groups.loc[groups['DEC'] < 5.1]\n",
    "groups = groups.loc[groups['DEC'] > 4.9]\n",
    "#groups = groups.loc[groups['Z'] > 0.19]\n",
    "#groups = groups.loc[groups['M_HALO'] < 10**13.5]\n",
    "#groups = groups.loc[groups['M_HALO'] > 10**13.5]\n",
    "print(len(groups))\n",
    "#bighalos = df.sort_values('M_HALO', ascending=False).head(60)\n",
    "\n",
    "#brightest_gals = df.sort_values('L_GAL', ascending=False).head(60)\n",
    "\n",
    "lost_galaxies = df.loc[z_flag_is_not_spectro_z(df['Z_ASSIGNED_FLAG'])]\n",
    "obs_galaxies = df.loc[z_flag_is_spectro_z(df['Z_ASSIGNED_FLAG'])]\n",
    "#print(\"Lost galaxies: \", len(lost_galaxies), \"Observed Galaxies: \", len(obs_galaxies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['TARGETID'] == 39627908548923268]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.IGRP == 2058109, ['TARGETID', 'RA', 'DEC', 'Z', 'Z_ASSIGNED_FLAG', 'IS_SAT', 'G_R']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLOTS_TO_MAKE = 1\n",
    "GALAXY_POOL = groups.reset_index()\n",
    "\n",
    "#START_INDEX = 777\n",
    "#for i in range(START_INDEX, START_INDEX + PLOTS_TO_MAKE):\n",
    "#    index = lost_galaxies.index[i]\n",
    "#    examine_around(index)\n",
    "print(\"Number of galaxies to choose from: \", len(GALAXY_POOL))\n",
    "#indexes = np.random.randint(0, len(GALAXY_POOL)-1, size=PLOTS_TO_MAKE)\n",
    "indexes = np.arange(0, PLOTS_TO_MAKE)\n",
    "for i in indexes:\n",
    "    if i >= len(GALAXY_POOL):\n",
    "        break\n",
    "    target = GALAXY_POOL.iloc[i]\n",
    "    #print RA and DEC of cetnral\n",
    "    print(f\"RA: {target['RA']}, DEC: {target['DEC']}, Z: {target['Z']}, M_HALO: {target['M_HALO']:.1e}, N_SAT: {target['N_SAT']}, IGRP: {target['IGRP']}\")\n",
    "    pp.examine_groups_near(target, df, nearby_angle=coord.Angle('12m'), zfilt=0.02)\n",
    "    #deg = coord.Angle('5m').to(u.degree).value\n",
    "    #pp.examine_area(target.RA - deg, target.RA + deg, target.DEC - deg, target.DEC + deg, data)\n",
    "\n",
    "    \n",
    "    # Print off a file of RA, DEC, TARGETID of the group members\n",
    "    igrp = target['IGRP']\n",
    "    group_members = df.loc[df['IGRP'] == igrp]\n",
    "    with open(f'group_{i+1}_members.txt', 'w') as f:\n",
    "        f.write(\"RA,DEC,TARGETID\\n\")\n",
    "        for index, row in group_members.iterrows():\n",
    "            f.write(f\"{row['RA']},{row['DEC']},{row['TARGETID']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many halos were assigned below a certain cutoff?\n",
    "df = bgs_y1_pzp_2_4.all_data\n",
    "M_HALL_CUT = 10**11\n",
    "small_halo_df = df[df['M_HALO'] < M_HALL_CUT]\n",
    "\n",
    "print(len(small_halo_df), len(df))\n",
    "\n",
    "junk=plt.hist(small_halo_df.Z, bins=100)\n",
    "plt.xlabel('Redshift')\n",
    "plt.ylabel('Count')\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
