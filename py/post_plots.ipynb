{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import astropy.coordinates as coord\n",
    "import astropy.units as u\n",
    "import emcee\n",
    "import sys\n",
    "from astropy.table import Table\n",
    "\n",
    "if './SelfCalGroupFinder/py/' not in sys.path:\n",
    "    sys.path.append('./SelfCalGroupFinder/py/')\n",
    "from pyutils import *\n",
    "import plotting as pp\n",
    "from dataloc import *\n",
    "from bgs_helpers import *\n",
    "import catalog_definitions as cat\n",
    "from groupcatalog import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the group finder is run, this notebook is used to postprocess the results, generating plots and such for analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading existing datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mxxl_all=deserialize(cat.mxxl_all)\n",
    "#mxxl_fiberonly=deserialize(cat.mxxl_fiberonly)\n",
    "#mxxl_nn=deserialize(cat.mxxl_nn)\n",
    "mxxl_simple_4=deserialize(cat.mxxl_simple_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdss_vanilla_v2 = deserialize(cat.sdss_vanilla_v2)\n",
    "sdss_colors_v2 = deserialize(cat.sdss_colors_v2)\n",
    "sdss_colors_chi_v2 = deserialize(cat.sdss_colors_chi_v2)\n",
    "sdss_vanilla_v1 = deserialize(cat.sdss_vanilla)\n",
    "sdss_colors_v1 = deserialize(cat.sdss_colors)\n",
    "sdss_colors_chi_v1 = deserialize(cat.sdss_colors_chi)\n",
    "\n",
    "sdss_bgscut = deserialize(cat.sdss_bgscut)\n",
    "\n",
    "cat.sdss_published.postprocess()\n",
    "sdss_published = cat.sdss_published # It really is ~exactly sdss_colors_chi, which is great news for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bgs_y1_pzp_2_4 = deserialize(cat.bgs_y1_pzp_2_4)\n",
    "bgs_y1_pzp_2_4_c1 = deserialize(cat.bgs_y1_pzp_2_4_c1)\n",
    "bgs_y3_pzp_2_4 = deserialize(cat.bgs_y3_pzp_2_4)\n",
    "bgs_y3_pzp_2_4_c1 = deserialize(cat.bgs_y3_pzp_2_4_c1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This technique of removing tiles isn't as accurate as taking Y3 and cutting to SV3 footprint\n",
    "#bgs_sv3_nn_7p = deserialize(cat.bgs_sv3_nn_7p)\n",
    "#bgs_sv3_simple_5_7p = deserialize(cat.bgs_sv3_simple_5_7p)\n",
    "#bgs_sv3_pz_2_0_7p = deserialize(cat.bgs_sv3_pz_2_0_7p)\n",
    "#bgs_sv3_pz_2_4_7p = deserialize(cat.bgs_sv3_pz_2_4_7p)\n",
    "#bgs_sv3_pz_3_1_7p = deserialize(cat.bgs_sv3_pz_3_1_7p)\n",
    "\n",
    "# Our best source of BGS Truth\n",
    "bgs_sv3_pz_2_4_10p = deserialize(cat.bgs_sv3_pz_2_4_10p) \n",
    "bgs_sv3_pz_2_4_10p_c1 = deserialize(cat.bgs_sv3_pz_2_4_10p_c1) \n",
    "#bgs_sv3_pz_2_4_10p_old = deserialize(cat.bgs_sv3_pz_2_4_10p_old) \n",
    "bgs_sv3_fiberonly_10p = deserialize(cat.bgs_sv3_fiberonly_10p)\n",
    "#bgs_sv3_nn_10p = deserialize(cat.bgs_sv3_nn_10p)\n",
    "\n",
    "# These ones are the best way to compare main survey to SV3 for fiber incompleteness study\n",
    "bgs_y3_like_sv3_fiberonly = deserialize(cat.bgs_y3_like_sv3_fiberonly)\n",
    "bgs_y3_like_sv3_pz_2_4 = deserialize(cat.bgs_y3_like_sv3_pz_2_4)\n",
    "bgs_y3_like_sv3_pz_2_4_c1 = deserialize(cat.bgs_y3_like_sv3_pz_2_4_c1)\n",
    "bgs_y3_like_sv3_pz_2_0 = deserialize(cat.bgs_y3_like_sv3_pz_2_0)\n",
    "bgs_y3_like_sv3_nn = deserialize(cat.bgs_y3_like_sv3_nn)\n",
    "\n",
    "bgs_sv3_10p_mcmc = deserialize(cat.bgs_sv3_10p_mcmc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_u = deserialize(cat.uchuu_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MCMC of z assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best preprocess photo-z-plus v2 result for SV3\n",
    "path = f'/mount/sirocco1/imw2293/GROUP_CAT/mcmc13_m4_2_4.h5'\n",
    "reader = emcee.backends.HDFBackend(path, read_only=True)\n",
    "bgs_sv3_pz2_mcmcbest = BGSGroupCatalog.from_MCMC(reader, Mode.PHOTOZ_PLUS_v2)\n",
    "bgs_sv3_pz2_mcmcbest.run_group_finder(popmock=False)\n",
    "bgs_sv3_pz2_mcmcbest.postprocess()\n",
    "serialize(bgs_sv3_pz2_mcmcbest)\n",
    "\n",
    "# 64.94% neighbor\n",
    "#[6.32102907 1.59813654 1.49851461 3.18966963 0.83098626 2.83411711 3.26649451 1.75386219 1.95862571 2.56928697 0.91387857 1.70360255 3.6181996 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best preprocess photo-z-plus v3 result for SV3\n",
    "path = f'/mount/sirocco1/imw2293/GROUP_CAT/mcmc13_m4_3_1.h5'\n",
    "reader = emcee.backends.HDFBackend(path, read_only=True)\n",
    "bgs_sv3_pz3_mcmcbest = BGSGroupCatalog.from_MCMC(reader, Mode.PHOTOZ_PLUS_v3)\n",
    "bgs_sv3_pz3_mcmcbest.run_group_finder(popmock=False)\n",
    "bgs_sv3_pz3_mcmcbest.postprocess()\n",
    "serialize(bgs_sv3_pz3_mcmcbest)\n",
    "\n",
    "# 37% Neighbor for this, not bad\n",
    "#[3.68776334 1.03045493 1.00947751 2.79354858 0.88263756 1.15014321 2.71197235 0.63517952 1.44684275 2.63171751 1.16820625 0.96790557 3.02351026]\n",
    "\n",
    "# This one is great at 63.13% Neighbor.\n",
    "# [8.26010114 1.29383299 1.54671643 3.01349293 1.2229046  0.86286149 2.58828658 0.87067123 0.61260216 2.44470607 1.11635435 1.29386183 3.16506802]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case MCMC has been dumb, check similar parameter values\n",
    "import copy\n",
    "bb, rb, br, rr = bgs_sv3_pz3_mcmcbest.extra_params[1:13].reshape(4, 3)\n",
    "\n",
    "params = [bb, rb, br, rr]\n",
    "colors = [[0, 0, 1.0], [1.0, 0, 0.4], [0.2, 0.7, 0.2], [1.0, 0.0, 0.0]]\n",
    "variants = []\n",
    "\n",
    "for i, (param, color) in enumerate(zip(params, colors), start=1):\n",
    "    variant = BGSGroupCatalog(\n",
    "        f\"PZP 3 Variant {i}\",\n",
    "        bgs_sv3_pz3_mcmcbest.mode,\n",
    "        bgs_sv3_pz3_mcmcbest.mag_cut,\n",
    "        bgs_sv3_pz3_mcmcbest.catalog_mag_cut,\n",
    "        bgs_sv3_pz3_mcmcbest.sdss_fill,\n",
    "        bgs_sv3_pz3_mcmcbest.num_passes,\n",
    "        bgs_sv3_pz3_mcmcbest.drop_passes,\n",
    "        bgs_sv3_pz3_mcmcbest.data_cut,\n",
    "        bgs_sv3_pz3_mcmcbest.extra_params\n",
    "    )\n",
    "    variant.extra_params = [bgs_sv3_pz3_mcmcbest.extra_params[0], param, param, param, param]\n",
    "    variant.color = color\n",
    "    variant.preprocess()\n",
    "    variant.run_group_finder(popmock=False)\n",
    "    variant.postprocess()\n",
    "    variants.append(variant)\n",
    "\n",
    "bgs_sv3_pz3_mcmcbest_var1, bgs_sv3_pz3_mcmcbest_var2, bgs_sv3_pz3_mcmcbest_var3, bgs_sv3_pz3_mcmcbest_var4 = variants\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Release Candidate Catalogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sdss_published.sanity_tests() # The published catalog has some issues\n",
    "sdss_vanilla_v2.sanity_tests()\n",
    "sdss_colors_v2.sanity_tests()\n",
    "#sdss_colors_chi_v2.sanity_tests()\n",
    "\n",
    "sdss_vanilla_v1.sanity_tests()\n",
    "sdss_colors_v1.sanity_tests()\n",
    "#sdss_colors_chi_v1.sanity_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bgs_sv3_pz_2_4_10p_c1.basic_stats()\n",
    "bgs_sv3_pz_2_4_10p_c1.sanity_tests()\n",
    "bgs_y3_like_sv3_pz_2_4_c1.basic_stats()\n",
    "bgs_y3_like_sv3_pz_2_4_c1.sanity_tests()\n",
    "#bgs_y1_pzp_2_4.basic_stats()\n",
    "#bgs_y1_pzp_2_4.sanity_tests()\n",
    "bgs_y1_pzp_2_4_c1.basic_stats()\n",
    "bgs_y1_pzp_2_4_c1.sanity_tests()\n",
    "#bgs_y3_pzp_2_4.basic_stats()\n",
    "#bgs_y3_pzp_2_4.sanity_tests()\n",
    "bgs_y3_pzp_2_4_c1.basic_stats()\n",
    "bgs_y3_pzp_2_4_c1.sanity_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.plots(bgs_sv3_pz_2_4_10p, bgs_y1_pzp_2_4, bgs_y3_pzp_2_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pp.plots(bgs_sv3_pz_2_4_10p_c1, bgs_y3_like_sv3_pz_2_4_c1, bgs_y1_pzp_2_4_c1, bgs_y3_pzp_2_4_c1, sdss_colors_chi_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bgs_y1_pzp_2_4_c1.name = \"BGS Y1\"\n",
    "bgs_y1_pzp_2_4_c1.color = 'k'\n",
    "bgs_y1_pzp_2_4_c1.marker = '-'\n",
    "sdss_published.name = \"SDSS\"\n",
    "pp.plots(bgs_y1_pzp_2_4_c1, sdss_published)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.qf_cen_plot(bgs_sv3_pz_2_4_10p, test_methods=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bgs_y1_pzp_2_4.all_data.sort_values('L_GAL', ascending=False).loc[:, ['RA', 'DEC', 'Z', 'Z_PHOT', 'Z_ASSIGNED_FLAG']].head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bgs_y1_pzp_2_4.color = 'red'\n",
    "bgs_y3_pzp_2_4.color = 'blue'\n",
    "pp.plots(bgs_sv3_pz_2_4_10p, bgs_y1_pzp_2_4, bgs_y3_pzp_2_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.plots(sdss_colors_chi_v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fiber Incompleteness Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SV3 'Truth' Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bgs_sv3_pz_2_4_10p.calculate_projected_clustering(with_extra_randoms=True)\n",
    "#bgs_sv3_pz_2_4_10p.calculate_projected_clustering_in_magbins(with_extra_randoms=True)\n",
    "\n",
    "# TODO run this again\n",
    "bgs_sv3_pz_2_4_10p.add_jackknife_err_to_proj_clustering(with_extra_randoms=True, for_mag_bins=False)\n",
    "#bgs_sv3_pz_2_4_10p.add_jackknife_err_to_proj_clustering(with_extra_randoms=False, for_mag_bins=True) # BUG Broken\n",
    "serialize(bgs_sv3_pz_2_4_10p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.printoptions(precision=2, suppress=True, linewidth=200):\n",
    "    print(bgs_sv3_pz_2_4_10p.wp_all_extra[1])\n",
    "    print(bgs_sv3_pz_2_4_10p.wp_err)\n",
    "\n",
    "    std_devs = np.sqrt(np.diag(bgs_sv3_pz_2_4_10p.wp_cov))\n",
    "    print(std_devs)\n",
    "\n",
    "    print(bgs_sv3_pz_2_4_10p.wp_cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question - does using the small set of randoms vs the full set of randoms make a difference?\n",
    "percent_diff = np.abs(bgs_sv3_pz_2_4_10p.wp_all[1] - bgs_sv3_pz_2_4_10p.wp_all_extra[1]) / bgs_sv3_pz_2_4_10p.wp_all[1] * 100\n",
    "print(percent_diff)\n",
    "red_p_diff = np.abs(bgs_sv3_pz_2_4_10p.wp_all[2] - bgs_sv3_pz_2_4_10p.wp_all_extra[2]) / bgs_sv3_pz_2_4_10p.wp_all[2] * 100\n",
    "print(red_p_diff)\n",
    "blue_p_diff = np.abs(bgs_sv3_pz_2_4_10p.wp_all[3] - bgs_sv3_pz_2_4_10p.wp_all_extra[3]) / bgs_sv3_pz_2_4_10p.wp_all[3] * 100\n",
    "print(blue_p_diff)\n",
    "# Answer - Less than 1% generally\n",
    "\n",
    "colors = ['k', 'r', 'b']\n",
    "f = bgs_sv3_pz_2_4_10p\n",
    "plt.figure(figsize=(5, 5))\n",
    "if f.wp_err is not None:\n",
    "    plt.errorbar(f.wp_all[0][:-1], f.wp_all[1], yerr=f.wp_err, marker='o', linestyle='-', label='All', color=colors[0], alpha=0.5)\n",
    "    plt.errorbar(f.wp_all[0][:-1], f.wp_all[2], yerr=f.wp_r_err, marker='o', linestyle='-', label='Red', color=colors[1], alpha=0.5)\n",
    "    plt.errorbar(f.wp_all[0][:-1], f.wp_all[3], yerr=f.wp_b_err, marker='o', linestyle='-', label='Blue', color=colors[2], alpha=0.5)\n",
    "else:\n",
    "    plt.plot(f.wp_all[0][:-1], f.wp_all[1], marker='o', linestyle='-', label='All', color=colors[0])\n",
    "    plt.plot(f.wp_all[0][:-1], f.wp_all[2], marker='o', linestyle='-', label='Red', color=colors[1])\n",
    "    plt.plot(f.wp_all[0][:-1], f.wp_all[3], marker='o', linestyle='-', label='Blue', color=colors[2])\n",
    "\n",
    "plt.plot(f.wp_all_extra[0][:-1], f.wp_all_extra[1], marker='x', linestyle='--', label='All Extra Rands', color=colors[0])\n",
    "plt.plot(f.wp_all_extra[0][:-1], f.wp_all_extra[2], marker='x', linestyle='--', label='Red Extra Rands', color=colors[1])\n",
    "plt.plot(f.wp_all_extra[0][:-1], f.wp_all_extra[3], marker='x', linestyle='--', label='Blue Extra Rands', color=colors[2])\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.ylim(8, 2000)\n",
    "plt.yscale('log')\n",
    "plt.xlabel(r'$r_p$ [Mpc/h]')\n",
    "plt.ylabel(r'$w_p(r_p)$')\n",
    "plt.legend()\n",
    "plt.title('Full Sample $w_p(r_p)$ ')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sets = [bgs_y3_like_sv3_fiberonly, bgs_y3_like_sv3_nn, bgs_y3_like_sv3_pz_2_0, bgs_y3_like_sv3_pz_2_4] \n",
    "#sets = [bgs_y3_like_sv3_pz_2_4_c1]\n",
    "\n",
    "bgs_y3_like_sv3_pz_2_4.name = \"New Technique\"\n",
    "bgs_y3_like_sv3_pz_2_4_c1.name = \"New Technique\"\n",
    "bgs_y3_like_sv3_nn.name = \"Use Nearest Neighbor\"\n",
    "bgs_y3_like_sv3_pz_2_0.name = \"Use Photo-z\"\n",
    "bgs_y3_like_sv3_fiberonly.name = \"Drop Lost Galaxies\"\n",
    "bgs_sv3_pz_2_4_10p.name = \"SV3 ~Truth\"\n",
    "bgs_sv3_pz_2_4_10p_c1.name = \"SV3 ~Truth C1\"\n",
    "bgs_sv3_pz_2_4_10p.color = 'k'\n",
    "bgs_sv3_pz_2_4_10p_c1.color = 'k'\n",
    "\n",
    "for s in [bgs_sv3_pz_2_4_10p_c1, *sets]:\n",
    "    print(f\"--- {s.name} ---\")\n",
    "    print(f\"Has extra randoms: {s.wp_all_extra is not None}\")\n",
    "    # Check if s.wp_slices_extra is None or amy it's slices are None\n",
    "    flag = s.wp_slices_extra !=  np.repeat(None, len(s.wp_slices_extra))\n",
    "    print(f\"Has extra randoms for slices: {flag}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.plots(*sets, show_err=bgs_sv3_pz_2_4_10p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth_catalog = bgs_sv3_pz_2_4_10p\n",
    "truth_df = truth_catalog.all_data\n",
    "for s in sets:\n",
    "    print(s.name)\n",
    "    s.get_true_z_from(truth_df)\n",
    "    s.refresh_df_views()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO debug all this\n",
    "pp.Lfunc_compare(bgs_y3_like_sv3_fiberonly, bgs_sv3_pz_2_4_10p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.correct_redshifts_assigned_plot(bgs_y3_like_sv3_nn, bgs_y3_like_sv3_pz_2_0, bgs_y3_like_sv3_pz_2_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.luminosity_function_plots(bgs_y3_like_sv3_nn, bgs_y3_like_sv3_pz_2_0, bgs_y3_like_sv3_pz_2_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in sets:\n",
    "    pp.single_plots(s)\n",
    "\n",
    "pp.single_plots(bgs_sv3_pz_2_4_10p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in sets:\n",
    "    data = s.all_data.loc[z_flag_is_not_spectro_z(s.all_data['Z_ASSIGNED_FLAG'])]\n",
    "    delta_red = data['Z'] - data['Z_T'] # I used to do z_obs for SV3 dropping passes... TODO\n",
    "    plt.hist(delta_red, bins=100, range=(-0.05, 0.05), histtype='step', label=s.name, color=s.color)\n",
    "    plt.yscale('log')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO BUG - issue. Targets in the 'truth' catalog that are NOT IN the other one aren't accounted for in the purity and completeness calculations.\n",
    "# This becomes obvious issue for the fiberonly runs.\n",
    "pp.test_purity_and_completeness(*sets, truth_catalog=truth_catalog, lost_only=False)\n",
    "pp.purity_complete_plots(*sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a bar plot of the z_assigned_flag values for each set\n",
    "for s in sets:\n",
    "    j=plt.hist(s.all_data['Z_ASSIGNED_FLAG'], bins=[-3,-2,-1,0,1,2,3,4,5,6,7,8,9,10,11,12], histtype='step', label=s.name)\n",
    "\n",
    "#plt.yscale('log')\n",
    "plt.ylim(0, 10000)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Popmock Lsat / Clustering Results\n",
    "Must have run popmock and corrfunc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "popmock_sets = [sdss_vanilla_v1, sdss_vanilla_v2, sdss_colors_v1, sdss_colors_v2, sdss_colors_chi_v1, sdss_colors_chi_v2]\n",
    "popmock_sets = [bgs_sv3_10p_mcmc, bgs_y1_pzp_2_4, bgs_y1_pzp_2_4_c1]\n",
    "\n",
    "for s in popmock_sets:\n",
    "    print(f\"--- {s.name} ---\")\n",
    "    s.chisqr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in popmock_sets:\n",
    "    pp.proj_clustering_plot(s)\n",
    "\n",
    "for s in popmock_sets:\n",
    "    pp.lsat_data_compare_plot(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv3_pip_bad = pickle.load(open(OUTPUT_FOLDER + 'sv3_pip_clustering.pkl', 'rb'))\n",
    "sv3_pip_clustering = pickle.load(open(OUTPUT_FOLDER + 'sv3_pip_clustering_proper.pkl', 'rb'))\n",
    "y3_likesv3_clustering = pickle.load(open(OUTPUT_FOLDER + 'y3_likesv3_pip_clustering_proper.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Clustering from processed ones to ~Truth\n",
    "for s in sets:\n",
    "    pp.wp_rp(s)\n",
    "    pp.compare_wp_rp(s, bgs_sv3_pz_2_4_10p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.wp_rp(bgs_sv3_pz_2_4_10p)\n",
    "pp.wp_rp(sv3_pip_bad)\n",
    "pp.wp_rp(sv3_pip_clustering)\n",
    "pp.wp_rp(y3_likesv3_clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load up SV3 PIP Clustering as created by the LS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does using randoms from full vs clustering with weights matter?\n",
    "pp.compare_wp_rp(sv3_pip_bad, sv3_pip_clustering)\n",
    "# BUG how is overall not average of red and blue?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This plot really should be close to 0% difference\n",
    "# It's SV3 10p, so 2% filled with my method, compared to the PIP method\n",
    "pp.compare_wp_rp(bgs_sv3_pz_2_4_10p, sv3_pip_clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If LSS team is calculating PIP weights right and I'm doing nothing wrong,\n",
    "# then these two should also largely agree\n",
    "pp.compare_wp_rp(y3_likesv3_clustering, sv3_pip_clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rough check that the footprints all match up\n",
    "for s in sets:\n",
    "    fig=pp.make_map(s.all_data['RA'].to_numpy(), s.all_data['DEC'].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other PLots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdss_colors_chi_v2.Mr_gal_labels = Mr_gal_labels[15:]\n",
    "sdss_colors_chi_v1.Mr_gal_labels = Mr_gal_labels[15:]\n",
    "sdss_colors_chi_v2.color = 'darkred'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.plots(sdss_colors_chi_v2, sdss_colors_chi_v1, deserialize(cat.bgs_sv3_10p_mcmc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sets = [bgs_y3_like_sv3_fiberonly, bgs_y3_like_sv3_nn, bgs_y3_like_sv3_pz_2_0, bgs_y3_like_sv3_pz_2_4] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Targets in SV3 region observed in main survey got new redshift measurements\n",
    "# Q: How different are those z's compared to SV3 z's? \n",
    "# A: They are similar, but not identical. The difference is less than 0.001 for 99.7% so it's OK for us I think.\n",
    "#    (Subdominant to v_peculiar)\n",
    "\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astropy import units as u\n",
    "from astropy.coordinates import match_coordinates_sky\n",
    "\n",
    "def find_unique_and_matched_objects(cat1, cat2):\n",
    "    df1 = cat1.all_data.loc[z_flag_is_spectro_z(cat1.all_data['Z_ASSIGNED_FLAG'])].reset_index()\n",
    "    df2 = cat2.all_data.loc[z_flag_is_spectro_z(cat2.all_data['Z_ASSIGNED_FLAG'])].reset_index()\n",
    "                                                 \n",
    "    # Extract RA and Dec from the catalogs\n",
    "    ra1, dec1 = df1['RA'].to_numpy(), df1['DEC'].to_numpy()\n",
    "    ra2, dec2 = df2['RA'].to_numpy(), df2['DEC'].to_numpy()\n",
    "        \n",
    "    # Create SkyCoord objects\n",
    "    coords1 = SkyCoord(ra=ra1*u.degree, dec=dec1*u.degree)\n",
    "    coords2 = SkyCoord(ra=ra2*u.degree, dec=dec2*u.degree)\n",
    "    \n",
    "    # Match coordinates\n",
    "    idx, d2d, _ = match_coordinates_sky(coords1, coords2)\n",
    "\n",
    "    df1['FID'] = idx\n",
    "    df2['FID'] = df2.index\n",
    "    \n",
    "    # Find objects in df1 that are not in df2\n",
    "    unique_mask = d2d > 1*u.arcsec  # You can adjust the threshold as needed\n",
    "\n",
    "    # join with df2 for matched_objects on the FID\n",
    "    matched_objects = df1.join(df2.set_index('FID'), on='FID', rsuffix='_2')\n",
    "    matched_objects = matched_objects[~unique_mask]\n",
    "\n",
    "    print(f\"Total spectroscopic galaxies in cat1: {len(df1)}, cat2: {len(df2)}\")\n",
    "    print(f'Unique objects in cat1: {unique_mask.sum()}, Matched objects in cat1: {len(matched_objects)}')\n",
    "    \n",
    "    return df1[unique_mask], matched_objects\n",
    "\n",
    "# Example usage\n",
    "unique_objects, matched_objects = find_unique_and_matched_objects(bgs_sv3_pz_2_4_10p, bgs_y3_like_sv3_pz_2_4)\n",
    "\n",
    "print(np.isclose(matched_objects['Z'], matched_objects['z_2'], atol=0.001, rtol=0).sum() / len(matched_objects))\n",
    "#fig=pp.make_map(unique_objects.RA.to_numpy(), unique_objects['DEC'].to_numpy())\n",
    "\n",
    "plt.hist(matched_objects['Z'] - matched_objects['z_2'], bins=np.linspace(-0.005, 0.005, 100))\n",
    "plt.yscale('log')\n",
    "\n",
    "# Draw verticle line at 0.005\n",
    "plt.axvline(x=0.005, color='r', linestyle='--')\n",
    "plt.axvline(x=-0.005, color='r', linestyle='--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SV3 10p and SDSS BGS-cut are very similar!\n",
    "bgs_sv3_pz_2_4_10p.color = 'k'\n",
    "pp.plots(bgs_sv3_pz_2_4_10p, sdss_bgscut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cutting SDSS to remove regions with poor BGS overlap barely improves the completeness\n",
    "pp.plots(sdss_bgscut, sdss_vanilla_v2)\n",
    "print(f\"{spectroscopic_complete_percent(sdss_bgscut.all_data['Z_ASSIGNED_FLAG']):.2f}% spectroscopic complete for BGS cut\")\n",
    "print(f\"{spectroscopic_complete_percent(sdss_vanilla_v2.all_data['Z_ASSIGNED_FLAG']):.2f}% spectroscopic complete for Vanilla\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdss_vanilla_v2.marker = '--'\n",
    "sdss_colors_mine.color = 'navy'\n",
    "sdss_colors_mine.marker = '--'\n",
    "sdss_colors_chi_v2.color = 'deeppink'\n",
    "sdss_colors_chi_v2.marker = '--'\n",
    "bgs_y1_pzp_2_4.color = 'brown'\n",
    "bgs_y1_pzp_2_4.marker = '-'\n",
    "pp.plots(bgs_y1_pzp_2_4, bgs_sv3_pz_2_4_10p, sdss_vanilla_v2, sdss_colors_mine, sdss_colors_chi_v2)\n",
    "#pp.plots(cat.sdss_published, sdss_colors_chi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why doesn't mstar missing % exactly match z_assigned_flag? \n",
    "# Probably redshift failures. Still have a spectra so still have mstar\n",
    "print(np.sum(np.isnan(bgs_y1_pzp_2_4.all_data['MSTAR'])) / len(bgs_y1_pzp_2_4.all_data['MSTAR']))\n",
    "print(np.sum(bgs_y1_pzp_2_4.all_data['Z_ASSIGNED_FLAG'] != 0) / len(bgs_y1_pzp_2_4.all_data['Z_ASSIGNED_FLAG']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.compare_fsat_color_split(sdss_vanilla_v1, sdss_vanilla_v2, project_percent=0.52)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.compare_fsat_color_split(bgs_sv3_pz_2_4_10p, sdss_vanilla_v2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.qf.centered_plot(bgs_y1_pzp_2_4)\n",
    "pp.qf.centered_plot(sdss_published)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.fsat_by_z_bins(bgs_y1_pzp_2_4, z_bins=np.array([0.0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.5, 1.0]), show_plots=True)\n",
    "#pp.fsat_by_z_bins(mxxl_simple_4, z_bins=np.array([0.0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.5, 1.0]), show_plots=False, aggregation=pp.fsat_truth_vmax_weighted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(bgs_y1_pzp_2_4.all_data['MSTAR'].dropna(), np.logspace(6, 13, 100))\n",
    "plt.xlabel('Stellar Mass')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Stellar Masses for bgs_y1_pzp_2_4.all_data')\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bgs_y1_pzp_2_4.all_data['Mstar_bin'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bgs_y1_pz_2_4.postprocess()\n",
    "#bgs_y1_pz_2_4.all_data['Mstar_bin'] = pd.cut(x = bgs_y1_pz_2_4.all_data['MSTAR'], bins = mstar_bins, labels = mstar_labels, include_lowest = True)\n",
    "pp.qf_cen_plot(bgs_y1_pzp_2_4, mstar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.fsat_by_z_bins(bgs_y1_pzp_2_4, z_bins=np.array([0.0, 0.2, 1.0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out biggest group size\n",
    "for dataset in [bgs_y1_pzp_2_4, sdss_vanilla_v2]:\n",
    "    print(dataset.name)\n",
    "    print(dataset.all_data.groupby('IGRP').size().max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SDSS Examine Bimodality\n",
    "\n",
    "z=sdss_vanilla_v2.all_data['Z']\n",
    "gmr=sdss_vanilla_v2.all_data['MAG_G'] - sdss_vanilla_v2.all_data['MAG_R']\n",
    "junk=plt.hist(gmr, bins=np.linspace(-1,3,300), alpha=0.4)\n",
    "#junk=plt.hist(k_correct(sdss_vanilla.all_data['MAG_G'], z, gmr, band='g')  - k_correct(sdss_vanilla.all_data['MAG_R'], z, gmr, band='r'), bins=500, alpha=0.4)\n",
    "junk=plt.hist(sdss_vanilla_v2.all_data['DN4000'], bins=np.linspace(0,4,300), alpha=0.4)\n",
    "plt.xlim(-1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate changes in halo mass function from wcen\n",
    "m1=np.log10(sdss_vanilla_v2.all_data['M_HALO'])\n",
    "m2=np.log10(sdss_colors_v2.all_data['M_HALO'])\n",
    "m3=np.log10(sdss_colors_chi_v2.all_data['M_HALO'])\n",
    "\n",
    "# bin m1,m2,m3 the same way\n",
    "n_bins = 20\n",
    "bins = np.linspace(10.8, 15.0, n_bins)\n",
    "d1 = np.digitize(m1, bins)\n",
    "d2 = np.digitize(m2, bins)\n",
    "d3 = np.digitize(m3, bins)\n",
    "\n",
    "# count the number of galaxies in each bin\n",
    "n1 = np.array([np.sum(d1==i) for i in range(1, n_bins+1)])\n",
    "n2 = np.array([np.sum(d2==i) for i in range(1, n_bins+1)])\n",
    "n3 = np.array([np.sum(d3==i) for i in range(1, n_bins+1)])\n",
    "\n",
    "# Do the same but for log10(counts)\n",
    "n1 = np.log10(n1)\n",
    "n2 = np.log10(n2)\n",
    "n3 = np.log10(n3)\n",
    "print(n1,n2,3)\n",
    "\n",
    "# Log difference\n",
    "p1 = np.abs(n1-n2)\n",
    "p2 = np.abs(n1-n3)\n",
    "\n",
    "plt.plot(bins, p1, label='SDSS Colors vs Vanilla')\n",
    "plt.plot(bins, p2, label='SDSS Colors+Chi vs Vanilla')\n",
    "\n",
    "plt.xlabel('log10(M_halo)')\n",
    "plt.ylabel('Log10 Difference in Counts')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make single group CSV for legacysurvey.org/viewer visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(OUTPUT_FOLDER + 'NERSC_BGS_1pass_v1.out')\n",
    "centrals_of_big_groups = df['N_SAT'] > 0\n",
    "group_ids = df.loc[centrals_of_big_groups]['IGRP'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['IGRP'] == 1644058]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(group_ids[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in group_ids[0:10]:\n",
    "for i in [1644058, 1644051]:\n",
    "    #df.loc[df['IGRP'] == i, ['RA', 'DEC']].to_csv(OUTPUT_FOLDER + f'group{i}.csv', index=False)\n",
    "    print(df.loc[df['IGRP'] == i, ['RA', 'DEC', 'Z', 'Z_ASSIGNED_FLAG']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Study z_phot vs z_spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = bgs_sv3_pz_2_4_10p.all_data\n",
    "\n",
    "low_cut = 0.0001 # not a dramatic shift when moving from here to 0.1\n",
    "quality = (df['Z_PHOT'] != NO_PHOTO_Z) & z_flag_is_spectro_z(df['Z_ASSIGNED_FLAG']) & (df['Z_OBS'] > low_cut) & (df['L_GAL'] > 3E10)\n",
    "#quality = (df['Z_PHOT'] != NO_PHOTO_Z) & z_flag_is_spectro_z(df['Z_ASSIGNED_FLAG']) & (df['Z_OBS'] > low_cut)\n",
    "\n",
    "# Investigate the photo-z error distribution for red and blue galaxies\n",
    "# Blue exhibits and offset and a less peaked distribution than red\n",
    "# Red does have some skew\n",
    "data = df.loc[np.logical_and(df['QUIESCENT'], quality)]\n",
    "blue = df.loc[np.logical_and(~df['QUIESCENT'], quality)]\n",
    "delta_red = data['Z_PHOT'] - data['Z_OBS']\n",
    "delta_blue = blue['Z_PHOT'] - blue['Z_OBS']\n",
    "delta_all = df.loc[quality, 'Z_PHOT'] - df.loc[quality, 'Z_OBS']\n",
    "\n",
    "x = 0.1\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(delta_red, bins=50, range=(-x, x), histtype='step', color='red', label='Red')\n",
    "plt.hist(delta_blue, bins=50, range=(-x, x), histtype='step', color='blue', label='Blue')\n",
    "plt.hist(delta_all, bins=50, range=(-x, x), histtype='step', color='k', label='All')\n",
    "#plt.yscale('log')\n",
    "plt.xlabel('Photo-z - Spectro-z')\n",
    "plt.legend()\n",
    "\n",
    "# draw a vertical line at 0\n",
    "plt.axvline(0, color='black', lw=1)\n",
    "plt.axvline(-SIM_Z_THRESH, color='green')\n",
    "plt.axvline(SIM_Z_THRESH, color='green')\n",
    "\n",
    "percentiles = np.percentile(delta_all, [16, 50, 84])\n",
    "print(f\"Median delta z: {percentiles[1]:.4f}, 16th percentile: {percentiles[0]:.4f}, 84th percentile: {percentiles[2]:.4f}\")\n",
    "# add bars for the percentiles\n",
    "#plt.axvline(percentiles[0], color='green')\n",
    "#plt.axvline(percentiles[2], color='green')\n",
    "\n",
    "# What % fall within 0.005 of the true redshift?\n",
    "within_5_milli = np.abs(delta_all) < SIM_Z_THRESH\n",
    "print(f\"{np.sum(within_5_milli) / len(delta_all) * 100:.2f}% of galaxies have a photometric redshift within {SIM_Z_THRESH} of the spectroscopic redshift.\")\n",
    "print(f\"For red: {np.sum(np.abs(delta_red) < SIM_Z_THRESH) / len(delta_red) * 100:.2f}%\")\n",
    "print(f\"For blue: {np.sum(np.abs(delta_blue) < SIM_Z_THRESH) / len(delta_blue) * 100:.2f}%\")\n",
    "\n",
    "# Find the +/- that gives 95% of the data\n",
    "percentiles = np.percentile(delta_all, [2.5, 97.5])\n",
    "print(f\"2.5th percentile: {percentiles[0]:.4f}, 97.5th percentile: {percentiles[1]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BGS and SDSS Target Overlap Analysis\n",
    "\n",
    "TODO: SDSS magnitudes have e-corrections of them that fastspecfit on BGS does not have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sloan BGS overlap - are their abs mag's the same? \n",
    "# Control for different z fits\n",
    "\n",
    "bgs_to_use = bgs_y3_pzp_2_4_c1.all_data\n",
    "#lost_bgs = bgs_to_use.loc[z_flag_is_not_spectro_z(bgs_to_use['Z_ASSIGNED_FLAG'])]\n",
    "sdss_obs = sdss_vanilla_v1.all_data.loc[sdss_vanilla_v1.all_data['Z_ASSIGNED_FLAG'] == AssignedRedshiftFlag.SDSS_SPEC.value]\n",
    "bgs_obs = bgs_to_use.loc[bgs_to_use['Z_ASSIGNED_FLAG'] == AssignedRedshiftFlag.DESI_SPEC.value].reset_index()\n",
    "\n",
    "catalog = coord.SkyCoord(ra=sdss_obs.RA.to_numpy()*u.degree, dec=sdss_obs['DEC'].to_numpy()*u.degree, frame='icrs')\n",
    "to_match = coord.SkyCoord(ra=bgs_obs.RA.to_numpy()*u.degree, dec=bgs_obs['DEC'].to_numpy()*u.degree, frame='icrs')\n",
    "\n",
    "idx, d2d, d3d = coord.match_coordinates_sky(to_match, catalog, nthneighbor=1, storekdtree=False)\n",
    "matched = d2d < 1*u.arcsec\n",
    "\n",
    "bgs_obs['SDSS_LOGLGAL'] = np.where(matched, sdss_obs['LOGLGAL'].to_numpy()[idx], np.nan)\n",
    "bgs_obs['SDSS_Z'] = np.where(matched, sdss_obs['Z'].to_numpy()[idx], np.nan)\n",
    "bgs_obs = bgs_obs.loc[matched]\n",
    "print(f\"Matched {len(bgs_obs)} out of {len(bgs_to_use)}\")\n",
    "\n",
    "bgs_obs['M_R'] = log_solar_L_to_abs_mag_r(bgs_obs['LOGLGAL'])\n",
    "Q=1.6\n",
    "e_corr = Q*(bgs_obs['SDSS_Z'] - 0.1)\n",
    "bgs_obs['SDSS_M_R'] = log_solar_L_to_abs_mag_r(bgs_obs['SDSS_LOGLGAL']) - e_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograms to compare\n",
    "sample = bgs_obs\n",
    "sample = bgs_obs.loc[np.logical_or(bgs_obs['M_R'] < -22, bgs_obs['SDSS_M_R'] < -22)]\n",
    "\n",
    "# plot the difference\n",
    "diff = sample['M_R'] - sample['SDSS_M_R']\n",
    "delta_z = sample['Z'] - sample['SDSS_Z']\n",
    "zagreed = np.abs(delta_z) < 0.0001\n",
    "print(f\"Catastrophic z mismatch: {np.sum(np.abs(delta_z) > 0.005) / len(delta_z) * 100:.2f}%\")\n",
    "diff = diff[zagreed]\n",
    "print(f\"Keeping {len(diff)} out of {len(sample)}\")\n",
    "print(f\"Catastrophic Mr mismatch: {np.sum(np.abs(diff) > 0.5) / len(diff) * 100:.2f}%\")\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(diff, bins=np.linspace(-1, 1, 100), histtype='step')\n",
    "plt.xlabel('Difference in M_R')\n",
    "plt.ylabel('Number of Galaxies')\n",
    "plt.title('BGS $M_r$ - SDSS $M_r$')\n",
    "plt.axvline(0, color='red', linestyle='--')\n",
    "plt.yscale('log')\n",
    "plt.tight_layout()\n",
    "plt.plot()\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(delta_z, bins=np.linspace(-0.005, 0.005, 200), histtype='step')\n",
    "plt.xlabel('Difference in z')\n",
    "plt.ylabel('Number of Galaxies')\n",
    "plt.title('BGS z - SDSS z')\n",
    "plt.axvline(0, color='red', linestyle='--')\n",
    "plt.yscale('log')\n",
    "plt.tight_layout()\n",
    "plt.plot()\n",
    "\n",
    "# For 1,2,3 sigma\n",
    "percentages = np.percentile(np.abs(diff), [68, 95, 99.7])\n",
    "print(f\"68% of galaxies have an absolute difference within {percentages[0]:.2f}\")\n",
    "print(f\"95% of galaxies have an absolute difference within {percentages[1]:.2f}\")\n",
    "print(f\"99.7% of galaxies have an absolute difference within {percentages[2]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# difference in abs mag as a function of BGS mag\n",
    "delta_z = bgs_obs['Z'] - bgs_obs['SDSS_Z']\n",
    "zagreed = np.abs(delta_z) < 0.0001\n",
    "bgs_obs_zagreed = bgs_obs.loc[zagreed]\n",
    "\n",
    "# make abs mag bins\n",
    "bins = np.linspace(-23.5, -16.5, 29)\n",
    "bgs_obs_zagreed['M_R_BIN'] = pd.cut(bgs_obs_zagreed['M_R'], bins=bins, labels=bins[:-1], include_lowest=True)\n",
    "bgs_obs_zagreed['M_R_DIFF'] = bgs_obs_zagreed['M_R'] - bgs_obs_zagreed['SDSS_M_R']\n",
    "bgs_obs_zagreed['M_R_ABS_DIFF'] = np.abs(bgs_obs_zagreed['M_R'] - bgs_obs_zagreed['SDSS_M_R'])\n",
    "\n",
    "# Print off counts in each bin\n",
    "counts = bgs_obs_zagreed.groupby('M_R_BIN').size()\n",
    "#print(counts)\n",
    "\n",
    "# Within each bin, calculate the median and 2sigma differences\n",
    "binned = bgs_obs_zagreed.groupby('M_R_BIN')['M_R_DIFF'].apply(lambda x: np.percentile(x, [2.5, 50, 97.5]))\n",
    "binned = pd.DataFrame(binned.tolist(), index=binned.index, columns=['2.5', 'median', '97.5'])\n",
    "\n",
    "plt.figure()\n",
    "plt.errorbar(binned.index, binned['median'], yerr=[binned['median'] - binned['2.5'], binned['97.5'] - binned['median']], fmt='o', capsize=3)\n",
    "plt.xlabel('$M_r^{BGS}$')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('95% Interval of $M_r^{BGS} - M_r^{SDSS}$')\n",
    "plt.ylim(-0.8, 0.8)\n",
    "plt.title('BGS vs SDSS Difference in $M_r$')\n",
    "plt.axhline(0, color='red', linestyle='--')\n",
    "plt.tight_layout()\n",
    "plt.grid()\n",
    "plt.plot()\n",
    "\n",
    "# Within each bin, calculate the median and 2sigma differences\n",
    "#binned = bgs_obs.groupby('M_R_BIN')['M_R_ABS_DIFF'].agg(['median', 'std'])\n",
    "# Don't use std, we want asymmetric errors\n",
    "binned = bgs_obs_zagreed.groupby('M_R_BIN')['M_R_ABS_DIFF'].apply(lambda x: np.percentile(x, [2.5, 50, 97.5]))\n",
    "binned = pd.DataFrame(binned.tolist(), index=binned.index, columns=['2.5', 'median', '97.5'])\n",
    "\n",
    "plt.figure()\n",
    "plt.errorbar(binned.index, binned['median'], yerr=[binned['median'] - binned['2.5'], binned['97.5'] - binned['median']], fmt='o', capsize=3)\n",
    "plt.xlabel('$M_r^{BGS}$')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('95% Interval of |$M_r^{BGS} - M_r^{SDSS}$|')\n",
    "plt.ylim(-0.05, 1.0)\n",
    "plt.title('BGS vs SDSS Absolute Difference in $M_r$')\n",
    "plt.axhline(0, color='red', linestyle='--')\n",
    "plt.tight_layout()\n",
    "plt.grid()\n",
    "plt.plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Info for clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bgs_sv3_pz_2_4_10p.all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bgs_sv3_pz_2_4_10p.all_data\n",
    "\n",
    "tbl = Table.read(IAN_BGS_SV3_MERGED_NOY3_FILE)\n",
    "tbl.keep_columns(['TARGETID', 'ABSMAG01_SDSS_R'])\n",
    "df = tbl.to_pandas()\n",
    "\n",
    "df = bgs_sv3_pz_2_4_10p.all_data.join(df.set_index('TARGETID'), on='TARGETID')\n",
    "\n",
    "df['R'] = log_solar_L_to_abs_mag_r(df['LOGLGAL'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why are some observed redshifts higher than my theoretical max? \n",
    "# Peculiar velocity of course. Since it should be symmetric in the redshift direction, it's okay to use the theoretical max.\n",
    "mags = np.array([-14, -15, -16, -17, -18, -19, -20, -21, -22, -23])\n",
    "\n",
    "LIM = 17.6\n",
    "print(\"SDSS FLUXLIM\")\n",
    "print(\"DIM\")\n",
    "for m in mags:\n",
    "    print(f\"M_R={m}: THEORY MAX={get_max_observable_z(m, LIM).value:.5f}\")\n",
    "    #print(f\"M_R={m}: THEORY MAX={get_max_observable_z_m30(m, LIM).value:.5f}\")\n",
    "print(\"BRIGHT\")\n",
    "\n",
    "LIM = 19.5\n",
    "print(\"BGS BRIGHT FLUXLIM\")\n",
    "print(\"DIM\")\n",
    "for m in mags:\n",
    "    print(f\"M_R={m}: THEORY MAX={get_max_observable_z(m, LIM).value:.5f}\")\n",
    "print(\"BRIGHT\")\n",
    "\n",
    "#LIM = 20.0\n",
    "#print(\"BGS FAINT FLUXLIM\")\n",
    "#print(\"DIM\")\n",
    "#for m in mags:\n",
    "#    print(f\"M_R={m}: THEORY MAX={get_max_observable_z(m, LIM).value:.5f}\")\n",
    "#print(\"BRIGHT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure out abs mag bins redshift maxes to use\n",
    "#df = bgs_sv3_pz_2_4_10p.all_data\n",
    "#df['MAG'] = log_solar_L_to_abs_mag_r(np.log10(df['L_GAL']))\n",
    "\n",
    "df = df.loc[df['Z_ASSIGNED_FLAG'] == AssignedRedshiftFlag.DESI_SPEC.value]\n",
    "\n",
    "mags = np.array([-14, -15, -16, -17, -18, -19, -20, -21, -22, -23])\n",
    "\n",
    "# Why are some observed redshifts higher than my theoretical max? \n",
    "# Peculiar velocity of course. Since it should be symmetric in the redshift direction, it's okay to use the theoretical max.\n",
    "print(\"DIM\")\n",
    "for m in mags:\n",
    "    # This column is k-corr to 0.1 and uses h=1.0\n",
    "    # Not sure why OBS MAX is so high for some of them (beyond vpec...)\n",
    "    print(f\"ABSMAG01_SDSS_R > {m}: THEORY MAX={get_max_observable_z(m, 19.5).value:.5f}  OBS MAX={df.loc[df['ABSMAG01_SDSS_R'] > m, 'Z'].max()}\")\n",
    "print(\"BRIGHT\")\n",
    "\n",
    "print(\"DIM\")\n",
    "for m in mags:\n",
    "    # This is k-corr to 0.1 as well using my method I think\n",
    "    print(f\"R^0.1 > {m}: THEORY MAX={get_max_observable_z(m, 19.5).value:.5f}  OBS MAX={df.loc[df['R'] > m, 'Z'].max()}\")\n",
    "print(\"BRIGHT\")\n",
    "\n",
    "# For noncumulative mag ranges now, using the theory values\n",
    "print(\"\\n Generic Cosmology\")\n",
    "mags = np.array([-14, -15, -16, -17, -18, -19, -20, -21, -22, -23])\n",
    "zmaxes = np.array([get_max_observable_z(m, 19.5).value for m in mags])\n",
    "zmins = np.array([get_max_observable_z(m+1, 19.5).value for m in mags])\n",
    "for m, zmin, zmax in zip(mags, zmins, zmaxes):\n",
    "    print(f\"{m} < Mag-5log(h) <= {m+1}:  zmin={zmin:.5f}  zmax={zmax:.5f}\")\n",
    "\n",
    "print(\"\\n MXXL Cosmology\")\n",
    "zmaxes = np.array([get_max_observable_z_mxxlcosmo(m, 19.5).value for m in mags])\n",
    "zmins = np.array([get_max_observable_z_mxxlcosmo(m+1, 19.5).value for m in mags])\n",
    "for m, zmin, zmax in zip(mags, zmins, zmaxes):\n",
    "    print(f\"{m} < Mag-{5*np.log10(0.7):.2f} <= {m+1}:  zmin={zmin:.5f}  zmax={zmax:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bgs_sv3_pz_2_4_10p.calculate_projected_clustering()\n",
    "# why is n=0? # BUG\n",
    "pp.wp_rp(bgs_sv3_pz_2_4_10p.wp_all[0], bgs_sv3_pz_2_4_10p.wp_all[1])#, bgs_sv3_pz_2_4_10p.wp_all[2], bgs_sv3_pz_2_4_10p.wp_all[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Co-moving Dist:  {get_cosmology().comoving_distance([0.01, 0.1, 0.2, 0.4]).value}\") # / Mpc/h\n",
    "print(f\"Co-moving Dist:  {get_cosmology().luminosity_distance([0.01, 0.1, 0.2, 0.4]).value / np.array([1.01, 1.1, 1.2, 1.4])}\") # / Mpc/h\n",
    "print(f\"Luminosity Dist: {get_cosmology().luminosity_distance([0.01, 0.1, 0.2, 0.4]).value}\") # / Mpc/h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SDSS Tutorial of Corrfunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mock catalog (SDSS-North) supplied with Corrfunc\n",
    "mock_catalog = pjoin(dirname(abspath(Corrfunc.__file__)), \"../mocks/tests/data/\", \"Mr19_mock_northonly.rdcz.ff\")\n",
    "RA, DEC, CZ = read_catalog(mock_catalog)\n",
    "\n",
    "# Randoms catalog (SDSS-North) supplied with Corrfunc\n",
    "randoms_catalog = pjoin(dirname(abspath(Corrfunc.__file__)), \"../mocks/tests/data/\", \"Mr19_randoms_northonly.rdcz.ff\")\n",
    "RAND_RA, RAND_DEC, RAND_CZ = read_catalog(randoms_catalog)\n",
    "\n",
    "rbins, wp = calculate_wp(RA, DEC, CZ, RAND_RA, RAND_DEC, RAND_CZ)\n",
    "\n",
    "pp.wp_rp(rbins, wp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j=plt.hist(RAND_CZ, bins=100, histtype='step', density=True)\n",
    "j=plt.hist(CZ, bins=100, histtype='step', density=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code to support others projects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bernie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MASS_CUT = 10**15.0\n",
    "df = bgs_y3_pzp_2_4.all_data[bgs_y3_pzp_2_4.all_data['M_HALO'] > MASS_CUT]\n",
    "df = df[df['DEC'] < 30.0] # Rough for ACT footprint matching\n",
    "\n",
    "print(\"\\nBGS Y3\")\n",
    "#print(df['Z_ASSIGNED_FLAG'].value_counts())    \n",
    "print(df.sort_values('L_GAL', ascending=False).loc[:, ['RA', 'DEC', 'L_GAL', 'Z', 'Z_PHOT', 'Z_OBS', 'Z_ASSIGNED_FLAG']].head(10))\n",
    "\n",
    "centrals = df[df['IGRP'] == df.index]\n",
    "clusters = centrals[centrals['N_SAT'] >= 14]\n",
    "print(f\"Clusters with 15+ members: {len(clusters)}\")\n",
    "\n",
    "x = df['IGRP'].value_counts() > 14\n",
    "print(f\"Clusters with 15+ members: {np.sum(x)}\")\n",
    "\n",
    "# Why do the above 2 not agree?\n",
    "\n",
    "df_spec = df[z_flag_is_spectro_z(df['Z_ASSIGNED_FLAG'])]\n",
    "y = df_spec['IGRP'].value_counts() > 14\n",
    "print(f\"Clusters with 15+ spectroscopic members: {np.sum(y)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viraj Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viraj Compare\n",
    "path = DATA_FOLDER + 'VIRAJ/jura_bgs_bright_catalog_for_ian.fits'\n",
    "table = Table.read(path)\n",
    "viraj_df = table.to_pandas()\n",
    "viraj_df.set_index('TARGETID', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_viraj_ian(viraj_df: pd.DataFrame, gc : GroupCatalog):\n",
    "    ian_df = gc.all_data.set_index('TARGETID').loc[:, ['Z', 'L_GAL', 'VMAX', 'P_SAT', 'M_HALO', 'N_SAT', 'L_TOT',\n",
    "       'IGRP', 'WEIGHT', 'app_mag', 'Z_ASSIGNED_FLAG', 'G_R', 'Z_PHOT', 'IS_SAT', 'QUIESCENT']]\n",
    "    print(ian_df['IGRP'].dtype)\n",
    "    #ian_df['QUIESCENT'] = ian_df['QUIESCENT'].astype(float)\n",
    "    ian_df['N_SAT'] = ian_df['N_SAT'].astype(int)\n",
    "    together = viraj_df.join(ian_df, how='inner', validate='one_to_one')\n",
    "    print(together['IGRP'].dtype)\n",
    "    print(f\"Viraj targets: {len(viraj_df):,}, Ian {gc.name} Catalog: {len(ian_df):,}, # of Viraj Targets found in Ian's: {(~np.isnan(together['IS_SAT'])).sum():,}\")\n",
    "    return together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "together1 = merge_viraj_ian(viraj_df, bgs_simple_4_1pass)\n",
    "together2 = merge_viraj_ian(viraj_df, simple4_BGS)\n",
    "together3 = merge_viraj_ian(viraj_df, bgs_y3_simple_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing=together3.loc[np.isnan(together3['IS_SAT'])]\n",
    "missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bgs_y3_simple_5.all_data['IGRP'].dtype)\n",
    "print(together3['IGRP'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_write = Table.from_pandas(together3, index=True)\n",
    "to_write.write(DATA_FOLDER + 'VIRAJ/jura_bgs_bright_catalog_for_ian_matched.fits', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = Table.read(DATA_FOLDER + 'VIRAJ/jura_bgs_bright_catalog_for_ian_matched.fits', format='fits')\n",
    "df = combined.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mock and SV3 Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UCHUU Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(all_u.all_data['M_HALO'], bins=pp.Mhalo_bins, alpha=0.4)\n",
    "plt.hist(all_u.all_data['uchuu_halo_mass']*10**10, bins=pp.Mhalo_bins, alpha=0.4)\n",
    "plt.loglog()\n",
    "\n",
    "# TODO do we expect the mass distribution of halos to be so different from the UCHUU SHAM catalog and our assigned halo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 1 / VMax corrections do odd thing to UCHUU Truth. Why?\n",
    "pp.hod_plots(all_u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What effect does Fiber Assignment have on the luminosity function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pp.group_finder_centrals_halo_masses_plots(mxxl_all, [mxxl_fiberonly, mxxl_simple_4])\n",
    "pp.group_finder_centrals_halo_masses_plots(bgs_sv3_pz_2_4_10p, [bgs_sv3_pz_1_7p, bgs_sv3_simple_5_7p])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare halos to truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.assigned_halo_analysis(mxxl_simple_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare assigned implied abs mags to truth from MXXL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_unobs_counts = mxxl_all.all_data[mxxl_all.all_data['Z_ASSIGNED_FLAG'] != 0].groupby('LGAL_BIN').RA.count()\n",
    "simple_4_ubobs_counts = mxxl_simple_4.all_data.groupby('LGAL_BIN').RA.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.L_func_plot([mxxl_all, mxxl_simple_4], [all_unobs_counts, simple_4_ubobs_counts])\n",
    "\n",
    "\n",
    "\n",
    "#pp.L_func_plot([all, simple_4], [all.all_data.L_gal[all.all_data['Z_ASSIGNED_FLAG'] == 0], simple_4.all_data.L_gal[simple_4.all_data['Z_ASSIGNED_FLAG'] == 0]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SV3 Edge Effects Quantification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc = bgs_sv3_pz_2_4_10p\n",
    "inner_galaxies = filter_SV3_to_avoid_edges(gc)\n",
    "inner_galaxies.color = 'k'\n",
    "inner_galaxies.name = 'SV3 Inner Galaxies'\n",
    "pp.plots(inner_galaxies, gc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = pp.make_map(gc.all_data.RA.to_numpy(), gc.all_data['DEC'].to_numpy())\n",
    "fig = pp.make_map(inner_galaxies.all_data.RA.to_numpy(), inner_galaxies.all_data['DEC'].to_numpy(), fig=fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.all_data.groupby('LGAL_BIN')['Z'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centering_versions = [\n",
    "    filter_SV3_to_avoid_edges(gc, 1.5),\n",
    "    filter_SV3_to_avoid_edges(gc, 1.4),\n",
    "    filter_SV3_to_avoid_edges(gc, 1.3),\n",
    "    filter_SV3_to_avoid_edges(gc, 1.2),\n",
    "    filter_SV3_to_avoid_edges(gc, 1.1),\n",
    "    filter_SV3_to_avoid_edges(gc, 1.0),\n",
    "    filter_SV3_to_avoid_edges(gc, 0.9),\n",
    "]\n",
    "pickle.dump(centering_versions, open('centering_versions.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "lowz = gc.all_data.loc[gc.all_data.z < 0.03]\n",
    "lowz_gc = copy.deepcopy(gc)\n",
    "lowz_gc.all_data = lowz\n",
    "lowz_gc.refresh_df_views()\n",
    "centering_versions_lowz = [\n",
    "    filter_SV3_to_avoid_edges(lowz_gc, 1.5),\n",
    "    filter_SV3_to_avoid_edges(lowz_gc, 1.4),\n",
    "    filter_SV3_to_avoid_edges(lowz_gc, 1.3),\n",
    "    filter_SV3_to_avoid_edges(lowz_gc, 1.2),\n",
    "    filter_SV3_to_avoid_edges(lowz_gc, 1.1),\n",
    "    filter_SV3_to_avoid_edges(lowz_gc, 1.0),\n",
    "    filter_SV3_to_avoid_edges(lowz_gc, 0.9),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.load(open('centering_versions.pkl', 'rb'))\n",
    "\n",
    "for i, d in enumerate(centering_versions):\n",
    "    d.color = [0, i/len(centering_versions), 0]\n",
    "    d.name = f'SV3 10p, {1.5-i*0.1:.1f} deg center cut'\n",
    "\n",
    "pp.LEGENDS_ON = False\n",
    "gc.color = 'blue'\n",
    "pp.fsat_by_zbins_sv3_centers(gc, *centering_versions, z_bins=np.array([0.0, 0.03, 1.0]))\n",
    "pp.single_plots(gc)\n",
    "pp.single_plots(centering_versions[2])\n",
    "pp.single_plots(centering_versions[4])\n",
    "pp.single_plots(centering_versions[6])\n",
    "pp.LEGENDS_ON = True\n",
    "\n",
    "#pp.fsat_by_z_bins(gc, z_bins=np.array([0.0, 0.03, 1.0]))\n",
    "#for d in centering_versions:\n",
    "#    pp.fsat_by_z_bins(d, z_bins=np.array([0.0, 0.03, 1.0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowz_gc.color = 'blue'\n",
    "pp.single_plots(lowz_gc)\n",
    "pp.single_plots(centering_versions_lowz[2])\n",
    "pp.single_plots(centering_versions_lowz[4])\n",
    "pp.single_plots(centering_versions_lowz[6])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig = pp.make_map(gc.all_data.RA.to_numpy(), gc.all_data['DEC'].to_numpy())\n",
    "\n",
    "#for i, gc in enumerate(centering_versions):\n",
    "#    fig = pp.make_map(gc.all_data.RA.to_numpy(), gc.all_data['DEC'].to_numpy(), fig=fig)\n",
    "\n",
    "#plot_positions(gc, *centering_versions, tiles_df=None, split=False, DEG_LONG=7, ra_min = 186.5, dec_min = 60)\n",
    "# BUG pass in all_data, not the GroupCatalog object\n",
    "plot_positions(gc.all_data, *centering_versions, tiles_df=None, split=False, DEG_LONG=6, ra_min = 147, dec_min = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lost Galaxy Luminosity Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a cut of SV3 whose completeness is similar to Y1 BGS.\n",
    "\n",
    "Question: is the luminosity function of lost galaxies (that were later observed) is different from the luminosity function observed galaxies?\n",
    "\n",
    "They seem similar; perhaps a mild slant. Overall it seems that trying to match the observed luminosity function with the lost ones is ok.\n",
    "\n",
    "Now for lost galaxies in 6pass that we have later got redshifts for.\n",
    "\n",
    "Question: What did our processing do to the luminosity function for lost galaxies?\n",
    "\n",
    "Our processing squeezes the luminosity function. We move galaxies from the wings towards the middle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Galaxy Neighborhood Examiner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_bsat_column(catalog: GroupCatalog):\n",
    "    bprob = 10\n",
    "    if 'beta0q' in catalog.GF_props:\n",
    "        beta0q = catalog.GF_props['beta0q']\n",
    "        beta0sf = catalog.GF_props['beta0sf']\n",
    "        betaLq = catalog.GF_props['betaLq']\n",
    "        betaLsf = catalog.GF_props['betaLsf']\n",
    "        bprob = np.zeros(len(catalog.all_data))\n",
    "        bprob = np.where(catalog.all_data['QUIESCENT'], beta0q + betaLq*(catalog.all_data['LOGLGAL']-9.5), beta0sf + betaLsf*(catalog.all_data['LOGLGAL']-9.5))\n",
    "        bprob = np.where(bprob < 0.001, 0.001, bprob)\n",
    "\n",
    "    catalog.all_data['BSAT'] = bprob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_halo_columns(bgs_sv3_pz_2_4_10p_c1)\n",
    "add_bsat_column(bgs_sv3_pz_2_4_10p_c1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = bgs_sv3_pz_2_4_10p_c1.all_data\n",
    "groups = df.loc[np.logical_and(df['N_SAT'] > 1, df['IGRP'] == df.index)]\n",
    "groups = groups.loc[groups['Z'] < 0.21]\n",
    "groups = groups.loc[groups['Z'] > 0.19]\n",
    "groups = groups.loc[groups['M_HALO'] < 10**13.1]\n",
    "groups = groups.loc[groups['M_HALO'] > 10**12.9]\n",
    "print(len(groups))\n",
    "bighalos = df.sort_values('M_HALO', ascending=False).head(60)\n",
    "\n",
    "brightest_gals = df.sort_values('L_GAL', ascending=False).head(60)\n",
    "\n",
    "lost_galaxies = df.loc[z_flag_is_not_spectro_z(df['Z_ASSIGNED_FLAG'])]\n",
    "obs_galaxies = df.loc[z_flag_is_spectro_z(df['Z_ASSIGNED_FLAG'])]\n",
    "#print(\"Lost galaxies: \", len(lost_galaxies), \"Observed Galaxies: \", len(obs_galaxies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLOTS_TO_MAKE = 10\n",
    "GALAXY_POOL = groups.reset_index()\n",
    "\n",
    "#START_INDEX = 777\n",
    "#for i in range(START_INDEX, START_INDEX + PLOTS_TO_MAKE):\n",
    "#    index = lost_galaxies.index[i]\n",
    "#    examine_around(index)\n",
    "print(\"Number of galaxies to choose from: \", len(GALAXY_POOL))\n",
    "indexes = np.random.randint(0, len(GALAXY_POOL)-1, size=PLOTS_TO_MAKE)\n",
    "#indexes = np.arange(0, PLOTS_TO_MAKE)\n",
    "for i in indexes:\n",
    "    target = GALAXY_POOL.iloc[i]\n",
    "    pp.examine_groups_near(target, data, nearby_angle=coord.Angle('7m'), zfilt=0.07)\n",
    "    #deg = coord.Angle('5m').to(u.degree).value\n",
    "    #pp.examine_area(target.RA - deg, target.RA + deg, target.DEC - deg, target.DEC + deg, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many halos were assigned below a certain cutoff?\n",
    "df = bgs_y1_pzp_2_4.all_data\n",
    "M_HALL_CUT = 10**11\n",
    "small_halo_df = df[df['M_HALO'] < M_HALL_CUT]\n",
    "\n",
    "print(len(small_halo_df), len(df))\n",
    "\n",
    "junk=plt.hist(small_halo_df.z, bins=100)\n",
    "plt.xlabel('Redshift')\n",
    "plt.ylabel('Count')\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
