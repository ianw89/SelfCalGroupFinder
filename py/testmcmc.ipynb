{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "#sys.path.append(os.path.join(os.getcwd(), '../desi/'))\n",
    "#import catalog_definitions as cat\n",
    "import numpy as np\n",
    "import emcee\n",
    "import time\n",
    "from multiprocessing import Pool\n",
    "import corner\n",
    "\n",
    "if './SelfCalGroupFinder/py/' not in sys.path:\n",
    "    sys.path.append('./SelfCalGroupFinder/py/')\n",
    "from dataloc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_prob(x, mu, cov):\n",
    "    diff = x - mu\n",
    "    return -0.5 * np.dot(diff, np.linalg.solve(cov, diff))\n",
    "\n",
    "ndim = 3\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    " # Fixed values, not parameters.\n",
    "means = np.random.rand(ndim) # Means are between 0 and 1\n",
    "# There are some covariancse between the gaussians\n",
    "cov = 0.5 - np.random.rand(ndim**2).reshape((ndim, ndim)) \n",
    "cov = np.triu(cov)\n",
    "cov += cov.T - np.diag(cov.diagonal())\n",
    "cov = np.dot(cov, cov)\n",
    "\n",
    "nwalkers = 16\n",
    "niter = 5000\n",
    "nburnin = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [0.5, 0.5, 0.5]\n",
    "mu = means \n",
    "np.linalg.solve(cov, x-mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(means)\n",
    "print(cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random priors from a uniform distribution between 0 and 1 given to each walker\n",
    "p0 = np.random.uniform(low=np.zeros(ndim), high=np.ones(ndim), size=(nwalkers, ndim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "backend = emcee.backends.HDFBackend(OUTPUT_FOLDER + \"test_backend\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- run emcee\n",
    "sampler = emcee.EnsembleSampler(nwalkers, ndim, log_prob, args=[means, cov])\n",
    "#state = sampler.run_mcmc(p0, nburnin)\n",
    "#sampler.reset()\n",
    "\n",
    "start = time.time()\n",
    "# If you run this multiple times, it will keep doing 10000 more steps (per walker)\n",
    "sampler.run_mcmc(p0, niter, progress=True)\n",
    "end = time.time()\n",
    "serial_time = end - start\n",
    "\n",
    "print(\"Serial took {0:.1f} seconds\".format(serial_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = sampler.get_chain()\n",
    "ndim = samples.shape[2]\n",
    "print(f'Number of steps: {samples.shape[0] * samples.shape[1]} (total); {samples.shape[0]} (per walker), ')\n",
    "print(f'Number of walkers: {samples.shape[1]}')\n",
    "print(f'Number of parameters: {ndim}')\n",
    "\n",
    "try:\n",
    "    tau = sampler.get_autocorr_time()\n",
    "    print(tau)\n",
    "except:\n",
    "    print(\"Not burnt in yet\")\n",
    "\n",
    "burn = int(np.max(tau) * 2)\n",
    "flatchain = sampler.get_chain(discard=burn, flat=True)\n",
    "fig = corner.corner(flatchain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.argmax(sampler.get_log_prob(flat=True))\n",
    "values = sampler.get_chain(flat=True)[idx]\n",
    "print(values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Pool() as pool: # default Pool() gest CPU_count processes. Good!\n",
    "    sampler2 = emcee.EnsembleSampler(nwalkers, ndim, log_prob, pool=pool, args=[means, cov])\n",
    "    state2 = sampler2.run_mcmc(p0, nburnin)\n",
    "    sampler2.reset()\n",
    "\n",
    "    start = time.time()\n",
    "    sampler2.run_mcmc(p0, niter, progress=True)\n",
    "    end = time.time()\n",
    "    multi_time = end - start\n",
    "    \n",
    "    print(\"Multiprocessing took {0:.1f} seconds\".format(multi_time))\n",
    "    print(\"{0:.1f} times faster than serial\".format(serial_time / multi_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "samples = sampler.get_chain(flat=True) # length is walkers * steps run total\n",
    "PARAMETER_INDEX = 0\n",
    "\n",
    "# View distribution of values (not including burn in) of a single parameter\n",
    "plt.hist(samples[:, PARAMETER_INDEX], 100, color=\"k\", histtype=\"step\")\n",
    "plt.xlabel(r\"$\\theta_1$\")\n",
    "plt.ylabel(r\"$p(\\theta_1)$\")\n",
    "plt.title(f'Posterior Distribution for parameter {PARAMETER_INDEX}')\n",
    "plt.gca().set_yticks([])\n",
    "plt.show()\n",
    "\n",
    "# View chain for a single parameter\n",
    "plt.plot(samples[:, PARAMETER_INDEX])\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel(f'Parameter Value')\n",
    "plt.title(f'Parameter Chain for parameter {PARAMETER_INDEX}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Mean acceptance fraction: {0:.3f}\".format(\n",
    "        np.mean(sampler.acceptance_fraction)\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"Mean autocorrelation time: {0:.3f} steps\".format(\n",
    "        np.mean(sampler.get_autocorr_time())\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corner Plots\n",
    "import corner\n",
    "\n",
    "fig = corner.corner(\n",
    "    flat_samples, labels=labels, truths=[m_true, b_true, np.log(f_true)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nobs = 20\n",
    "x_true = np.random.uniform(0,10, size=Nobs)\n",
    "y_true = np.random.uniform(-1,1, size=Nobs)\n",
    "alpha_true = 0.5\n",
    "beta_x_true = 1.0\n",
    "beta_y_true = 10.0\n",
    "eps_true = 0.5\n",
    "z_true = alpha_true + beta_x_true*x_true + beta_y_true*y_true\n",
    "z_obs = z_true + np.random.normal(0, eps_true, size=Nobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(x_true, z_obs, c=y_true, marker='o')\n",
    "plt.colorbar()\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Z')\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(y_true, z_obs, c=x_true, marker='o')\n",
    "plt.colorbar()\n",
    "plt.xlabel('Y')\n",
    "plt.ylabel('Z')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a linear regression example. Need to understand what these three do\n",
    "\n",
    "def lnprior(p):\n",
    "    # The parameters are stored as a vector of values, so unpack them\n",
    "    alpha,betax,betay,eps = p\n",
    "    # We're using only uniform priors, and only eps has a lower bound\n",
    "    if eps <= 0:\n",
    "        return -np.inf\n",
    "    return 0\n",
    "\n",
    "def lnlike(p, x, y, z):\n",
    "    alpha,betax,betay,eps = p\n",
    "    model = alpha + betax*x + betay*y\n",
    "    # the likelihood is sum of the lot of normal distributions\n",
    "    denom = np.power(eps,2)\n",
    "    lp = -0.5*sum(np.power((z - model),2)/denom + np.log(denom) + np.log(2*np.pi))\n",
    "    return lp\n",
    "\n",
    "def lnprob(p, x, y, z):\n",
    "    lp = lnprior(p)\n",
    "    if not np.isfinite(lp):\n",
    "        return -np.inf\n",
    "    return lp + lnlike(p, x, y, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood(p, x, y, z):\n",
    "    alpha,betax,betay,eps = p\n",
    "    model = alpha + betax*x + betay*y\n",
    "    # the likelihood is sum of the lot of normal distributions\n",
    "    denom = np.power(eps,2)\n",
    "    lp = sum(np.power((z - model),2)/denom + np.log(denom) + np.log(2*np.pi))\n",
    "    return lp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lnlike([alpha_true, beta_x_true, beta_y_true, eps_true], x_true, y_true, z_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nwalker,Ndim = 50,4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.optimize as opt\n",
    "nll = lambda *args: -lnlike(*args)\n",
    "result = opt.minimize(nll, [alpha_true, beta_x_true, beta_y_true, eps_true], args=(x_true, y_true, z_obs))\n",
    "print(result['x'])\n",
    "p0 = [result['x']+1.e-4*np.random.randn(Ndim) for i in range(Nwalker)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "p0 = np.random.uniform(low=np.zeros(Ndim), high=np.ones(Ndim), size=(Nwalker, Ndim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = emcee.EnsembleSampler(Nwalker,Ndim,lnprob,args=(x_true,y_true,z_obs))\n",
    "pos,prob,state = sampler.run_mcmc(p0, 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=plt.plot(sampler.chain[:,:,0].T, '-', color='k', alpha=0.3)\n",
    "plt.axhline(alpha_true, color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.argmax(sampler.get_log_prob(flat=True))\n",
    "m_alpha,m_betax,m_betay,m_eps = sampler.get_chain(flat=True)[idx]\n",
    "\n",
    "#m_alpha,m_betax,m_betay,m_eps = np.median(sampler.flatchain, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(x_true, z_obs-m_alpha-m_betay*y_true, 'o')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Z - alpha - beta_y y')\n",
    "# Now plot the model\n",
    "xx = np.array([x_true.min(), x_true.max()])\n",
    "plt.plot(xx, xx*m_betax)\n",
    "plt.plot(xx, xx*m_betax + m_eps, '--', color='k')\n",
    "plt.plot(xx, xx*m_betax - m_eps, '--', color='k')\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(y_true, z_obs-m_alpha-m_betax*x_true, 'o')\n",
    "plt.xlabel('Y')\n",
    "plt.ylabel('Z - alpha - beta_x x')\n",
    "yy = np.array([y_true.min(), y_true.max()])\n",
    "plt.plot(yy, yy*m_betay)\n",
    "plt.plot(yy, yy*m_betay + m_eps, '--', color='k')\n",
    "plt.plot(yy, yy*m_betay - m_eps, '--', color='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
