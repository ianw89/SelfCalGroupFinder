{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57684ff2",
   "metadata": {},
   "source": [
    "# Spectroscopic Properties Lookup Tables\n",
    "\n",
    "For lost galaxies we need to fill in spectroscopic properties. Redshift is handled first and specially. Here we create two other lookup tables. The lookup tables are a two-step process after z has been assigned to lost galaxies. First you use kcorrlookup, then dn4000lookup. Use train_kcorrlookup.py and train_dn4000.py to search parameters for the best way to do this. This notebook lets you look at those results, save the model via pickle, and validate the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f556713",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import astropy.io.fits as fits\n",
    "from astropy.table import Table\n",
    "import sys\n",
    "#from pykdtree.kdtree import KDTree as FastKDTree\n",
    "from scipy.spatial.kdtree import KDTree \n",
    "import pickle\n",
    "import emcee\n",
    "import corner\n",
    "\n",
    "if './SelfCalGroupFinder/py/' not in sys.path:\n",
    "    sys.path.append('./SelfCalGroupFinder/py/')\n",
    "from pyutils import *\n",
    "from dataloc import *\n",
    "from bgs_helpers import *\n",
    "from groupcatalog import BGS_Z_MAX, BGS_Z_MIN\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9557244e",
   "metadata": {},
   "source": [
    "## Load BGS Data\n",
    "\n",
    "Load the merged BGS data file to use for creating the lookup table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ec1e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the merged table\n",
    "table = Table.read(IAN_BGS_Y1_MERGED_FILE, format='fits')\n",
    "df = table_to_df(table, 20.175, BGS_Z_MIN, BGS_Z_MAX, True, 1)\n",
    "print(f\"Loaded {len(df):,} galaxies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9de070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all but the columns we need\n",
    "df = df[['TARGETID', 'Z', 'ABS_MAG_R', 'ABS_MAG_G', 'ABSMAG01_SDSS_R', 'ABSMAG01_SDSS_G', 'DN4000_MODEL', 'G_R_BEST', 'LOG_L_GAL', 'LOGMSTAR']]\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eeb0dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "magr_k_gama = k_correct_gama(df['ABS_MAG_R'], df['Z'], df['ABS_MAG_G'] - df['ABS_MAG_R'], band='r')\n",
    "magg_k_gama = k_correct_gama(df['ABS_MAG_G'], df['Z'], df['ABS_MAG_G'] - df['ABS_MAG_R'], band='g')\n",
    "badmatch = (np.abs(magr_k_gama - df['ABSMAG01_SDSS_R']) > 1.0) | (np.abs(magg_k_gama - df['ABSMAG01_SDSS_G']) > 1.0)\n",
    "goodidx = ~np.isnan(df['ABS_MAG_R']) & ~np.isnan(df['ABS_MAG_G']) & ~np.isnan(df['Z']) & ~np.isnan(df['ABSMAG01_SDSS_R']) & ~np.isnan(df['ABSMAG01_SDSS_G']) & ~badmatch \n",
    "print(f\"Number of galaxies with good data: {np.sum(goodidx):,}\")\n",
    "\n",
    "# Prepare training and test data\n",
    "z_arr = df.loc[goodidx, 'Z'].to_numpy()\n",
    "magr_arr = df.loc[goodidx, 'ABS_MAG_R'].to_numpy() # These have no k-corr\n",
    "magg_arr = df.loc[goodidx, 'ABS_MAG_G'].to_numpy() # These have no k-corr\n",
    "gmr_arr = magg_arr - magr_arr\n",
    "kcorr_r_arr = magr_arr - df.loc[goodidx, 'ABSMAG01_SDSS_R'].to_numpy() # These have the fastspecfit k-corr in them\n",
    "kcorr_g_arr = magg_arr - df.loc[goodidx, 'ABSMAG01_SDSS_G'].to_numpy() # These have the fastspecfit k-corr in them  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c91103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot of redshift and g-r\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(magg_arr, z_arr, s=5, alpha=0.05, label='All Y1 Good Data')\n",
    "plt.xlabel('g - r')\n",
    "plt.ylabel('Redshift')\n",
    "plt.title('Redshift vs. g-r Color')\n",
    "plt.legend()\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc796c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the distribution of k-corrections in r and g bands\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(kcorr_r_arr, bins=1250, alpha=0.7, color='r', label='r-band')\n",
    "plt.hist(kcorr_g_arr, bins=1250, alpha=0.7, color='g', label='g-band')\n",
    "plt.xlabel('K-correction (mag)')\n",
    "plt.ylabel('Number of galaxies')\n",
    "plt.title('Distribution of K-corrections (Training Set)')\n",
    "plt.legend()\n",
    "plt.xlim(-0.4, 1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e5cf43",
   "metadata": {},
   "source": [
    "## View MCMC results; Save Best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8276dc46",
   "metadata": {},
   "source": [
    "### kcorrlookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f07b847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sampler\n",
    "backend = emcee.backends.HDFBackend(OUTPUT_FOLDER + \"kcorr_lookup_optimization.h5\")\n",
    "chains = backend.get_log_prob(flat=True)\n",
    "argmax = np.argmax(chains)\n",
    "samples = backend.get_chain(flat=True)\n",
    "chisqr = - backend.get_log_prob(flat=True)\n",
    "\n",
    "print(f\"Total Samples: {len(samples)}\")\n",
    "print(f\"Best-fit parameters: {samples[argmax]}\")\n",
    "print(f\"Best-fit chi-squared: {chisqr[argmax]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2503f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot of chisqr values, chi sqr is color and 2d map is the two parameters \n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(samples[:, 0], samples[:, 1], c=chisqr, cmap='viridis', s=10)\n",
    "plt.colorbar(label='Chi-squared')\n",
    "plt.xlabel('METRIC_Z')\n",
    "plt.ylabel('METRIC_GMR')\n",
    "plt.title('MCMC Samples Colored by Chi-squared')\n",
    "plt.show()\n",
    "\n",
    "# Plot corner plot\n",
    "fig = corner.corner(samples, labels=[\"METRIC_Z\", \"METRIC_GMR\"], show_titles=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b223c0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now for the best fit metric, build the tree and lookup to save off (using the full data now)\n",
    "\n",
    "# MCMC says it doesn't matter much\n",
    "optimal_metric_z = 50.0\n",
    "optimal_metric_gmr = 10.0\n",
    "\n",
    "# Filter to galaxies with all required data\n",
    "badmatch = (np.abs(magr_k_gama - df['ABSMAG01_SDSS_R']) > 1.0) | (np.abs(magg_k_gama - df['ABSMAG01_SDSS_G']) > 1.0)\n",
    "goodidx = ~np.isnan(df['ABS_MAG_R']) & ~np.isnan(df['ABS_MAG_G']) & ~np.isnan(df['Z']) & ~np.isnan(df['ABSMAG01_SDSS_R']) & ~np.isnan(df['ABSMAG01_SDSS_G']) & ~badmatch\n",
    "\n",
    "print(f\"Building final lookup table with metric:\")\n",
    "print(f\"  METRIC_Z: {optimal_metric_z}\")\n",
    "print(f\"  METRIC_GMR: {optimal_metric_gmr}\")\n",
    "\n",
    "# Prepare full dataset (all good galaxies)\n",
    "z_full = df.loc[goodidx, 'Z'].to_numpy()\n",
    "magr_full = df.loc[goodidx, 'ABS_MAG_R'].to_numpy()\n",
    "magg_full = df.loc[goodidx, 'ABS_MAG_G'].to_numpy()\n",
    "gmr_full = magg_full - magr_full\n",
    "\n",
    "# Calculate k-corrections for the full dataset\n",
    "kcorr_r_full = magr_full - df.loc[goodidx, 'ABSMAG01_SDSS_R'].to_numpy()\n",
    "kcorr_g_full = magg_full - df.loc[goodidx, 'ABSMAG01_SDSS_G'].to_numpy()\n",
    "\n",
    "# Scale the features with optimal metrics\n",
    "z_scaled = z_full * optimal_metric_z\n",
    "gmr_scaled = gmr_full * optimal_metric_gmr\n",
    "magr_scaled = magr_full  # metric_absmag_r = 1.0\n",
    "\n",
    "# Build the KDTree\n",
    "lookup_points = np.vstack((z_scaled, gmr_scaled, magr_scaled)).T\n",
    "kdtree = KDTree(lookup_points)\n",
    "\n",
    "# Store the k-corrections as lookup tables\n",
    "kcorr_r_lookup = kcorr_r_full\n",
    "kcorr_g_lookup = kcorr_g_full\n",
    "\n",
    "print(f\"Built KDTree with {len(lookup_points):,} galaxies\")\n",
    "print(f\"K-correction lookup table shape: {kcorr_r_lookup.shape}\")\n",
    "\n",
    "# Save the lookup table and optimal metrics\n",
    "lookup_data = (lookup_points, kcorr_r_lookup, kcorr_g_lookup, optimal_metric_z, optimal_metric_gmr, 1.0)\n",
    "\n",
    "with open(BGS_Y3_KCORR_LOOKUP_FILE, 'wb') as f:\n",
    "    pickle.dump(lookup_data, f)\n",
    "\n",
    "print(f\"\\nSaved lookup table to {BGS_Y3_KCORR_LOOKUP_FILE}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52504a7e",
   "metadata": {},
   "source": [
    "### dn4000lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c2d4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sampler\n",
    "backend2 = emcee.backends.HDFBackend(OUTPUT_FOLDER + \"dn4000_lookup_optimization.h5\")\n",
    "chains = backend2.get_log_prob(flat=True)\n",
    "argmax = np.argmax(chains)\n",
    "samples = backend2.get_chain(flat=True)\n",
    "chisqr = - backend2.get_log_prob(flat=True)\n",
    "\n",
    "print(f\"Total Samples: {len(samples)}\")\n",
    "print(f\"Best-fit parameters: {samples[argmax]}\")\n",
    "print(f\"Best-fit chi-squared: {chisqr[argmax]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbd41b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show chi squared distrubtion as function of parameters\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot chi-squared as a function of each parameter\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.scatter(samples[:, 0], chisqr, alpha=0.5)\n",
    "plt.xlabel(\"METRIC_GMR\")\n",
    "plt.ylabel(\"Chi-squared\")\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.scatter(samples[:, 1], chisqr, alpha=0.5)\n",
    "plt.xlabel(\"K\")\n",
    "plt.ylabel(\"Chi-squared\")\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.scatter(samples[:, 2], chisqr, alpha=0.5)\n",
    "plt.xlabel(\"INNER_METRIC_DN4000\")\n",
    "plt.ylabel(\"Chi-squared\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63267f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot corner plot\n",
    "fig = corner.corner(samples, labels=[\"METRIC_GMR\", \"K\", \"INNER_METRIC_DN4000\"], show_titles=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f999a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_metric_gmr = 5.0\n",
    "optimal_innermetric_dn4000 = 5.0\n",
    "    \n",
    "# Filter to galaxies with all required data\n",
    "goodidx = (~np.isnan(df['ABSMAG01_SDSS_R']) & \n",
    "            ~np.isnan(df['ABSMAG01_SDSS_G']) & \n",
    "            ~np.isnan(df['DN4000_MODEL']) &\n",
    "            ~np.isnan(df['LOGMSTAR']))\n",
    "# Build final lookup table with optimal metrics\n",
    "print(f\"\\nBuilding dn4000/mstellar final lookup table...\")\n",
    "\n",
    "# Use all good galaxies for final table\n",
    "magr_scaled = df.loc[goodidx, 'ABSMAG01_SDSS_R'].to_numpy()  # metric_absmag_r = 1.0\n",
    "gmr_scaled = (df.loc[goodidx, 'ABSMAG01_SDSS_G'] - df.loc[goodidx, 'ABSMAG01_SDSS_R']).to_numpy() * optimal_metric_gmr\n",
    "dn4000 = df.loc[goodidx, 'DN4000_MODEL'].to_numpy()\n",
    "logmstar = df.loc[goodidx, 'LOGMSTAR'].to_numpy()\n",
    "\n",
    "lookup_points = np.vstack((magr_scaled, gmr_scaled)).T\n",
    "kdtree = KDTree(lookup_points)\n",
    "\n",
    "print(f\"Built KDTree with {len(lookup_points):,} galaxies\")\n",
    "\n",
    "# Save lookup table\n",
    "lookup_data = (kdtree, dn4000, logmstar, optimal_metric_gmr, 1.0, optimal_innermetric_dn4000, 1.0)\n",
    "\n",
    "with open(BGS_Y3_DN4000_LOOKUP_FILE, 'wb') as f:\n",
    "    pickle.dump(lookup_data, f)\n",
    "\n",
    "print(f\"\\nSaved lookup table to {BGS_Y3_DN4000_LOOKUP_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9aad9e1",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40022f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_file = OUTPUT_FOLDER + \"y3_validation_set.pkl\"\n",
    "\n",
    "if os.path.exists(validation_file):\n",
    "    df_valid = pd.read_pickle(validation_file)\n",
    "    print(f\"Loaded Y3 validation set from {validation_file}\")\n",
    "\n",
    "else:\n",
    "    print(f\"Creating Y3 validation set {validation_file}\")\n",
    "    table_y3 = Table.read(IAN_BGS_Y3_MERGED_FILE_LOA, format='fits')\n",
    "    df_y3 = table_to_df(table_y3, 20.175, BGS_Z_MIN, BGS_Z_MAX, True, 1)\n",
    "\n",
    "    # Take a random subset of Y3 for validation (up to 100k)\n",
    "    n_validate = min(100000, len(df_y3))\n",
    "    validation_idx = np.random.choice(len(df_y3), size=n_validate, replace=False)\n",
    "    df_y3_subset = df_y3.iloc[validation_idx].copy()\n",
    "    print(f\"Loaded {n_validate:,} Y3 galaxies for validation\")\n",
    "\n",
    "    # Remove data points that were in Y1 (use TARGETID)\n",
    "    y1_targetids = set(df['TARGETID'].values)\n",
    "    not_in_y1 = ~df_y3_subset['TARGETID'].isin(y1_targetids)\n",
    "    df_y3_subset = df_y3_subset[not_in_y1].copy()\n",
    "    print(f\"After removing Y1 galaxies: {len(df_y3_subset):,} Y3 galaxies remaining\")\n",
    "\n",
    "    # Filter to galaxies with complete data for all validations\n",
    "    has_kcorr = ~np.isnan(df_y3_subset['ABSMAG01_SDSS_R']) & ~np.isnan(df_y3_subset['ABSMAG01_SDSS_G'])\n",
    "    has_mags = ~np.isnan(df_y3_subset['ABS_MAG_R']) & ~np.isnan(df_y3_subset['ABS_MAG_G'])\n",
    "    has_z = ~np.isnan(df_y3_subset['Z'])\n",
    "    has_dn4000 = ~np.isnan(df_y3_subset['DN4000_MODEL'])\n",
    "    has_logmstar = ~np.isnan(df_y3_subset['LOGMSTAR'])\n",
    "    has_quiescent = ~np.isnan(df_y3_subset['QUIESCENT'])\n",
    "\n",
    "    valid_all = has_kcorr & has_mags & has_z & has_dn4000 & has_logmstar & has_quiescent\n",
    "    df_valid = df_y3_subset[valid_all].copy()\n",
    "    print(f\"Validation sample: {len(df_valid):,} Y3 galaxies with complete data\\n\")\n",
    "\n",
    "    # Pickle save off this validation set\n",
    "    df_valid.to_pickle(OUTPUT_FOLDER + \"y3_validation_set.pkl\")\n",
    "    print(f\"Saved Y3 validation set to {validation_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f51d1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load lookup tables\n",
    "print(\"Loading lookup tables...\")\n",
    "kcorr_lookup = kcorrlookup()\n",
    "dn4000_lookup = dn4000lookup()\n",
    "\n",
    "# Extract test data\n",
    "z_val = df_valid['Z'].to_numpy()\n",
    "magr_val = df_valid['ABS_MAG_R'].to_numpy()\n",
    "magg_val = df_valid['ABS_MAG_G'].to_numpy()\n",
    "gmr_val = magg_val - magr_val\n",
    "\n",
    "# Get predictions from k-correction lookup\n",
    "pred_kcorr_r, pred_kcorr_g = kcorr_lookup.query(gmr_val, z_val, magr_val)\n",
    "\n",
    "# Apply k-corrections\n",
    "pred_magr_kcorr = magr_val - pred_kcorr_r\n",
    "pred_magg_kcorr = magg_val - pred_kcorr_g\n",
    "pred_gmr_kcorr = pred_magg_kcorr - pred_magr_kcorr\n",
    "\n",
    "# Get predictions from dn4000 lookup\n",
    "pred_dn4000, pred_logmstar = dn4000_lookup.query(pred_magr_kcorr, pred_gmr_kcorr)\n",
    "\n",
    "# Get quiescent classification using predicted values\n",
    "pred_log_l = abs_mag_r_to_log_solar_L(pred_magr_kcorr)\n",
    "pred_quiescent = is_quiescent_BGS_dn4000(pred_log_l, pred_dn4000, pred_gmr_kcorr)\n",
    "pred_quiescent_gmrk = is_quiescent_BGS_gmr(pred_log_l, pred_gmr_kcorr)\n",
    "pred_quiescent_gmr = is_quiescent_lost_gal_guess(gmr_val)\n",
    "\n",
    "# True values\n",
    "true_kcorr_r = magr_val - df_valid['ABSMAG01_SDSS_R'].to_numpy()\n",
    "true_kcorr_g = magg_val - df_valid['ABSMAG01_SDSS_G'].to_numpy()\n",
    "true_dn4000 = df_valid['DN4000_MODEL'].to_numpy()\n",
    "true_logmstar = df_valid['LOGMSTAR'].to_numpy()\n",
    "true_quiescent = df_valid['QUIESCENT'].to_numpy().astype(bool)\n",
    "\n",
    "# Calculate errors\n",
    "err_kcorr_r = pred_kcorr_r - true_kcorr_r\n",
    "err_kcorr_g = pred_kcorr_g - true_kcorr_g\n",
    "err_dn4000 = pred_dn4000 - true_dn4000\n",
    "err_logmstar = pred_logmstar - true_logmstar\n",
    "\n",
    "# Calculate percentile-based sigma levels\n",
    "def get_sigma_levels(errors):\n",
    "    \"\"\"Get 1, 2, 3 sigma levels using percentiles (68.27%, 95.45%, 99.73%)\"\"\"\n",
    "    sigma_1 = np.percentile(np.abs(errors), 68.27)\n",
    "    sigma_2 = np.percentile(np.abs(errors), 95.45)\n",
    "    sigma_3 = np.percentile(np.abs(errors), 99.73)\n",
    "    return sigma_1, sigma_2, sigma_3\n",
    "\n",
    "# Calculate quiescent classification accuracy\n",
    "quiescent_correct = np.sum(pred_quiescent == true_quiescent)\n",
    "quiescent_accuracy = 100.0 * quiescent_correct / len(pred_quiescent)\n",
    "\n",
    "quiescent_correct_gmrk = np.sum(pred_quiescent_gmrk == true_quiescent)\n",
    "quiescent_accuracy_gmrk = 100.0 * quiescent_correct_gmrk / len(pred_quiescent_gmrk)\n",
    "\n",
    "quiescent_correct_gmr = np.sum(pred_quiescent_gmr == true_quiescent)\n",
    "quiescent_accuracy_gmr = 100.0 * quiescent_correct_gmr / len(pred_quiescent_gmr)\n",
    "\n",
    "# Print summary results\n",
    "print(\"=\"*70)\n",
    "print(\"LOOKUP TABLE VALIDATION SUMMARY (Y3 DATA)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nSample size: {len(df_valid):,} galaxies\")\n",
    "print(f\"Redshift range: {z_val.min():.3f} - {z_val.max():.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"ERROR ON LOOKED UP QUANTITIES\")\n",
    "print(\"-\"*70)\n",
    "s1, s2, s3 = get_sigma_levels(err_kcorr_r)\n",
    "print(f\"r-band k-correction:  1σ = {s1:.4f},  2σ = {s2:.4f},  3σ = {s3:.4f}\")\n",
    "s1, s2, s3 = get_sigma_levels(err_kcorr_g)\n",
    "print(f\"g-band k-correction:  1σ = {s1:.4f},  2σ = {s2:.4f},  3σ = {s3:.4f}\")\n",
    "s1, s2, s3 = get_sigma_levels(err_dn4000)\n",
    "print(f\"Dn4000:               1σ = {s1:.4f},  2σ = {s2:.4f},  3σ = {s3:.4f}\")\n",
    "s1, s2, s3 = get_sigma_levels(err_logmstar)\n",
    "print(f\"log(M*/M_sun):        1σ = {s1:.4f},  2σ = {s2:.4f},  3σ = {s3:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"QUIESCENT CLASSIFICATION\")\n",
    "print(\"-\"*70)\n",
    "print(f\"Accuracy:             {quiescent_accuracy:.2f}%\")\n",
    "print(f\"Correct:              {quiescent_correct:,} / {len(pred_quiescent):,}\")\n",
    "print(f\"Accuracy (gmrk):      {quiescent_accuracy_gmrk:.2f}%\")\n",
    "print(f\"Correct (gmrk):       {quiescent_correct_gmrk:,} / {len(pred_quiescent_gmrk):,}\")\n",
    "print(f\"Accuracy (gmr):       {quiescent_accuracy_gmr:.2f}%\")\n",
    "print(f\"Correct (gmr):        {quiescent_correct_gmr:,} / {len(pred_quiescent_gmr):,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# Optional: Create a compact visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Plot 1: K-correction errors\n",
    "ax = axes[0, 0]\n",
    "ax.hist(err_kcorr_r, bins=250, alpha=0.6, label='r-band', color='r', density=True)\n",
    "ax.hist(err_kcorr_g, bins=250, alpha=0.6, label='g-band', color='g', density=True)\n",
    "ax.axvline(0, color='k', linestyle='--', lw=1)\n",
    "s1_r, s2_r, s3_r = get_sigma_levels(err_kcorr_r)\n",
    "ax.axvline(s1_r, color='r', linestyle=':', alpha=0.5, label=f'1σ (r)={s1_r:.3f}')\n",
    "ax.axvline(-s1_r, color='r', linestyle=':', alpha=0.5)\n",
    "ax.set_xlabel('K-correction Error (mag)')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title('K-correction Errors')\n",
    "ax.set_xlim(-0.5, 0.5)\n",
    "ax.legend(fontsize=8)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Dn4000 errors\n",
    "ax = axes[0, 1]\n",
    "ax.hist(err_dn4000, bins=100, alpha=0.7, density=True)\n",
    "ax.axvline(0, color='k', linestyle='--', lw=1)\n",
    "s1, s2, s3 = get_sigma_levels(err_dn4000)\n",
    "ax.axvline(s1, color='r', linestyle=':', alpha=0.5, label=f'1σ={s1:.3f}')\n",
    "ax.axvline(-s1, color='r', linestyle=':', alpha=0.5)\n",
    "ax.set_xlabel('Dn4000 Error')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title('Dn4000 Errors')\n",
    "ax.set_xlim(-0.5, 0.5)\n",
    "ax.legend(fontsize=8)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Stellar mass errors\n",
    "ax = axes[1, 0]\n",
    "ax.hist(err_logmstar, bins=100, alpha=0.7, density=True, color='green')\n",
    "ax.axvline(0, color='k', linestyle='--', lw=1)\n",
    "s1, s2, s3 = get_sigma_levels(err_logmstar)\n",
    "ax.axvline(s1, color='r', linestyle=':', alpha=0.5, label=f'1σ={s1:.3f}')\n",
    "ax.axvline(-s1, color='r', linestyle=':', alpha=0.5)\n",
    "ax.set_xlabel('log(M*) Error (dex)')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title('Stellar Mass Errors')\n",
    "ax.set_xlim(-1.0, 1.0)\n",
    "ax.legend(fontsize=8)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Quiescent classification confusion\n",
    "ax = axes[1, 1]\n",
    "confusion = np.zeros((2, 2))\n",
    "confusion[0, 0] = np.sum((~pred_quiescent) & (~true_quiescent))  # TN\n",
    "confusion[0, 1] = np.sum((pred_quiescent) & (~true_quiescent))   # FP\n",
    "confusion[1, 0] = np.sum((~pred_quiescent) & (true_quiescent))   # FN\n",
    "confusion[1, 1] = np.sum((pred_quiescent) & (true_quiescent))    # TP\n",
    "im = ax.imshow(confusion, cmap='Blues', aspect='auto')\n",
    "ax.set_xticks([0, 1])\n",
    "ax.set_yticks([0, 1])\n",
    "ax.set_xticklabels(['Star-forming', 'Quiescent'])\n",
    "ax.set_yticklabels(['Star-forming', 'Quiescent'])\n",
    "ax.set_xlabel('Predicted')\n",
    "ax.set_ylabel('True')\n",
    "ax.set_title(f'Quiescent Classification\\nAccuracy: {quiescent_accuracy:.2f}%')\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        text = ax.text(j, i, f'{int(confusion[i, j]):,}',\n",
    "                      ha=\"center\", va=\"center\", color=\"black\", fontsize=10)\n",
    "plt.colorbar(im, ax=ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_FOLDER + 'lookup_validation_summary.png', dpi=150, bbox_inches='tight')\n",
    "print(f\"\\nSaved validation plot to {OUTPUT_FOLDER}lookup_validation_summary.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ff2bf9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
