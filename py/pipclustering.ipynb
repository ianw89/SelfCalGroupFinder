{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import astropy.constants as const\n",
    "\n",
    "if './SelfCalGroupFinder/py/' not in sys.path:\n",
    "    sys.path.append('./SelfCalGroupFinder/py/')\n",
    "from pyutils import *\n",
    "from dataloc import *\n",
    "from bgs_helpers import *\n",
    "from plotting import *\n",
    "import wp\n",
    "import catalog_definitions as cat\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is for calculating PIP clustering using the clustering catalogs and randoms.\n",
    "\n",
    "For group catalog purposes, it is to get clustering results to compare clustering compute from the full sample that has our redshfit assignments in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clustering_catalog(filename, year):\n",
    "    table = Table.read(filename, format='fits')\n",
    "    print(table.colnames)\n",
    "    \n",
    "    table.keep_columns(['TARGETID', 'DEC', 'RA', 'Z','NTILE', 'WEIGHT', 'WEIGHT_ZFAIL'])\n",
    "    \n",
    "    \n",
    "    add_NTILE_MINE_to_table(table, year)\n",
    "    table['NTID'] = table['NEAREST_TILEIDS'][:,0]\n",
    "    table.remove_columns(['NEAREST_TILEIDS'])\n",
    "    \n",
    "    sv3tiles = read_tiles_Y3_sv3()\n",
    "    galaxies_df = table_to_df(table)\n",
    "    ntiles_inside, nearest_tile_ids = find_tiles_for_galaxies(sv3tiles, galaxies_df, 10)\n",
    "    if 'NTILE_MINE_SV3' in table.columns:\n",
    "        table.remove_columns(['NTILE_MINE_SV3'])\n",
    "    #if 'NEAREST_TILEIDS_SV3' in table.columns:\n",
    "    #    table.remove_columns(['NEAREST_TILEIDS_SV3'])\n",
    "    table.add_column(ntiles_inside, name=\"NTILE_MINE_SV3\")\n",
    "    #table.add_column(nearest_tile_ids, name=\"NEAREST_TILEIDS_SV3\")\n",
    "    \n",
    "    return table.to_pandas()\n",
    "\n",
    "\n",
    "def prep_for_clustering(df: pd.DataFrame):\n",
    "        \n",
    "    # check for duplicate targetid\n",
    "    df = df.drop_duplicates(subset='TARGETID', keep='first')\n",
    "    print(len(df))\n",
    "\n",
    "    df.rename(columns={'DEC': 'Dec', 'Z': 'z'}, inplace=True)\n",
    "    df['REGION'] = tile_to_region(df['NTID'])\n",
    "\n",
    "    innerdf = df[df['NTILE_MINE_SV3'] >= 10]\n",
    "    print(len(innerdf))\n",
    "\n",
    "    # Drop the bad two regions for equal comparison\n",
    "    to_remove = np.isin(innerdf['REGION'], sv3_poor_y3overlap)\n",
    "    innerdf = innerdf.loc[~to_remove]\n",
    "\n",
    "    innerdf.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    group_catalog = deserialize(cat.bgs_sv3_fiberonly_10p)\n",
    "    group_catalog.all_data.rename(columns={'target_id': 'TARGETID'}, inplace=True)\n",
    "    innerdf = pd.merge(innerdf, group_catalog.all_data.loc[:, ['TARGETID', 'quiescent']], on='TARGETID', how='inner', validate='one_to_one')\n",
    "\n",
    "\n",
    "    return innerdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SV3 PIP Clustering Calculation\n",
    "randoms = pickle.load(open(MY_RANDOMS_SV3_CLUSTERING, 'rb'))\n",
    "randoms = randoms[randoms['NTILE_MINE'] >= 10] # Match footrpint of data\n",
    "dfN = get_clustering_catalog(BGS_SV3_CLUSTERING_N_BRIGHT_FILE, 'sv3')\n",
    "dfS = get_clustering_catalog(BGS_SV3_CLUSTERING_S_BRIGHT_FILE, 'sv3')\n",
    "df = pd.concat([dfN, dfS])\n",
    "innerdf = prep_for_clustering(df)\n",
    "results = wp.calculate_wp_from_df(innerdf, randoms, data_weights=innerdf['WEIGHT'], rand_weights=randoms['WEIGHT'])\n",
    "pickle.dump(results, open(OUTPUT_FOLDER + 'sv3_pip_clustering_proper.pkl', 'wb'))\n",
    "\n",
    "randoms = pickle.load(open(MY_RANDOMS_SV3, 'rb'))\n",
    "randoms = randoms[randoms['NTILE_MINE'] >= 10] # Match footrpint of data\n",
    "dfN = get_clustering_catalog(BGS_SV3_CLUSTERING_N_BRIGHT_FILE, 'sv3')\n",
    "dfS = get_clustering_catalog(BGS_SV3_CLUSTERING_S_BRIGHT_FILE, 'sv3')\n",
    "df = pd.concat([dfN, dfS])\n",
    "innerdf = prep_for_clustering(df)\n",
    "results = wp.calculate_wp_from_df(innerdf, randoms, data_weights=innerdf['WEIGHT'])\n",
    "pickle.dump(results, open(OUTPUT_FOLDER + 'sv3_pip_clustering.pkl', 'wb')) # no random weights, bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y3 Cut to SV3 Clustering Calculation\n",
    "# Using SV3 randoms, not Y3 cut to SV3. Is that right? TODO BUG\n",
    "# No, the weights will be all wrong. Need to use Y3 randoms\n",
    "\n",
    "y3_likesv3_df = get_clustering_catalog(BGS_Y3_CLUSTERING_FILE, '3')\n",
    "y3_innerdf = prep_for_clustering(y3_likesv3_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randoms = pickle.load(open(MY_RANDOMS_Y3_LIKESV3_CLUSTERING, 'rb'))\n",
    "results = wp.calculate_wp_from_df(y3_innerdf, randoms, data_weights=y3_innerdf['WEIGHT'], rand_weights=randoms['WEIGHT'])\n",
    "pickle.dump(results, open(OUTPUT_FOLDER + 'y3_likesv3_pip_clustering_proper.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect difference between full and clustering randoms\n",
    "rtable = Table.read(BGS_SV3_CLUSTERING_RAND_FILE.replace(\"X\", str(0)), format='fits')\n",
    "rtable2 = Table.read(BGS_SV3_RAND_FILE.replace(\"X\", str(0)), format='fits')\n",
    "\n",
    "print(rtable.colnames)\n",
    "print(rtable2.colnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What do SV3 Weights look like?\n",
    "randoms = pickle.load(open(MY_RANDOMS_SV3_CLUSTERING, 'rb'))\n",
    "print(np.isclose(randoms.WEIGHT, 1.0).sum() / len(randoms))\n",
    "print(np.average(randoms.WEIGHT))\n",
    "plt.hist(randoms.WEIGHT, bins=np.logspace(-.01, 1.15, 40))\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
