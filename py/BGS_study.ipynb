{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is code for studying the BGS data from the LSS Catalogs prior to running group finding. There are blocks to create the \"Merged Files\" that are inputs for the preprocessing and groupfinding routines in the GroupCatalog class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import astropy.coordinates as coord\n",
    "import astropy.units as u\n",
    "import astropy.io.fits as fits\n",
    "from astropy.table import Table,join,vstack,unique,QTable\n",
    "import sys\n",
    "from urllib.parse import urljoin\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "if './SelfCalGroupFinder/py/' not in sys.path:\n",
    "    sys.path.append('./SelfCalGroupFinder/py/')\n",
    "from pyutils import *\n",
    "from dataloc import *\n",
    "from photoz import *\n",
    "from bgs_helpers import *\n",
    "import groupcatalog as gc\n",
    "from nnanalysis import *\n",
    "from plotting import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEEP_PASSES = 1\n",
    "APP_MAG_CUT = 19.5 # BGS BRIGHT, though 19.54 for some cameras. I don't know if FLUX_R has been corrected for this.\n",
    "#APP_MAG_CUT = 20.0 # BGS FAINT, and in SV3 it is 20.3 instead\n",
    "Z_MIN = 0.001\n",
    "Z_MAX = 0.5\n",
    "KEEP_ONLY_OBSERVED = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Merged Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build SV3 Merged File\n",
    "This requires Y3 Merged file to be built first, because we add in Y3 galaxies to supplement the NN catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv3_table = Table.read(BGS_SV3_ANY_FULL_FILE, format='fits')\n",
    "sv3_table.keep_columns(['TARGETID', 'SPECTYPE', 'DEC', 'RA', 'Z_not4clus', 'NUMOBS', 'FLUX_R', 'FLUX_G', 'PROB_OBS', 'ZWARN', 'DELTACHI2', 'NTILE', 'TILES', 'TILEID', 'MASKBITS'])\n",
    "sv3_table.rename_column('Z_not4clus', 'Z')\n",
    "\n",
    "sv3_df = sv3_table.to_pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv3_table = Table.read(BGS_SV3_ANY_FULL_FILE, format='fits')\n",
    "print(f\"Read {len(sv3_table)} galaxies from {BGS_SV3_ANY_FULL_FILE}\")\n",
    "\n",
    "# The lost galaxies will not have fastspecfit rows I think\n",
    "sv3_table = add_fastspecfit_columns(sv3_table, 'sv3')\n",
    "\n",
    "# Filter to needed columns only and save\n",
    "sv3_table.keep_columns(['TARGETID', 'SPECTYPE', 'DEC', 'RA', 'Z_not4clus', 'NUMOBS', 'FLUX_R', 'FLUX_G', 'PROB_OBS', 'ZWARN', 'DELTACHI2', 'NTILE', 'TILES', 'TILEID', 'MASKBITS', 'DN4000', 'DN4000_MODEL', 'ABSMAG01_SDSS_G', 'ABSMAG01_SDSS_R'])\n",
    "sv3_table.rename_column('Z_not4clus', 'Z')\n",
    "\n",
    "add_mag_columns(sv3_table)\n",
    "sv3_table = add_NTILE_MINE_to_table(sv3_table, \"sv3\")\n",
    "\n",
    "sv3_table.write(IAN_BGS_SV3_MERGED_NOY3_FILE, format='fits', overwrite='True')\n",
    "\n",
    "add_photz_columns(IAN_BGS_SV3_MERGED_NOY3_FILE, IAN_PHOT_Z_FILE_NOSPEC) # For SV3 Analysis we want to analyze pure photo-z's, so use the version without external spec-z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now remove galaxies in the patches of SV3 that have poor overlap with Y3\n",
    "sv3_table: Table = Table.read(IAN_BGS_SV3_MERGED_NOY3_FILE, format='fits')\n",
    "print(len(sv3_table))\n",
    "\n",
    "sv3_table['region'] = tile_to_region(sv3_table['NEAREST_TILEIDS'][:,0])\n",
    "to_remove = np.isin(sv3_table['region'], sv3_poor_y3overlap)\n",
    "sv3_table.remove_rows(to_remove)\n",
    "print(len(sv3_table))\n",
    "\n",
    "sv3_table.write(IAN_BGS_SV3_MERGED_NOY3_FILE, format='fits', overwrite='True')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now add Y3 galaxies to supplement the NN catalog, especialy valuable at the edges of the regions\n",
    "# They won't go into the main catalog because their NTILE_MINE is < 10\n",
    "sv3_table: Table = Table.read(IAN_BGS_SV3_MERGED_NOY3_FILE, format='fits')\n",
    "y3_table: Table = Table.read(IAN_BGS_Y3_MERGED_FILE_KIBO, format='fits')\n",
    "#print(np.mean(y3_table['PROB_OBS']))\n",
    "# Let's cut Y3 data down to targets somewhat close to SV3 regions\n",
    "# No point in keeping rest and slowing down code\n",
    "gals_coord = coord.SkyCoord(ra=y3_table['RA']*u.degree, dec=y3_table['DEC']*u.degree, frame='icrs')\n",
    "close_array = gc.get_objects_near_sv3_regions(gals_coord, 2.5) # 2.5 deg radius is generously around the center of each SV3 region\n",
    "\n",
    "y3_table = y3_table[close_array]\n",
    "\n",
    "print(f\"{len(y3_table)} galaxies will be added for SV3 NN catalog\")\n",
    "assert (y3_table['NTILE_MINE'] > 9).sum() == 0, \"Y3 shouldn't add any galaxies that will go into the catalog\"\n",
    "\n",
    "# if the y3 table doesn't have prob_obs as is the case in JURA, add a prob_obs with 0.5 for everything\n",
    "#if 'PROB_OBS' not in y3_table.columns:\n",
    "#    print(\"PROB_OBS column missing from Y3\")\n",
    "#    y3_table.add_column(np.full(len(y3_table), 0.5), name=\"PROB_OBS\")\n",
    "\n",
    "# Find targets in Y3 that are already in SV3\n",
    "y3_in_sv3 = np.isin(y3_table['TARGETID'], sv3_table['TARGETID'])\n",
    "sv3_in_y3 = np.isin(sv3_table['TARGETID'], y3_table['TARGETID'])\n",
    "assert y3_in_sv3.sum() == sv3_in_y3.sum()\n",
    "print(f\"Common targets between Y3 and SV3: {y3_in_sv3.sum()}; {y3_in_sv3.sum() / len(y3_table):.2%} of Y3 cut and {sv3_in_y3.sum() / len(sv3_table):.2%} of SV3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Steal p_obs from SV3 for Y3 galaxies that are already in SV3\n",
    "print(f\"Stealing {y3_in_sv3.sum()} p_obs values from  Y3 galaxies for SV3\")\n",
    "sv3_table['PROB_OBS'][sv3_in_y3] = y3_table['PROB_OBS'][y3_in_sv3]\n",
    "sv3_table['PROB_OBS'][~sv3_in_y3] = np.nan #0.689"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows from y3_table that have TARGETID already in sv3_table\n",
    "print(f\"Removing {y3_in_sv3.sum()} (of {len(y3_table)}) Y3 galaxies that are already in SV3 catalog\")\n",
    "y3_table.remove_rows(y3_in_sv3)\n",
    "\n",
    "#colname = 'NEAREST_TILEIDS'\n",
    "#print(sv3_table[colname].shape)\n",
    "#print(y3_table[colname].shape)\n",
    "\n",
    "sv3_table.remove_column('TILES')\n",
    "y3_table.remove_column('TILES')\n",
    "\n",
    "combined = vstack([sv3_table, y3_table], join_type='outer')\n",
    "\n",
    "# Ensure resultant data is what we want\n",
    "print(sv3_table.columns)\n",
    "print(y3_table.columns)\n",
    "print(combined.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.write(IAN_BGS_SV3_MERGED_FILE, format='fits', overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Empty Photo-z Ledger file\n",
    " \n",
    "See photoz.py for the code that populates this file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INITIALIZE LIGHTWEIGHT DESI BGS PHOTO-Z TABLE\n",
    "# Don't re-run! Will overwrite the file.\n",
    "\"\"\"\n",
    "desi_table = Table.read(IAN_BGS_Y3_MERGED_FILE, format='fits')\n",
    "desi_table2 = Table.read(IAN_BGS_SV3_MERGED_NOY3_FILE, format='fits')\n",
    "\n",
    "assert len(np.unique(desi_table['TARGETID'])) == len(desi_table), \"There are duplicate TARGETIDs in the Y3 file\"\n",
    "assert len(np.unique(desi_table2['TARGETID'])) == len(desi_table2), \"There are duplicate TARGETIDs in the SV3 file\"\n",
    "\n",
    "desi_table.keep_columns(['TARGETID', 'RA', 'DEC'])\n",
    "desi_table2.keep_columns(['TARGETID', 'RA', 'DEC'])\n",
    "\n",
    "desi_targets_table = vstack([desi_table, desi_table2], join_type='inner')\n",
    "desi_targets_table = unique(desi_targets_table, 'TARGETID')\n",
    "desi_targets_table['Z_LEGACY_BEST'] = NO_PHOTO_Z\n",
    "\n",
    "# add columns for 'RELEASE', 'BRICKID', 'OBJID', 'REF_CAT', 'MATCH_DIST' with no values\n",
    "desi_targets_table.add_column(np.zeros(len(desi_targets_table), dtype=int), name='RELEASE')\n",
    "desi_targets_table.add_column(np.zeros(len(desi_targets_table), dtype=int), name='BRICKID')\n",
    "desi_targets_table.add_column(np.zeros(len(desi_targets_table), dtype=int), name='OBJID')\n",
    "desi_targets_table.add_column(np.zeros(len(desi_targets_table), dtype='S2'), name='REF_CAT')\n",
    "desi_targets_table.add_column(np.full(len(desi_targets_table), 999999, dtype=float), name='MATCH_DIST')\n",
    "\n",
    "\n",
    "desi_targets_table = desi_targets_table.to_pandas()\n",
    "desi_targets_table.set_index('TARGETID', inplace=True)\n",
    "pickle.dump(desi_targets_table, open(IAN_PHOT_Z_FILE, 'wb'))\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read BGS_IMAGES_FOLDER + \"terminal.txt\" into an array of strings, 1 per line\n",
    "f = open(BGS_IMAGES_FOLDER + \"terminal.txt\", \"r\")\n",
    "lines = f.readlines()\n",
    "f.close()\n",
    "\n",
    "# Now filter it down to ones like this:\n",
    "#Start processing brick #X\n",
    "#Matched 0 out of Z\n",
    "#start_lines = [line for line in lines if \"Start processing brick\" in line]\n",
    "matched_lines = [line for line in lines if \"Matched\" in line]\n",
    "\n",
    "#print(len(start_lines))\n",
    "print(len(matched_lines))\n",
    "\n",
    "# Extract X, Y, Z from each line \n",
    "#start_lines = [line.split()[3] for line in start_lines]    \n",
    "matched_lines = [int(line.split()[1]) for line in matched_lines]\n",
    "\n",
    "# Strip away # and convert to int\n",
    "#start_lines = [int(line[1:]) for line in start_lines]\n",
    "\n",
    "# The index is the brick number for matched_lines\n",
    "# Find the brick numbers where the value is 0 and remember those indexes\n",
    "zero_indexes = [i for i in range(len(matched_lines)) if matched_lines[i] == 0]\n",
    "\n",
    "#pickle.dump(zero_indexes, open(BRICKS_TO_SKIP_S_FILE, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle.dump([], open(BRICKS_TO_SKIP_N_FILE, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Y1 Merged File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_merged_file(BGS_ANY_FULL_FILE, IAN_BGS_MERGED_FILE, \"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a file with just TARGETID and QUIESCENT columns to send to Joe for Clustering\n",
    "tbl = Table.read(IAN_BGS_MERGED_FILE, format='fits') \n",
    "tbl.keep_columns(['TARGETID', 'QUIESCENT'])\n",
    "tbl.write(BGS_Y1_FOLDER + \"COLOR_LOOKUP_IRON.fits\", format='fits', overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Y3 Merged File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_sv3_clustering_randoms_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_sv3_full_randoms_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_merged_file(BGS_Y3_ANY_FULL_FILE, IAN_BGS_Y3_MERGED_FILE_KIBO, \"3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra step\n",
    "# Add columns to allow the possibility to make a Y3 catalog that is cut to SV3 regions.\n",
    "table = Table.read(IAN_BGS_Y3_MERGED_FILE_KIBO, format='fits')\n",
    "sv3tiles = read_tiles_Y3_sv3()\n",
    "\n",
    "galaxies_df = table_to_df(table)\n",
    "\n",
    "ntiles_inside, nearest_tile_ids = find_tiles_for_galaxies(sv3tiles, galaxies_df, 10)\n",
    "if 'NTILE_MINE_SV3' in table.columns:\n",
    "    table.remove_columns(['NTILE_MINE_SV3'])\n",
    "if 'NEAREST_TILEIDS_SV3' in table.columns:\n",
    "    table.remove_columns(['NEAREST_TILEIDS_SV3'])\n",
    "table.add_column(ntiles_inside, name=\"NTILE_MINE_SV3\")\n",
    "table.add_column(nearest_tile_ids, name=\"NEAREST_TILEIDS_SV3\")\n",
    "\n",
    "table.write(IAN_BGS_Y3_MERGED_FILE_KIBO, format='fits', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a file with just TARGETID and QUIESCENT columns to send to Joe for Clustering\n",
    "tbl = Table.read(IAN_BGS_Y3_MERGED_FILE_KIBO, format='fits') \n",
    "tbl.keep_columns(['TARGETID', 'QUIESCENT'])\n",
    "tbl.write(BGS_Y3_FOLDER_KIBO + \"COLOR_LOOKUP_KIBO.fits\", format='fits', overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine data in a Merged BGS File (SV3, Y1, Y3, whatever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick one\n",
    "#table = Table.read(IAN_BGS_MERGED_FILE, format='fits')\n",
    "#table = Table.read(IAN_BGS_SV3_MERGED_FILE, format='fits')\n",
    "#table = Table.read(IAN_BGS_SV3_MERGED_NOY3_FILE, format='fits')\n",
    "table = Table.read(IAN_BGS_Y3_MERGED_FILE_KIBO, format='fits')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cut to the galaxy data we actually need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO this gets easilly out of sync with the .py file that does the 'production' filtering\n",
    "\n",
    "if np.ma.is_masked(table['Z']):\n",
    "    print(\"Masked table\")\n",
    "    z_obs = table['Z'].data.data.astype(\"<f8\")\n",
    "    obj_type = table['SPECTYPE'].data.data\n",
    "    unobserved = table['Z'].mask # the masked values are what is unobserved\n",
    "    deltachi2 = table['DELTACHI2'].data.data  \n",
    "else:\n",
    "    print(\"Unmasked table\")\n",
    "    # SV3 version didn't do this\n",
    "    z_obs = table['Z']\n",
    "    obj_type = table['SPECTYPE']\n",
    "    unobserved = table['Z'].astype(\"<i8\") == 999999\n",
    "    deltachi2 = table['DELTACHI2']\n",
    "\n",
    "maskbits = table['MASKBITS'].data.data if np.ma.is_masked(table['MASKBITS']) else table['MASKBITS']\n",
    "dec = table['DEC'].astype(\"<f8\")\n",
    "ra = table['RA'].astype(\"<f8\")\n",
    "z_phot = table['Z_PHOT'].astype(\"<f8\")\n",
    "target_id = table['TARGETID']\n",
    "app_mag_r = get_app_mag(table['FLUX_R'])\n",
    "app_mag_g = get_app_mag(table['FLUX_G'])\n",
    "flux_r = table['FLUX_R'].astype(\"<f8\")\n",
    "flux_g = table['FLUX_G'].astype(\"<f8\")\n",
    "g_r_apparent = app_mag_g - app_mag_r\n",
    "abs_mag_r = table['ABS_MAG_R']\n",
    "abs_mag_g = table['ABS_MAG_G']\n",
    "#sdss_g_r = table['ABSMAG_SDSS_G'] - table['ABSMAG_SDSS_R'] \n",
    "G_R_JM1 = table['ABSMAG01_SDSS_G'] - table['ABSMAG01_SDSS_R']\n",
    "p_obs = table['PROB_OBS'] \n",
    "ntiles = table['NTILE'].astype(\"<i8\")\n",
    "#tiles = table['TILES']\n",
    "tile_id = table['TILEID'] if 'TILEID' in table.columns else None\n",
    "#numobs = table['NUMOBS']\n",
    "#tile_locid = table['TILELOCID']\n",
    "ntiles_mine = table['NTILE_MINE']\n",
    "tileids = table['NEAREST_TILEIDS'][:,0].astype(\"<i8\") # TODO there are 10 here, we want NTILES_MINE many...\n",
    "#abs_mag_sdss = table['ABSMAG_SDSS_R']\n",
    "dn4000 = table['DN4000_MODEL'].data.data\n",
    "ref_cat = table['REF_CAT']\n",
    "quiescent = table['QUIESCENT']\n",
    "\n",
    "\n",
    "before_count = len(dec)\n",
    "print(before_count, \"objects in FITS file\")\n",
    "\n",
    "# TODO BUG Can we be mistaking STARS for GALAXIES?\n",
    "# Make filter array (True/False values)\n",
    "PASSES_REQUIRED = [1,2,3,4,10]\n",
    "\n",
    "galaxy_observed_filter = obj_type == b'GALAXY'\n",
    "app_mag_filter = app_mag_r < APP_MAG_CUT\n",
    "redshift_filter = z_obs > Z_MIN\n",
    "redshift_hi_filter = z_obs < Z_MAX\n",
    "deltachi2_filter = deltachi2 > 40\n",
    "#abs_mag_sdss_filter = abs_mag_sdss < 100\n",
    "#observed_requirements = np.all([galaxy_observed_filter, app_mag_filter, redshift_filter, redshift_hi_filter, deltachi2_filter, abs_mag_sdss_filter], axis=0)\n",
    "observed_requirements = np.all([galaxy_observed_filter, app_mag_filter, redshift_filter, redshift_hi_filter, deltachi2_filter], axis=0)\n",
    "\n",
    "treat_as_unobserved = np.all([galaxy_observed_filter, app_mag_filter, np.invert(deltachi2_filter)], axis=0)\n",
    "\n",
    "unobserved = np.all([app_mag_filter, np.logical_or(unobserved, treat_as_unobserved)], axis=0)\n",
    "keep = np.all([np.logical_or(observed_requirements, unobserved)], axis=0)\n",
    "\n",
    "print(\"\\nWhole sample:\")\n",
    "print(f\"There are {len(obj_type):,} objects in the entire sample, of which {np.sum(galaxy_observed_filter):,} are observed galaxies.\") \n",
    "\n",
    "for n in PASSES_REQUIRED:\n",
    "    n_pass_filter = ntiles_mine >= n\n",
    "    n_pass_filter_old = ntiles >= n\n",
    "    unobserved_n = np.all([n_pass_filter, unobserved], axis=0)\n",
    "    observed_requirements_n = np.all([n_pass_filter, observed_requirements], axis=0)\n",
    "    keepn = np.all([np.logical_or(observed_requirements_n, unobserved_n)], axis=0)\n",
    "\n",
    "    print(f\"\\n{n}-pass analysis (NTILE_MINE):\")\n",
    "    print(f\"There are {np.sum(observed_requirements_n):,} galaxies in the <{APP_MAG_CUT} mag sample that pass our quality checks.\")\n",
    "    print(f\"There are {np.sum(unobserved_n):,} unobserved galaxies, including bad observed galaxies.\")\n",
    "    print(f\"This {n}-pass catalog would have {np.sum(keepn):,} galaxies ({np.sum(unobserved_n) / np.sum(keepn) * 100:.2f}% lost).\")\n",
    "\n",
    "    # We've demonstratred this is definetely not what we want\n",
    "    #unobserved_n_old = np.all([n_pass_filter_old, unobserved], axis=0)\n",
    "    #observed_requirements_n_old = np.all([n_pass_filter_old, observed_requirements], axis=0)\n",
    "    #keepn_old = np.all([np.logical_or(observed_requirements_n_old, unobserved_n_old)], axis=0)\n",
    "    #print(f\"\\n{n}-pass analysis (NTILE):\")\n",
    "    #print(f\"There are {np.sum(observed_requirements_n_old):,} galaxies in the bright (<{APP_MAG_CUT} mag) sample that pass our quality checks.\")\n",
    "    #print(f\"There are {np.sum(unobserved_n_old):,} unobserved galaxies, including bad observed galaxies.\")\n",
    "    #print(f\"This {n}-pass catalog would have {np.sum(keepn_old):,} galaxies ({np.sum(unobserved_n_old) / np.sum(keepn_old) * 100:.2f}% lost).\")\n",
    "\n",
    "# FOR PARTS BELOW SET WHAT YOU WANT TO KEEP!\n",
    "if KEEP_ONLY_OBSERVED:\n",
    "    keep = np.all([keep, ntiles_mine >= KEEP_PASSES, ~unobserved], axis=0)\n",
    "else:\n",
    "    keep = np.all([keep, ntiles_mine >= KEEP_PASSES], axis=0)\n",
    "\n",
    "obj_type = obj_type[keep]\n",
    "dec = dec[keep]\n",
    "ra = ra[keep]\n",
    "z_phot = z_phot[keep]\n",
    "z_obs = z_obs[keep]\n",
    "target_id = target_id[keep] \n",
    "flux_r = flux_r[keep]\n",
    "app_mag_r = app_mag_r[keep]\n",
    "app_mag_g = app_mag_g[keep]\n",
    "g_r_apparent = g_r_apparent[keep]\n",
    "p_obs = p_obs[keep]\n",
    "unobserved = unobserved[keep]\n",
    "deltachi2 = deltachi2[keep]\n",
    "ntiles = ntiles[keep]\n",
    "abs_mag_r = abs_mag_r[keep]\n",
    "abs_mag_g = abs_mag_g[keep]\n",
    "#tiles = tiles[keep]\n",
    "#ztileid = ztileid[keep]\n",
    "ntiles_mine = ntiles_mine[keep]\n",
    "tileids = tileids[keep]\n",
    "tile_id = tile_id[keep] if 'TILEID' in table.columns else None\n",
    "#numobs = numobs[keep]\n",
    "#tile_locid = tile_locid[keep]\n",
    "#abs_mag_sdss = abs_mag_sdss[keep]\n",
    "#sdss_g_r = sdss_g_r[keep]\n",
    "G_R_JM1 = G_R_JM1[keep]\n",
    "dn4000 = dn4000[keep]\n",
    "ref_cat = ref_cat[keep]\n",
    "maskbits = maskbits[keep]\n",
    "indexes_not_assigned = np.flatnonzero(unobserved)\n",
    "quiescent = quiescent[keep]\n",
    "\n",
    "after_count = len(dec)\n",
    "\n",
    "print(f\"\\nAfter all filters we have {after_count:,} of the original {before_count:,} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make maps\n",
    "two_pass_filter = ntiles_mine >= 2 \n",
    "three_pass_filter = ntiles_mine >= 3 \n",
    "four_pass_filter = ntiles_mine >= 4 \n",
    "\n",
    "ra2 = ra[two_pass_filter]\n",
    "dec2 = dec[two_pass_filter]\n",
    "tileids2 =  tileids[two_pass_filter]\n",
    "unobserved2 = unobserved[two_pass_filter]\n",
    "\n",
    "ra3 = ra[three_pass_filter]\n",
    "dec3 = dec[three_pass_filter]\n",
    "tileids3 =  tileids[three_pass_filter]\n",
    "unobserved3 = unobserved[three_pass_filter]\n",
    "\n",
    "ra4 = ra[four_pass_filter]\n",
    "dec4 = dec[four_pass_filter]\n",
    "tileids4 =  tileids[four_pass_filter]\n",
    "unobserved4 = unobserved[four_pass_filter]\n",
    "\n",
    "one_pass_df = pd.DataFrame({'RA': ra, 'DEC': dec, 'Z_ASSIGNED_FLAG': unobserved, 'TILEID': tileids})\n",
    "two_pass_df = pd.DataFrame({'RA': ra2, 'DEC': dec2, 'Z_ASSIGNED_FLAG': unobserved2, 'TILEID': tileids2})\n",
    "three_pass_df = pd.DataFrame({'RA': ra3, 'DEC': dec3, 'Z_ASSIGNED_FLAG': unobserved3, 'TILEID': tileids3})\n",
    "four_pass_df = pd.DataFrame({'RA': ra4, 'DEC': dec4, 'Z_ASSIGNED_FLAG': unobserved4, 'TILEID': tileids4})\n",
    "\n",
    "#fig=make_map(ra, dec)\n",
    "#ra_4 = ra[four_pass_filter]\n",
    "#dec_4 = dec[four_pass_filter]\n",
    "#print(f\"Number of 4-pass galaxies: {len(ra_4)}, number of 3-pass galaxies: {len(ra)}\")\n",
    "#fig=make_map(ra_4, dec_4, fig=fig, alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(ntiles_mine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiles_BGS = read_tiles_Y1_main()\n",
    "plot_positions(one_pass_df, three_pass_df, tiles_df=tiles_BGS, DEG_LONG=5, split=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See where missing photo-z's are\n",
    "#idx_no_zphot = z_phot == NO_PHOTO_Z\n",
    "#no_photoz_df = pd.DataFrame({'RA': ra[idx_no_zphot], 'DEC': dec[idx_no_zphot], 'Z_ASSIGNED_FLAG': unobserved[idx_no_zphot], 'TILEID': tileids[idx_no_zphot]})\n",
    "#plot_positions(one_pass_df, no_photoz_df, tiles_df=tiles_BGS, DEG_LONG=3, split=False)\n",
    "\n",
    "fig=make_map(ra3, dec3, dpi=300, alpha=0.02)\n",
    "#ra_4 = ra[four_pass_filter]\n",
    "#dec_4 = dec[four_pass_filter]\n",
    "#print(f\"Number of 4-pass galaxies: {len(ra_4)}, number of 3-pass galaxies: {len(ra)}\")\n",
    "#fig=make_map(ra[idx_no_zphot], dec[idx_no_zphot], fig=fig, alpha=0.1)\n",
    "#fig=make_map(ra[ntiles_mine >= 10 ], dec[ntiles_mine >= 10 ], fig=fig, alpha=0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_map(ra, dec, fig=fig, alpha=0.02)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the random redshift hashtables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For comparison\n",
    "with open(IAN_MXXL_LOST_APP_TO_Z_FILE, 'rb') as f:\n",
    "    mxxl_app_mag_bins, mxxl_the_map = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is how we did it for MXXL\n",
    "app_mag_bins, the_map = build_app_mag_to_z_map(app_mag_r[~unobserved], z_obs[~unobserved])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New better way I think\n",
    "app_mag_bins2, the_map2 = build_app_mag_to_z_map_2(app_mag_r[~unobserved], z_obs[~unobserved])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_mag_bins2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in the_map2.keys():\n",
    "    print(f\"App mag bin {key} has {len(the_map2[key])} galaxies.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    m = app_mag_r[~unobserved][i]\n",
    "    zp = z_phot[~unobserved][i]\n",
    "    distribution = the_map2[np.digitize(m, app_mag_bins2)]\n",
    "    percentiles = np.percentile(distribution, [5, 16, 50, 84, 95])\n",
    "    mean = np.mean(distribution)\n",
    "    std = np.std(distribution)\n",
    "    # what sigma does zp lie at?\n",
    "    sigma = (zp - mean) / std\n",
    "    print(f\"App mag: {m:.2f}, z_p: {zp:.4f}, sigma: {sigma:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And now the photo-z infused way (there is a version 3 as well which fills empty/small bins with neighborin values)\n",
    "app_mag_bins3, z_phot_bins, the_map3 = build_app_mag_to_z_map_4(app_mag_r[~unobserved].copy(), z_phot[~unobserved].copy(),  z_obs[~unobserved].copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.printoptions(precision=4, linewidth=200):\n",
    "    for k in the_map3.keys():\n",
    "        print(f\"Key {k} has {len(the_map3[k])} galaxies\")\n",
    "        #print(f\"Key {k} has {len(the_map3[k])} galaxies: {np.percentile(the_map3[k], [5, 16, 50, 84, 95])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(0, 0.5, 50)\n",
    "\n",
    "#print(bins)\n",
    "plt.figure(figsize=(10, 6))\n",
    "trash=plt.hist(the_map2[33],bins=bins, color='green', density=True, histtype='step', label=app_mag_bins2[33], linestyle='dashed')\n",
    "trash=plt.hist(the_map2[284],bins=bins, color='darkred', density=True, histtype='step', label=app_mag_bins2[284], linestyle='dashed')\n",
    "\n",
    "trash=plt.hist(the_map3[39, 10], bins=bins, color=[0.0, 1.0, 0.0], density=True, histtype='step', label=f\"z={z_phot_bins[10]:.3f} r={app_mag_bins3[39]:.2f}\", linestyle='dotted')\n",
    "trash=plt.hist(the_map3[30, 10], bins=bins, color=[0.1, 0.8, 0.0], density=True, histtype='step', label=f\"z={z_phot_bins[10]:.3f} r={app_mag_bins3[30]:.2f}\", linestyle='dotted')\n",
    "trash=plt.hist(the_map3[22, 10], bins=bins, color=[0.2, 0.6, 0.0], density=True, histtype='step', label=f\"z={z_phot_bins[10]:.3f} r={app_mag_bins3[22]:.2f}\", linestyle='dotted')\n",
    "trash=plt.hist(the_map3[15, 10], bins=bins, color=[0.3, 0.4, 0.0], density=True, histtype='step', label=f\"z={z_phot_bins[10]:.3f} r={app_mag_bins3[15]:.2f}\", linestyle='dotted')\n",
    "\n",
    "trash=plt.hist(mxxl_the_map[29],bins=bins, color='green', density=True, histtype='step', label=mxxl_app_mag_bins[29])\n",
    "#trash=plt.hist(app_mag_bins_read[50],bins=bins, color='orange', density=True, histtype='step', label=app_mag_bins_read[50], linestyle='dotted')\n",
    "trash=plt.hist(mxxl_the_map[90],bins=bins, color='darkred', density=True, histtype='step', label=mxxl_app_mag_bins[90])\n",
    "plt.legend()\n",
    "\n",
    "plt.xlim(0, 0.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(BGS_Y3_LOST_APP_TO_Z_FILE, 'wb') as f:\n",
    "    pickle.dump((app_mag_bins2, the_map2), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(BGS_Y3_LOST_APP_AND_ZPHOT_TO_Z_FILE, 'wb') as f:\n",
    "    pickle.dump((app_mag_bins3, z_phot_bins, the_map3), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make SDSS BGS-Cut Variant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=make_map(ra3, dec3)\n",
    "\n",
    "sdss_df = pd.read_csv(SDSS_v2_DAT_FILE, header=None, sep=' ', names=['RA', 'DEC', 'Z', 'LOGLGAL', 'VMAX', 'color_flag', 'chi'])\n",
    "sdss_gp_df = pd.read_csv(SDSS_v2_GALPROPS_FILE, delimiter=' ', header=None, names=('MAG_G', 'MAG_R', 'SIGMA_V', 'DN4000', 'CONCENTRATION', 'LOG_M_STAR', 'Z_ASSIGNED_FLAG'))\n",
    "\n",
    "#fig=make_map(sdss_df['RA'].to_numpy(), sdss_df['DEC'].to_numpy(), alpha=0.3, fig=fig)\n",
    "make_map(sdss_df.loc[sdss_gp_df.Z_ASSIGNED_FLAG == AssignedRedshiftFlag.NEIGHBOR_ONE.value, 'RA'].to_numpy(), sdss_df.loc[sdss_gp_df.Z_ASSIGNED_FLAG == AssignedRedshiftFlag.NEIGHBOR_ONE.value, 'DEC'].to_numpy(), alpha=0.5, dotsize=0.05, fig=fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sra = sdss_df['RA'].to_numpy()    \n",
    "ra_angles = coord.Angle(sra*u.degree)\n",
    "ra_angles = ra_angles.wrap_at(180*u.degree)\n",
    "sras = ra_angles.value\n",
    "sdec = sdss_df['DEC'].to_numpy()\n",
    "print(sra.min(), sra.max())\n",
    "print(sdec.min(), sdec.max())\n",
    "\n",
    "#stripe_to_cut = np.logical_and(sras < 90, sras > -90) # removes the 3 stripes\n",
    "block_to_cut = np.logical_and(np.logical_and(sra > 130, sra < 195), np.logical_and(sdec > 15, sdec < 55))\n",
    "block2_to_cut = np.logical_and(np.logical_and(sra > 210, sra < 225), np.logical_and(sdec > 15, sdec < 30))\n",
    "rows_to_cut = np.logical_or(block2_to_cut, block_to_cut)\n",
    "\n",
    "print(f\"Cutting {np.sum(rows_to_cut)} rows from SDSS data\")\n",
    "\n",
    "sdss_cut = sdss_df.loc[~rows_to_cut]\n",
    "make_map(sdss_cut['RA'].to_numpy(), sdss_cut['DEC'].to_numpy(), alpha=0.3, fig=fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{spectroscopic_complete_percent(sdss_gp_df.Z_ASSIGNED_FLAG):.1%} of SDSS v2 galaxies have a redshift\")\n",
    "print(f\"{spectroscopic_complete_percent(sdss_gp_df[~rows_to_cut].Z_ASSIGNED_FLAG):.1%} of SDSS BGS-Cut galaxies have a redshift\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write this new catalog to a file\n",
    "sdss_cut.to_csv(SDSS_BGSCUT_DAT_FILE, sep=' ', header=False, index=False)\n",
    "sdss_gp_df.loc[~rows_to_cut].to_csv(SDSS_BGSCUT_GALPROPS_FILE, sep=' ', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test reading the results\n",
    "test1 = pd.read_csv(SDSS_BGSCUT_DAT_FILE, delimiter=' ', header=None, names=['RA', 'DEC', 'Z', 'LOGLGAL', 'VMAX', 'color_flag', 'chi'])\n",
    "test2 = pd.read_csv(SDSS_BGSCUT_GALPROPS_FILE, delimiter=' ', header=None, names=('MAG_G', 'MAG_R', 'SIGMA_V', 'DN4000', 'CONCENTRATION', 'LOG_M_STAR', 'Z_ASSIGNED_FLAG'))\n",
    "print(len(test1))\n",
    "print(len(test2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Known area of SDSS is .179, assuming constant density of galaxies we can estimate the area of the cut catalog\n",
    "print(399434 / len(sra) * .179) \n",
    "\n",
    "# Compare with my crappy other estimation code\n",
    "print(f\"Orig area estimate: {estimate_frac_area(sra, sdec)}\")\n",
    "print(f\"Cut area estimate: {estimate_frac_area(test1.RA.to_numpy(), test1['DEC'].to_numpy())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Siena Galaxy Atlas Analysis (SGA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(ref_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understand how many galaxies are affected by Siena Galaxy Atlas (SGA) Masks\n",
    "has_a_maskbit = maskbits != 0\n",
    "idx_with_masks = np.flatnonzero(maskbits)\n",
    "print(f\"{np.sum(has_a_maskbit):,} galaxies ({np.sum(has_a_maskbit) / len(maskbits) * 100:.2f}%) have a maskbit set.\")\n",
    "\n",
    "unobserved_with_maskbits = np.logical_and(has_a_maskbit, unobserved)\n",
    "print(f\"{np.sum(unobserved_with_maskbits):,} galaxies ({np.sum(unobserved_with_maskbits) / len(maskbits) * 100:.2f}%) have a maskbit set and are unobserved.\")\n",
    "\n",
    "# See https://www.legacysurvey.org/dr9/bitmasks/\n",
    "# https://github.com/legacysurvey/legacypipe/blob/master/py/legacypipe/bits.py\n",
    "BITMASK_SGA = 0x1000 \n",
    "sga_collision = (maskbits & BITMASK_SGA) != 0\n",
    "print(f\"{np.sum(sga_collision):,} galaxies ({np.sum(sga_collision) / len(maskbits) * 100:.2f}%) have a SGA collision.\")\n",
    "\n",
    "#sga_central = np.logical_or(ref_cat == b'L3', ref_cat == b'G2', ref_cat == b'GE')\n",
    "sga_central = ref_cat == b'L3'\n",
    "print(f\"{np.sum(sga_central):,} galaxies ({np.sum(sga_central) / len(maskbits) * 100:.2f}%) are SGA centrals.\")\n",
    "\n",
    "to_remove = sga_collision & ~sga_central\n",
    "print(f\"{np.sum(to_remove):,} galaxies ({np.sum(to_remove) / len(maskbits) * 100:.2f}%) have a SGA collision and are not SGA centrals.\")\n",
    "\n",
    "# Examine color of these compared to the rest\n",
    "g_r_sga = g_r_apparent[to_remove]\n",
    "bins = np.linspace(-2, 2, 100)\n",
    "plt.hist(g_r_sga, bins=bins, alpha=0.5, label='SGA Collision', density=True)\n",
    "g_r_rest = g_r_apparent[np.logical_and(~unobserved, z_obs < 0.05)]\n",
    "plt.hist(g_r_rest, bins=bins, alpha=0.5, label='Rest', density=True)\n",
    "plt.legend()\n",
    "\n",
    "to_remove_blue = to_remove & (g_r_apparent < 0.8)\n",
    "print(f\"{np.sum(to_remove_blue):,} galaxies ({np.sum(to_remove_blue) / len(maskbits) * 100:.2f}%) have a SGA collision, are not SGA centrals, and are blue enough to remove.\")\n",
    "\n",
    "df = pd.DataFrame({'RA': ra[to_remove_blue], 'DEC': dec[to_remove_blue]})\n",
    "df.to_csv(OUTPUT_FOLDER + f'sga_collisions.csv', index=False)\n",
    "\n",
    "df = pd.DataFrame({'RA': ra[to_remove & ~to_remove_blue], 'DEC': dec[to_remove & ~to_remove_blue]})\n",
    "df.to_csv(OUTPUT_FOLDER + f'sga_collisions_not_blue_enough.csv', index=False)\n",
    "\n",
    "# Inspecting these, I see that most are galaxies, not HII regions. Not sure if we want to remove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Photo-Z vs Spec-Z Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"There are {(z_phot != NO_PHOTO_Z).sum():,} ({(z_phot != NO_PHOTO_Z).sum() / len(z_phot):.1%}) targets with phot-z\")\n",
    "\n",
    "good_idx = np.flatnonzero((z_phot != NO_PHOTO_Z) & ~unobserved) #& ~(np.isclose(z_phot, z_obs, atol=0.00025, rtol=0.000001)))\n",
    "print(f\"Amongst observed galaxies there are {len(good_idx):,} ({len(good_idx) / np.sum(~unobserved) * 100:.2f}%) galaxies with photo-z.\")\n",
    "\n",
    "unobserved_with_photz = np.flatnonzero((z_phot != NO_PHOTO_Z) & unobserved)\n",
    "print(f\"Amongst unobserved galaxies there are {len(unobserved_with_photz):,} ({len(unobserved_with_photz) / np.sum(unobserved) * 100:.2f}%) with photo-z.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_z = z_phot[good_idx] - z_obs[good_idx]\n",
    "plt.hist(delta_z, bins=500, range=(-0.1, 0.1))\n",
    "#plt.yscale(\"log\")\n",
    "plt.title(\"Photo-z Quality\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xlabel(\"z_phot - z_spec\")\n",
    "plt.ylim(0, 20000)\n",
    "\n",
    "# add bars for my z_thresh\n",
    "plt.axvline(-SIM_Z_THRESH, color='red')\n",
    "plt.axvline(SIM_Z_THRESH, color='red')\n",
    "\n",
    "percentiles = np.percentile(delta_z, [16, 50, 84])\n",
    "print(f\"Median delta z: {percentiles[1]:.4f}, 16th percentile: {percentiles[0]:.4f}, 84th percentile: {percentiles[2]:.4f}\")\n",
    "percentiles = np.percentile(delta_z, [2.5, 97.5])\n",
    "print(f\"2.5th percentile: {percentiles[0]:.4f}, 97.5th percentile: {percentiles[1]:.4f}\")\n",
    "# add bars for the percentiles\n",
    "#plt.axvline(percentiles[0], color='green')\n",
    "#plt.axvline(percentiles[2], color='green')\n",
    "\n",
    "# What % fall within 0.005 of the true redshift?\n",
    "within_5_milli = np.abs(delta_z) < SIM_Z_THRESH\n",
    "print(f\"{np.sum(within_5_milli) / len(delta_z) * 100:.2f}% of galaxies have a photometric redshift within {SIM_Z_THRESH} of the spectroscopic redshift.\")\n",
    "\n",
    "# Now look only at quiescent galaxies less than 10^9 solar luminosities\n",
    "# TODO \n",
    "#luminosity = abs_mag_r_to_log_solar_L(app_mag_to_abs_mag_k(app_mag_r, z_obs, g_r_apparent))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define Gaussian and Lorentzian functions\n",
    "def gaussian(x, amp, mean, stddev):\n",
    "    return amp * np.exp(-((x - mean) ** 2) / (2 * stddev ** 2))\n",
    "\n",
    "def lorentzian(x, amp, mean, gamma, exp):\n",
    "    return amp * gamma**2 / (np.power(np.abs(x - mean), exp) + gamma**2)\n",
    "\n",
    "# Prepare data\n",
    "abs_delta_z = delta_z\n",
    "hist, bin_edges = np.histogram(abs_delta_z, bins=1000, range=(-0.2, 0.2), density=True)\n",
    "bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "\n",
    "# Fit Gaussian\n",
    "popt_gauss, _ = curve_fit(gaussian, bin_centers, hist, p0=[1, 0, 0.01], maxfev=100000)\n",
    "# Poor fit\n",
    "\n",
    "# Fit Lorentzian\n",
    "popt_lorentz, _ = curve_fit(lorentzian, bin_centers, hist, p0=[24, 0.0001, 0.0153, 2.0], maxfev=100000)\n",
    "#Lorentzian fit parameters: amp=22.902018525656057, mean=8.151405674056218e-05, gamma=0.009227129158691942, exp=2.2857148452956757\n",
    "\n",
    "\n",
    "# Plot histogram and fits\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(abs_delta_z, bins=1000, range=(-0.2, 0.2), density=True, alpha=0.6, label='Data')\n",
    "plt.plot(bin_centers, gaussian(bin_centers, *popt_gauss), label='Gaussian fit', color='red')\n",
    "plt.plot(bin_centers, lorentzian(bin_centers, *popt_lorentz), label='~Lorentzian fit', color='green')\n",
    "plt.xlabel('z_phot - z_spec')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.title('Fitting delta_z with Gaussian and ~Lorentzian')\n",
    "plt.show()\n",
    "\n",
    "# Print fit parameters\n",
    "print(f\"Gaussian fit parameters: amp={popt_gauss[0]}, mean={popt_gauss[1]}, stddev={popt_gauss[2]}\")\n",
    "print(f\"Lorentzian fit parameters: amp={popt_lorentz[0]}, mean={popt_lorentz[1]}, gamma={popt_lorentz[2]}, exp={popt_lorentz[3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SV3 Analysis\n",
    "\n",
    "SV3 is composed of 20 regions where 10 or 11 exposures eacj were taken, almost completely on top of each other.  Our SV3 analysis takes the inner part of these patches (NTILE_MINE >= 10) of these regions as the data set.  \n",
    "\n",
    "Then, we can eliminate 1 tile from each of these regions to make test sets in order to view our systematics as a function of NTILE_MINE. The order they are eliminated in matters; we need to go backwards in time.}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a DataFrame filtered down to the galaxies we want to keep\n",
    "sv3_merged_table = Table.read(IAN_BGS_SV3_MERGED_FILE, format='fits')\n",
    "sv3_merged_table.remove_column('NEAREST_TILEIDS')\n",
    "sv3_df = sv3_merged_table.to_pandas()\n",
    "print(len(sv3_df))\n",
    "sv3_df['APP_MAG_R'] = get_app_mag(sv3_df['FLUX_R'])\n",
    "unobserved = sv3_merged_table['Z'].astype(\"<i8\") == 999999\n",
    "galaxy_observed_filter = sv3_df['SPECTYPE'] == b'GALAXY'\n",
    "redshift_filter = sv3_df['Z'] > Z_MIN\n",
    "redshift_hi_filter = sv3_df['Z'] < Z_MAX\n",
    "deltachi2_filter = sv3_df['DELTACHI2'] > 40\n",
    "app_mag_filter = sv3_df['APP_MAG_R'] < 20.3\n",
    "observed_requirements = np.all([galaxy_observed_filter, app_mag_filter, redshift_filter, redshift_hi_filter, deltachi2_filter], axis=0)\n",
    "treat_as_unobserved = np.all([galaxy_observed_filter, app_mag_filter, np.invert(deltachi2_filter)], axis=0)\n",
    "\n",
    "unobserved = np.all([app_mag_filter, np.logical_or(unobserved, treat_as_unobserved)], axis=0)\n",
    "sv3_df['OBSERVED'] = np.invert(unobserved)\n",
    "keep = np.all([np.logical_or(observed_requirements, unobserved)], axis=0)\n",
    "keep = np.all([keep, sv3_df['NTILE_MINE'] >= 10], axis=0)\n",
    "\n",
    "sv3_df = sv3_df.loc[keep] \n",
    "sv3_df.reset_index(drop=True, inplace=True)\n",
    "print(len(sv3_df))\n",
    "\n",
    "# Initialize new columns for observed as function of N pass\n",
    "for i in range(0, 12):\n",
    "    sv3_df[f'OBSERVED_{i}'] = sv3_df['OBSERVED']\n",
    "\n",
    "for FAINT in [False, True]:\n",
    "\n",
    "    if not FAINT:\n",
    "        mag_filter = sv3_df['APP_MAG_R'] < 19.5\n",
    "    else:\n",
    "        mag_filter = sv3_df['APP_MAG_R'] > 19.5\n",
    "        \n",
    "    print(f\"{len(sv3_df[mag_filter]) / 138.192} galaxies per sq degree\")\n",
    "\n",
    "    for patch_number in range(len(sv3_regions_sorted)):\n",
    "        tilelist = sv3_regions_sorted[patch_number]\n",
    "        #print(f'Patch {patch_number} - TILE IDs: {tilelist}')\n",
    "        \n",
    "        row_selector = np.logical_and(sv3_df['TILEID'].isin(tilelist), mag_filter)\n",
    "\n",
    "        #one_patch_df = sv3_df[sv3_df['TILEID'].isin(tilelist)]\n",
    "        #print(f\"{len(one_patch_df)} galaxies, {np.sum(one_patch_df['OBSERVED']) / len(one_patch_df) :.1%} of the targets are observed\")\n",
    "        #one_patch_df[f'OBSERVED_{len(tilelist)}'] = one_patch_df['OBSERVED']\n",
    "        \n",
    "        #print (\"Remove tiles in reverse TILEID order:\")\n",
    "        for i in np.flip(np.arange(0, len(tilelist))):\n",
    "            tileid = tilelist[i]\n",
    "            observed_by_this_tile = sv3_df.loc[row_selector, 'TILEID'] == tileid\n",
    "            #print(f'{np.sum(observed_by_this_tile)} galaxies were observed by tile {tileid} ({i+1}/{len(tilelist)})')\n",
    "            prev = sv3_df.loc[row_selector, f'OBSERVED_{i+1}']\n",
    "            sv3_df.loc[row_selector, f'OBSERVED_{i}'] = np.where(observed_by_this_tile, False, prev)\n",
    "            \n",
    "        #for i in np.flip(np.arange(0, len(tilelist)+1)):\n",
    "        #    if FAINT:\n",
    "        #        totals_observed_faint[i] += np.sum(sv3_df.loc[row_selector, f'OBSERVED_{i}'])\n",
    "        #        totals_all_faint[i] += len(sv3_df.loc[row_selector])\n",
    "        #    else:\n",
    "        #        totals_observed_bright[i] += np.sum(sv3_df.loc[row_selector, f'OBSERVED_{i}'])\n",
    "        #        totals_all_bright[i] += len(sv3_df.loc[row_selector])\n",
    "\n",
    "            #print(f\"{np.sum(one_patch_df[f'OBSERVED_{i}']) / len(one_patch_df) :.1%} of the targets are observed with {i} passes\")\n",
    "                \n",
    "    #for i in range(1, 12):\n",
    "    #    if FAINT:\n",
    "    #        print(f\"{totals_observed_faint[i]:,} ({totals_observed_faint[i] / totals_all_faint[i]:.1%}) faint galaxies are observed with {i} passes\")\n",
    "    #    else: \n",
    "    #        print(f\"{totals_observed_bright[i]:,} ({totals_observed_bright[i] / totals_all_bright[i]:.1%}) bright galaxies are observed with {i} passes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observed_faint = np.zeros(12, dtype=int)\n",
    "observed_bright = np.zeros(12, dtype=int)\n",
    "\n",
    "total_faint = np.sum(sv3_df['APP_MAG_R'] > 19.5)\n",
    "total_bright = np.sum(sv3_df['APP_MAG_R'] < 19.5)\n",
    "\n",
    "for i in range(0, 12):\n",
    "    observed_faint[i] = np.sum(sv3_df.loc[sv3_df['APP_MAG_R'] > 19.5, f'OBSERVED_{i}'])\n",
    "    observed_bright[i] = np.sum(sv3_df.loc[sv3_df['APP_MAG_R'] < 19.5, f'OBSERVED_{i}'])\n",
    "\n",
    "\n",
    "plt.plot(observed_bright / total_bright, color='b', label=\"BGS BRIGHT ME\")\n",
    "plt.plot(observed_faint / total_faint, color='orange', label=\"BGS FAINT ME\")\n",
    "plt.plot([1,2,3,4], [.29, .52, 0.68, .81], '--', color='b', label=\"BGS BRIGHT PAPER\")\n",
    "plt.plot([1,2,3,4], [.15, .32, 0.47, .62], '--', color='orange', label=\"BGS FAINT PAPER\")\n",
    "plt.xlabel(\"Number of passes\")\n",
    "plt.ylabel(\"Fraction of targets observed\")\n",
    "plt.title(\"SV3 BGS Completeness\")\n",
    "plt.xticks(np.arange(0, 12))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sv3_df.loc[sv3_df['APP_MAG_R'] < 19.5, 'OBSERVED'].count() - sv3_df.loc[sv3_df['APP_MAG_R'] < 19.5, 'OBSERVED'].sum())\n",
    "print(sv3_df.loc[sv3_df['APP_MAG_R'] < 19.5, 'OBSERVED'].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above numbers do not seem to track with the Y1 data. In Y1 no region has more than 4 passes so how do I have a fiber incompleteness better than the above number?\n",
    "\n",
    "Also Figure 17 of https://iopscience.iop.org/article/10.3847/1538-3881/accff8/pdf disagrees with my above analysis. So what is above is wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nearest Neighbor Analysis (for SV3 Merged File)\n",
    "\n",
    "Have a nearest neighbor catalog that is all the actually observed galaxies in SV3 with my Y3 supplement. \n",
    "\n",
    "Can run analysis on entire sample or a subset of 'pretend-to-be unobserved' galaxies (~obs_7p)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create bin files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percentiles for 10 equal bins\n",
    "percentiles = np.linspace(0, 100, 16)\n",
    "bin_edges = np.percentile(z_obs, percentiles)\n",
    "\n",
    "with np.printoptions(precision=3, linewidth=200):\n",
    "    print(f\"Even number z bin: {np.array(bin_edges[1:])}\")\n",
    "    print(f\"The Z Bins we use: {Z_BINS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAKE SURE that above KEEP_PASSES is set to 1 and you filter out unobserved galaxies as we need a source of 'truth'\n",
    "assert KEEP_PASSES == 1 and np.sum(unobserved) == 0\n",
    "assert Z_MIN == 0.001 and Z_MAX == 0.5\n",
    "\n",
    "gmr = app_mag_g - app_mag_r\n",
    "Rk = app_mag_to_abs_mag_k(app_mag_r, z_obs, gmr, band='r')\n",
    "Gk = app_mag_to_abs_mag_k(app_mag_g, z_obs, gmr, band='g')\n",
    "quiescent = is_quiescent_BGS_gmr(None, Gk-Rk)\n",
    "\n",
    "# We will only consider the observed galaxies in SV3 as we need a source of truth for the analysis.\n",
    "# This is ~98% so this analysis should be representative.\n",
    "# As usual for SV3, only want galaxies that were fully covered by the rosetting pattern. (in_10p_zone)\n",
    "# We then pretend to have not observed ~20% of galaxies, as is the case in the main survey. (obs_7p)\n",
    "# TODO BUG Ashley says my method is wrong for doing this.\n",
    "in_10p_zone = ntiles_mine >= 10\n",
    "obs_7p = ~gc.drop_SV3_passes(3, tile_id, unobserved)\n",
    "\n",
    "print(np.sum(quiescent))\n",
    "print(np.sum(in_10p_zone))\n",
    "print(np.sum(obs_7p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_id.mask.sum() # BUG prevents using LOST_GALAXIES_ONLY=True. Is it because of Y3 supplement has no TILE_ID?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "### Create NEIGHBOR_ANALYSIS_SV3_BINS_FILE\n",
    "###\n",
    "p_obs = np.where(np.isnan(p_obs), 0.689, p_obs) # Fill in missing p_obs with 0.689, the Y3 mean\n",
    "newobj = NNAnalyzer_cic.from_data(dec, ra, z_obs, app_mag_r, Rk, g_r_apparent, quiescent, obs_7p, p_obs)\n",
    "#newobj.set_row_locator( np.logical_and(obs_10p, app_mag_r < 19.5) ) # 10p inner regions and BRIGHT only\n",
    "newobj.set_row_locator(in_10p_zone)\n",
    "newobj.find_nn_properties(LOST_GALAXIES_ONLY=False) \n",
    "newobj.make_bins()\n",
    "newobj.save(NEIGHBOR_ANALYSIS_SV3_BINS_FILE)\n",
    "\n",
    "print(np.sum(newobj.all_ang_bincounts))\n",
    "print(np.sum(newobj.all_sim_z_bincounts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian smoothing version\n",
    "loaded_nna = NNAnalyzer_cic.from_results_file(NEIGHBOR_ANALYSIS_SV3_BINS_FILE)\n",
    "\n",
    "# Apply Gaussian filter\n",
    "loaded_nna.apply_gaussian_smoothing(0.75)\n",
    "\n",
    "# Save extra smoothed one\n",
    "loaded_nna.save(NEIGHBOR_ANALYSIS_SV3_BINS_SMOOTHED_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at bin results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_nna = NNAnalyzer_cic.from_results_file(NEIGHBOR_ANALYSIS_SV3_BINS_FILE)\n",
    "smoothed_nna = NNAnalyzer_cic.from_results_file(NEIGHBOR_ANALYSIS_SV3_BINS_SMOOTHED_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_nna.plot_angdist_appmag_per_zbin_cc(z_bin_numbers_to_plot=[2,4,6,8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothed_nna.plot_angdist_appmag_per_zbin_cc(z_bin_numbers_to_plot=[2,4,6,8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These look OK \n",
    "orig_nna.plot_angdist_appmag_per_zbin_cc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(smoothed_nna.all_ang_bincounts))\n",
    "print(np.prod(np.shape(smoothed_nna.all_ang_bincounts)))\n",
    "print(np.median(smoothed_nna.all_ang_bincounts))\n",
    "print(np.mean(smoothed_nna.all_ang_bincounts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These make P_obs seem useles...\n",
    "smoothed_nna.plot_angdist_pobs_per_zbin_cc(t_c=[1], nn_c=[0], z_bin_numbers_to_plot=[1,3,5,7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothed_nna.plot_angdist_pobs_per_zbin_cc(t_c=[1], nn_c=[1], z_bin_numbers_to_plot=[1,4,7,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO de-dupe with pre_process_BGS(...)\n",
    "\n",
    "# Examine photo-z NN relation\n",
    "NUM_NEIGHBORS = 30\n",
    "unobs_7p = ~obs_7p\n",
    "bright = app_mag_r[in_10p_zone] < 19.5\n",
    "use = bright & unobs_7p\n",
    "neighbor_indexes = np.zeros(shape=(NUM_NEIGHBORS, use.sum()), dtype=np.int32) # indexes point to CATALOG locations\n",
    "ang_distances = np.zeros(shape=(NUM_NEIGHBORS, use.sum()))\n",
    "\n",
    "catalog = coord.SkyCoord(ra=ra[in_10p_zone][obs_7p]*u.degree, dec=dec[in_10p_zone][obs_7p]*u.degree, frame='icrs')\n",
    "\n",
    "print(f\"Finding nearest {NUM_NEIGHBORS} neighbors... \", end='\\r')   \n",
    "for n in range(0, NUM_NEIGHBORS):\n",
    "    to_match = coord.SkyCoord(ra=ra[in_10p_zone][use]*u.degree, dec=dec[in_10p_zone][use]*u.degree, frame='icrs')\n",
    "    idx, d2d, d3d = coord.match_coordinates_sky(to_match, catalog, nthneighbor=n+1, storekdtree='sv3')\n",
    "    neighbor_indexes[n] = idx\n",
    "    ang_distances[n] = d2d.to(u.arcsec).value\n",
    "print(f\"Finding nearest {NUM_NEIGHBORS} neighbors... done!\")   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ztruth = z_obs[in_10p_zone][use]\n",
    "# fill zphot_fake with ztruth + a random gaussian draw around 0.0 with sigma of 0.01\n",
    "zphot = z_phot[in_10p_zone][use]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_pass_df.Z_ASSIGNED_FLAG = ~obs_7p\n",
    "plot_positions(one_pass_df, tiles_df=None, DEG_LONG=2.5, split=True, ra_min=one_pass_df.RA[0], dec_min=one_pass_df['DEC'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test of how gaussian filter works\n",
    "\n",
    "# Create a sample 2D array with zeros and a few ones\n",
    "array = np.zeros((30, 30))\n",
    "array[0, 0] = 1\n",
    "array[1, 0] = 1\n",
    "array[5, 5] = 1\n",
    "array[8, 5] = 1\n",
    "array[20, 20] = 3\n",
    "\n",
    "# Apply Gaussian filter\n",
    "sigma = 0.75  # Standard deviation for Gaussian kernel\n",
    "smoothed_array = gaussian_filter(array, sigma=sigma, axes=[0,1])\n",
    "\n",
    "assert np.isclose(np.sum(array), np.sum(smoothed_array))\n",
    "\n",
    "# Plot the original and smoothed arrays\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "ax[0].imshow(array, cmap='gray')\n",
    "ax[0].set_title('Original Array')\n",
    "ax[1].imshow(smoothed_array, cmap='gray')\n",
    "ax[1].set_title('Smoothed Array')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial look at Photo-z only matching to neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramspace = np.arange(0.001, 0.05, 0.001)\n",
    "                       \n",
    "cumulative_percent_correct_by_n_zp_firstonly = np.zeros((NUM_NEIGHBORS, len(paramspace)) , dtype=float)\n",
    "\n",
    "i = 0\n",
    "for PHOTOZ_MATCHING_THRESHOLD in paramspace:\n",
    "    \n",
    "    this_neighbor_correct = np.zeros(shape=(NUM_NEIGHBORS, len(ztruth)), dtype=bool)\n",
    "    z_phot_neighbor_match = np.zeros(shape=(NUM_NEIGHBORS, len(ztruth)), dtype=bool)\n",
    "    cumulative_percent_z_phot_neighbor_match = []\n",
    "    z_phot_first_neighbor_match_idx = np.ones(len(ztruth), dtype=int) * 999 # sentinal value for no match\n",
    "    z_phot_match_correct = np.zeros(shape=(NUM_NEIGHBORS, len(ztruth)), dtype=bool)\n",
    "    delta_z = np.zeros(shape=(NUM_NEIGHBORS, len(ztruth)), dtype=float)\n",
    "\n",
    "    percent_correct_by_n = []\n",
    "    percent_correct_by_n_zp = []\n",
    "    cumulative_percent_correct_by_n = []\n",
    "    cumulative_percent_correct_by_n_zp = []\n",
    "    cumulative_percent_correct_by_n_zp_closestonly = []\n",
    "    first_matched_but_incorrect = []\n",
    "\n",
    "    for n in range(0, NUM_NEIGHBORS):\n",
    "\n",
    "        z_neighbor = z_obs[in_10p_zone][obs_7p][neighbor_indexes[n]]\n",
    "        this_neighbor_correct[n] = close_enough(ztruth, z_neighbor)\n",
    "        percent_correct_by_n.append(this_neighbor_correct[n].sum() / len(ztruth))\n",
    "        any_neighbor_correct = this_neighbor_correct[0:n+1].max(axis=0) # max will be True if any neighbor is True\n",
    "        cumulative_percent_correct_by_n.append(any_neighbor_correct.sum() / len(ztruth))\n",
    "\n",
    "        z_phot_neighbor_match[n] = close_enough(zphot, z_neighbor, threshold=PHOTOZ_MATCHING_THRESHOLD)\n",
    "        any_z_phot_match = z_phot_neighbor_match[0:n+1].max(axis=0) # will be True if z_phot matches a neighbor\n",
    "        cumulative_percent_z_phot_neighbor_match.append(any_z_phot_match.sum() / len(ztruth))\n",
    "        z_phot_match_correct[n] = z_phot_neighbor_match[n] & this_neighbor_correct[n]\n",
    "        any_neighbor_zp_correct = z_phot_match_correct[0:n+1].max(axis=0) # will be True if z_phot matches a neighbor\n",
    "        percent_correct_by_n_zp.append(z_phot_match_correct[n].sum() / len(ztruth))\n",
    "        cumulative_percent_correct_by_n_zp.append(any_neighbor_zp_correct.sum() / len(ztruth))\n",
    "\n",
    "\n",
    "        # Now only consider the closest neighbor, by photo-z matching\n",
    "        delta_z[n] = zphot - z_neighbor\n",
    "        best_match_idx = np.argmin(delta_z[0:n+1], axis=0, keepdims=True)\n",
    "        closest_delta_z_correct = np.take_along_axis(z_phot_match_correct, best_match_idx, axis=0)[0] # will be True bset zphot match is correct\n",
    "        cumulative_percent_correct_by_n_zp_closestonly.append(closest_delta_z_correct.sum() / len(ztruth))\n",
    "\n",
    "        # Now only consider the first neighbor in order of angular distance\n",
    "        z_phot_first_neighbor_match_idx = np.minimum(z_phot_first_neighbor_match_idx, np.where(z_phot_neighbor_match[n],n,999))\n",
    "        has_match = z_phot_first_neighbor_match_idx != 999\n",
    "\n",
    "        # if z_phot_first_neighbor_match_idx is 999 for  row, first_correct will be False\n",
    "        # if z_phot_first_neighbor_match_idx is an index for row, first_correct will be z_phot_match_correct value for that row at that index\n",
    "        first_correct = np.repeat(False, len(ztruth))\n",
    "        first_correct[has_match] = z_phot_match_correct[z_phot_first_neighbor_match_idx[has_match], np.arange(len(ztruth))[has_match]]\n",
    "        cumulative_percent_correct_by_n_zp_firstonly[n,i] = (first_correct.sum() / len(ztruth))\n",
    "\n",
    "        first_matched_but_incorrect.append(cumulative_percent_z_phot_neighbor_match[n] - cumulative_percent_correct_by_n_zp_firstonly[n,i])\n",
    "\n",
    "    if i == 26:\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(range(1, NUM_NEIGHBORS+1), percent_correct_by_n, label=\"This Neighbor is correct z\")\n",
    "        plt.plot(range(1, NUM_NEIGHBORS+1), percent_correct_by_n_zp, label=\"This Neighbor photo-z matched and correct (thresh={PHOTOZ_MATCHING_THRESHOLD:.3})\")\n",
    "        plt.plot(range(1, NUM_NEIGHBORS+1), cumulative_percent_correct_by_n, label=\"Any Neighbor has correct z\")\n",
    "        plt.plot(range(1, NUM_NEIGHBORS+1), cumulative_percent_correct_by_n_zp, label=\"Any neighbor photo-z matched and correct (thresh={PHOTOZ_MATCHING_THRESHOLD:.3})\")\n",
    "        plt.plot(range(1, NUM_NEIGHBORS+1), cumulative_percent_correct_by_n_zp_closestonly, label=\"Minimum delta-z matched neighbor correct (thresh={PHOTOZ_MATCHING_THRESHOLD:.3})\")\n",
    "        plt.plot(range(1, NUM_NEIGHBORS+1), cumulative_percent_correct_by_n_zp_firstonly[:,i], label=f\"First photo-z matched neighbor correct (thresh={PHOTOZ_MATCHING_THRESHOLD:.3})\")\n",
    "        plt.plot(range(1, NUM_NEIGHBORS+1), cumulative_percent_z_phot_neighbor_match, label=\"Any Neighbor photo-z matched (thresh={PHOTOZ_MATCHING_THRESHOLD:.3})\")\n",
    "        plt.ylabel(\"Fraction\")\n",
    "        plt.xlabel(\"Nth Nearest Neighbor\")\n",
    "        plt.xticks(np.arange(1, NUM_NEIGHBORS+1))\n",
    "        plt.ylim(0, 0.8)\n",
    "        #plt.axhline(y=0.0125, color='r', linestyle='--', label=\"Random Chance\")\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.figure(figsize=(10, 5))\n",
    "        # stacked bar chart of \n",
    "        plt.bar(range(1, NUM_NEIGHBORS+1), cumulative_percent_correct_by_n_zp_firstonly[:,i], label=f\"First photo-z matched neighbor correct (thresh={PHOTOZ_MATCHING_THRESHOLD:.3})\")\n",
    "        plt.bar(range(1, NUM_NEIGHBORS+1), first_matched_but_incorrect, label=f\"First photo-z matched neighbor incorrect (thresh={PHOTOZ_MATCHING_THRESHOLD:.3})\", bottom = cumulative_percent_correct_by_n_zp_firstonly[:,i])\n",
    "\n",
    "        plt.ylabel(\"Fraction\")\n",
    "        plt.xlabel(\"Nth Nearest Neighbor\")\n",
    "        plt.xticks(np.arange(1, NUM_NEIGHBORS+1))\n",
    "        plt.ylim(0, 1.0)\n",
    "        plt.legend()\n",
    "    \n",
    "    i = i + 1\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the maximum point\n",
    "best = np.max(cumulative_percent_correct_by_n_zp_firstonly)\n",
    "best_idx = np.unravel_index(np.argmax(cumulative_percent_correct_by_n_zp_firstonly), cumulative_percent_correct_by_n_zp_firstonly.shape)\n",
    "print(cumulative_percent_correct_by_n_zp_firstonly.shape)\n",
    "print(best_idx)\n",
    "print(f\"Best photo-z match threshold: {paramspace[best_idx[1]]:.3f}, Neighbors={best_idx[0]+1}, {best:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, NUM_NEIGHBORS+1), cumulative_percent_correct_by_n_zp_firstonly[:,4],        label=f\"thresh={paramspace[4]:.3})\", color=[0.1, 0.8, 0])\n",
    "plt.plot(range(1, NUM_NEIGHBORS+1), cumulative_percent_correct_by_n_zp_firstonly[:,9],        label=f\"thresh={paramspace[9]:.3})\", color=[0.2, 0.7, 0])\n",
    "plt.plot(range(1, NUM_NEIGHBORS+1), cumulative_percent_correct_by_n_zp_firstonly[:,19],       label=f\"thresh={paramspace[19]:.3})\", color=[0.3, 0.6, 0])\n",
    "plt.plot(range(1, NUM_NEIGHBORS+1), cumulative_percent_correct_by_n_zp_firstonly[:,26], '--', label=f\"thresh={paramspace[26]:.3})\", color=[0.4, 0.5, 0])\n",
    "plt.plot(range(1, NUM_NEIGHBORS+1), cumulative_percent_correct_by_n_zp_firstonly[:,39],       label=f\"thresh={paramspace[39]:.3})\", color=[0.5, 0.4, 0])\n",
    "plt.plot(range(1, NUM_NEIGHBORS+1), cumulative_percent_correct_by_n_zp_firstonly[:,49],       label=f\"thresh={paramspace[49]:.3})\", color=[0.6, 0.3, 0])\n",
    "plt.plot(range(1, NUM_NEIGHBORS+1), cumulative_percent_correct_by_n_zp_firstonly[:,59],       label=f\"thresh={paramspace[59]:.3})\", color=[0.7, 0.2, 0])\n",
    "plt.plot(range(1, NUM_NEIGHBORS+1), cumulative_percent_correct_by_n_zp_firstonly[:,99],       label=f\"thresh={paramspace[99]:.3})\", color=[0.8, 0.1, 0])\n",
    "plt.ylabel(\"Fraction Correct\")\n",
    "plt.xlabel(\"Nth Nearest Neighbor\")\n",
    "plt.xticks(np.arange(1, NUM_NEIGHBORS+1))\n",
    "plt.ylim(0, 0.45)\n",
    "#plt.axhline(y=0.0125, color='r', linestyle='--', label=\"Random Chance\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a figure showing the best photo-z match for each galaxy\n",
    "plt.figure(figsize=(10, 5))\n",
    "for i in range(0, len(paramspace)):\n",
    "    color = [i/(len(paramspace)+1),0.5,0]\n",
    "    plt.plot(range(1, NUM_NEIGHBORS+1), cumulative_percent_correct_by_n_zp_firstonly[:,i], label=f\"Photo-z Match Threshold {paramspace[i]:.3f}\", color=color)\n",
    "plt.ylabel(\"Fraction Correct\")\n",
    "plt.xlabel(\"Nth Nearest Neighbor\")\n",
    "plt.xticks(np.arange(1, NUM_NEIGHBORS+1))\n",
    "plt.ylim(0, 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Color Analysis\n",
    "\n",
    "Lesson from this analysis: the BGS data, workign with my 0.1^G-R with GAMA k-corrections, does not distribute a per logLgal bin G-R; the global 0.76 split seems to work for all bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = app_mag_to_abs_mag(app_mag_g, z_obs)\n",
    "R = app_mag_to_abs_mag(app_mag_r, z_obs)\n",
    "\n",
    "G_R = G - R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It doesn't matter if you use g_r_apparent or G_R as the difference between the is the same!\n",
    "\n",
    "Gk = k_correct_bgs(G, z_obs, g_r_apparent, band='g')\n",
    "Rk = k_correct_bgs(R, z_obs, g_r_apparent, band='r')\n",
    "G_R_k_BGS1 = Gk - Rk\n",
    "\n",
    "Gk_GAMA = k_correct_gama(G, z_obs, g_r_apparent, band='g')\n",
    "Rk_GAMA = k_correct_gama(R, z_obs, g_r_apparent, band='r')\n",
    "G_R_k_GAMA = Gk_GAMA - Rk_GAMA\n",
    "\n",
    "Gk_BGS2 = k_correct_bgs_v2(G, z_obs, g_r_apparent, band='g')\n",
    "Rk_BGS2 = k_correct_bgs_v2(R, z_obs, g_r_apparent, band='r')\n",
    "G_R_k_BGS2 = Gk_BGS2 - Rk_BGS2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison of g-r computed a few ways\n",
    "bins = np.linspace(0, 2.0, 200)\n",
    "\n",
    "plt.figure()\n",
    "#junk=plt.hist(g_r_apparent, bins=bins, label=\"g-r\", histtype='step', density=True)\n",
    "#junk=plt.hist(sdss_g_r, bins=bins, label='From LSS Pipeline (JM?)', histtype='step', density=True)\n",
    "junk=plt.hist(G_R_JM1, bins=bins, label=\"0.1^(G-R) JM\", histtype='step', density=True)\n",
    "#junk=plt.hist(G_R, bins=bins, label=\"G-R\", histtype='step', density=True)\n",
    "junk=plt.hist(G_R_k_BGS1, bins=bins, label=\"0.1^(G-R) BGS poly v1\", histtype='step', density=True)\n",
    "junk=plt.hist(G_R_k_GAMA, bins=bins, label=\"0.1^(G-R) GAMA poly\", histtype='step', density=True)\n",
    "junk=plt.hist(G_R_k_BGS2, bins=bins, label=\"0.1^(G-R) BGS poly v2\", histtype='step', density=True)\n",
    "plt.xlabel(\"g-r\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend()\n",
    "plt.xlim(0.2, 1.3)\n",
    "plt.title(\"Comparison of g-r computed a few ways\")\n",
    "plt.tight_layout()\n",
    "plt.ylim(0,3.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can see global GLOBAL_RED_COLOR_CUT=0.76 here\n",
    "junk=plt.hist(G_R_k_GAMA, bins=300, alpha=0.5, label=\"0.1^(G-R) GAMA-style\")\n",
    "junk=plt.hist(G_R_JM1, bins=300, alpha=0.5, label=\"0.1^(G-R) JM\")\n",
    "plt.legend()\n",
    "plt.xlim(0.5, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(is_quiescent_lost_gal_guess(g_r_apparent).sum() / len(g_r_apparent))\n",
    "assert len(G_R_k_GAMA) == len(g_r_apparent)\n",
    "print(is_quiescent_BGS_gmr(None, G_R_k_GAMA).sum() / len(G_R_k_GAMA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyutils import *\n",
    "print(BGS_LOGLGAL_BINS)\n",
    "print(BINWISE_RED_COLOR_CUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_quiescent_BGS_gmr(np.array([5.8, 9.0, 14.5]), np.array([0.5, 0.9, 0.9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get logLgal bins\n",
    "log_L_gal = abs_mag_r_to_log_solar_L(Rk) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define 8 bins that have equal number of galaxies\n",
    "log_L_gal_bins = np.percentile(log_L_gal, np.linspace(0, 100, 9))\n",
    "print(log_L_gal_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "logLgal_bin_idx = np.digitize(log_L_gal, BGS_LOGLGAL_BINS)\n",
    "# 0 is less than the lowest, len(BGS_LOGLGAL_BINS) is greater than the highest entry in BGS_LOGLGAL_BINS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.min(log_L_gal))\n",
    "print(np.max(log_L_gal))\n",
    "print(np.min(logLgal_bin_idx))\n",
    "print(np.max(logLgal_bin_idx))\n",
    "plt.hist(log_L_gal, bins=BGS_LOGLGAL_BINS, align='mid')\n",
    "#plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a plot of G_R_k in each logLgal bin\n",
    "for i in range(0, len(BGS_LOGLGAL_BINS)+1):\n",
    "    galaxy_idx_for_this_bin = logLgal_bin_idx == i\n",
    "\n",
    "    plt.figure(dpi=80, figsize=(5, 3))\n",
    "    junk=plt.hist(G_R_k_GAMA[galaxy_idx_for_this_bin], bins=np.arange(0,1.3,0.02), label=f\"0.1^(G-R) Bin {i}\", align='mid')\n",
    "    plt.legend()\n",
    "    plt.xlim(0.4, 1.2)\n",
    "    plt.xticks(np.arange(0.4, 1.2, 0.04))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dn4000 Comparison (BGS, SDSS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdss = pd.read_csv(SDSS_v1_DAT_FILE, delimiter=' ', names=('RA', 'DEC', 'Z', 'LOGLGAL', 'VMAX', 'QUIESCENT', 'chi'), index_col=False)\n",
    "sdss_galprops = pd.read_csv(SDSS_v1_1_GALPROPS_FILE, delimiter=' ', names=('MAG_G', 'MAG_R', 'SIGMA_V', 'DN4000', 'CONCENTRATION', 'LOG_M_STAR', 'Z_ASSIGNED_FLAG'))\n",
    "sdss = pd.merge(sdss, sdss_galprops, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(dn4000, bins=np.linspace(-0.5, 5.0, 100), alpha=0.6, label=\"BGS\", density=True, histtype='step')    \n",
    "plt.hist(sdss.DN4000, bins=np.linspace(-0.5, 5.0, 100), alpha=0.8, label=\"SDSS\", density=True, histtype='step')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.xlabel('DN4000')\n",
    "plt.ylabel('Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(dn4000, bins=np.linspace(-0.5, 5.0, 100), alpha=0.6, label=\"BGS\")\n",
    "plt.hist(sdss.DN4000, bins=np.linspace(-0.5, 5.0, 100), alpha=0.8, label=\"SDSS\")\n",
    "plt.legend()\n",
    "plt.xlabel('DN4000')\n",
    "plt.ylabel('Count')\n",
    "plt.xlim(0.9,2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdss_catalog = coord.SkyCoord(ra=sdss.RA.to_numpy()*u.degree, dec=sdss['DEC'].to_numpy()*u.degree, frame='icrs')\n",
    "BGS_catalog = coord.SkyCoord(ra=ra*u.degree, dec=dec*u.degree, frame='icrs')\n",
    "\n",
    "neighbor_indexes, d2d, d3d = coord.match_coordinates_sky(BGS_catalog, sdss_catalog, storekdtree='sdss')\n",
    "ang_distances = d2d.to(u.arcsec).value\n",
    "\n",
    "match_found_filter = np.logical_and(ang_distances < 1.0, ~np.isnan(dn4000))\n",
    "bgs_matches = dn4000[match_found_filter]\n",
    "sdss_indexes = neighbor_indexes[match_found_filter]\n",
    "sdss_matches = sdss.iloc[sdss_indexes].DN4000.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{len(bgs_matches)} matches found (with Dn4000 available)\")\n",
    "print(f\"{np.isclose(bgs_matches, sdss_matches, atol=0.05).sum() / len(bgs_matches)} of the matches are within 0.05 of each other.\")\n",
    "print(f\"{np.isclose(bgs_matches, sdss_matches, atol=0.1).sum() / len(bgs_matches)} of the matches are within 0.1 of each other.\")\n",
    "print(f\"{np.isclose(bgs_matches, sdss_matches, atol=0.2).sum() / len(bgs_matches)} of the matches are within 0.2 of each other.\")\n",
    "print(f\"{np.isclose(bgs_matches, sdss_matches, atol=0.3).sum() / len(bgs_matches)} of the matches are within 0.3 of each other.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "difference = np.array(bgs_matches) - np.array(sdss_matches)\n",
    "\n",
    "# bin by sdss_matches\n",
    "bins = np.linspace(0.8, 2.5, 30)\n",
    "digitized = np.digitize(sdss_matches, bins)\n",
    "bin_means = [difference[digitized == i].mean() for i in range(1, len(bins))]\n",
    "#bin_stds = np.array([difference[digitized == i].std() for i in range(1, len(bins))])\n",
    "bin_counts = [np.sum(digitized == i) for i in range(1, len(bins))]\n",
    "\n",
    "# Calculate 1 sigma error bars\n",
    "bin_intervals1 = [np.percentile(difference[digitized == i], [16, 84]) for i in range(1, len(bins))]\n",
    "bin_lows1 = np.array([interval[0] for interval in bin_intervals1])\n",
    "bin_highs1 = np.array([interval[1] for interval in bin_intervals1])\n",
    "\n",
    "# Calculate 2 sigma error bars\n",
    "bin_intervals = [np.percentile(difference[digitized == i], [2.5, 97.5]) for i in range(1, len(bins))]\n",
    "bin_lows = np.array([interval[0] for interval in bin_intervals])\n",
    "bin_highs = np.array([interval[1] for interval in bin_intervals])\n",
    "\n",
    "# Convert yerr to a format that plt.errorbar can handle\n",
    "yerr_1sigma = np.array([bin_means - bin_lows1, bin_highs1 - bin_means])\n",
    "yerr_2sigma = np.array([bin_means - bin_lows, bin_highs - bin_means])\n",
    "\n",
    "plt.errorbar(bins[1:], bin_means, yerr=yerr_1sigma, fmt='o', label='Mean Difference (1 sigma)', color='k')\n",
    "plt.errorbar(bins[1:], bin_means, yerr=yerr_2sigma, fmt='o', label='Mean Difference (2 sigma)', alpha=0.5)\n",
    "\n",
    "plt.xlabel('SDSS Dn4000')\n",
    "plt.ylabel('BGS - SDSS Dn4000')\n",
    "plt.legend()\n",
    "plt.title(\"Shared Targets: Dn4000 Comparison\")\n",
    "#draw horizontal line at 0.0\n",
    "plt.axhline(0.0, color='r', linestyle='--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare quescient evaluation to SDSS for matched ones\n",
    "#general_match = ang_distances < 1.0\n",
    "bgs_quiescent_matched = quiescent[match_found_filter]\n",
    "print(len(bgs_quiescent_matched))\n",
    "\n",
    "bgs_matches = dn4000[match_found_filter]\n",
    "sdss_indexes_q = neighbor_indexes[match_found_filter]\n",
    "sdss_quiescent_matched = sdss.iloc[sdss_indexes_q]['QUIESCENT'].to_numpy()\n",
    "\n",
    "# Percent that agree on quiescent classification\n",
    "print(f\"{np.sum(bgs_quiescent_matched == sdss_quiescent_matched) / len(bgs_quiescent_matched)} of the matched galaxies agree on quiescent classification using production methods\")\n",
    "\n",
    "bgs_quescient_via_g_r_only = is_quiescent_BGS_gmr(None, G_R_k_GAMA[match_found_filter])\n",
    "\n",
    "print(f\"{np.sum(bgs_quescient_via_g_r_only == sdss_quiescent_matched) / len(bgs_quescient_via_g_r_only)} of the matched galaxies agree on quiescent classification when only using BGS global g-r cut\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dn4000 Lgal Bin Analysis\n",
    "\n",
    "Run Color Analysis and Dn4000 Comparison first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a plot of Dn4000 in each logLgal bin\n",
    "# Show SDSS and my new Dn4000 model quiescent breakpoints\n",
    "fig,axes=plt.subplots(dpi=80, figsize=(18, 8), ncols=4, nrows=2)\n",
    "axes = np.ravel(axes)\n",
    "\n",
    "for i in range(0, len(BGS_LOGLGAL_BINS)-1):\n",
    "    galaxy_idx_for_this_bin = logLgal_bin_idx == i+1\n",
    "\n",
    "    junk=axes[i].hist(dn4000[galaxy_idx_for_this_bin], bins=np.arange(1,2.2,0.02), label=f\"Dn4000 for L Bin {i+1}\", align='mid')\n",
    "    axes[i].legend()\n",
    "    axes[i].set_xlim(1.1, 2.1)\n",
    "    axes[i].set_xticks(np.arange(1.2, 2.1, 0.1))\n",
    "    axes[i].set_xlabel(\"DN4000\")\n",
    "    axes[i].set_ylabel(\"Count\")\n",
    "\n",
    "    # draw a vertical line at get_SDSS_Dcrit(logLgal)\n",
    "    midpoint = (BGS_LOGLGAL_BINS[i] + BGS_LOGLGAL_BINS[i+1]) / 2\n",
    "    axes[i].axvline(x=get_SDSS_Dcrit(midpoint), color='r', linestyle='-', alpha=1.0, linewidth=3)\n",
    "    axes[i].axvline(x=get_ian_Dcrit(midpoint), color='k', linestyle='--', alpha=1.0, linewidth=3)\n",
    "\n",
    "\n",
    "axes = np.reshape(axes, (2, len(BGS_LOGLGAL_BINS)//2))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isnan(dn4000).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dn4000))\n",
    "print(len(logLgal_bin_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to fit a Gaussian Mixture Model and plot the results\n",
    "def fit_gmm_and_plot(dn4000_data, logLgal_bin_idx, num_bins):\n",
    "    def find_intersection(mean1, cov1, mean2, cov2):\n",
    "        # Coefficients of the quadratic equation (a*x^2 + b*x + c = 0)\n",
    "        a = 1 / cov1 - 1 / cov2\n",
    "        b = -2 * (mean1 / cov1 - mean2 / cov2)\n",
    "        c = (mean1**2 / cov1 - mean2**2 / cov2) - 2 * np.log(np.sqrt(cov2 / cov1))\n",
    "        \n",
    "        # Solve the quadratic equation\n",
    "        roots = np.roots([a, b, c])\n",
    "        return roots\n",
    "    \n",
    "    midpoints = []\n",
    "\n",
    "    for i in range(1, num_bins + 1):\n",
    "        galaxy_idx_for_this_bin = logLgal_bin_idx == i\n",
    "        dn4000_bin = dn4000_data[galaxy_idx_for_this_bin]\n",
    "\n",
    "        if len(dn4000_bin) == 0:\n",
    "            continue\n",
    "\n",
    "        # Fit a Gaussian Mixture Model with 2 components\n",
    "        gmm = GaussianMixture(n_components=2, random_state=0)\n",
    "        dn4000_bin_reshaped = dn4000_bin.reshape(-1, 1)\n",
    "        gmm.fit(dn4000_bin_reshaped)\n",
    "\n",
    "        # Get the parameters of the fitted Gaussians\n",
    "        means = gmm.means_.flatten()\n",
    "        covariances = gmm.covariances_.flatten()\n",
    "        weights = gmm.weights_.flatten()\n",
    "\n",
    "        # Find the intersection points\n",
    "        intersections = find_intersection(means[0], covariances[0], means[1], covariances[1])\n",
    "        midpoints.append(np.max(intersections))\n",
    "\n",
    "        # Plot the histogram and the fitted Gaussians\n",
    "        plt.figure(dpi=80, figsize=(14, 6))\n",
    "        plt.hist(dn4000_bin, bins=np.arange(1, 2.2, 0.02), density=True, alpha=0.6, label=f\"Dn4000 for logLgal Bin {i}\")\n",
    "        x = np.linspace(1, 2.2, 1000)\n",
    "        y = (weights[0] * np.exp(-0.5 * ((x - means[0]) ** 2) / covariances[0]) / np.sqrt(2 * np.pi * covariances[0]) +\n",
    "             weights[1] * np.exp(-0.5 * ((x - means[1]) ** 2) / covariances[1]) / np.sqrt(2 * np.pi * covariances[1]))\n",
    "        plt.plot(x, y, label='Gaussian Mixture Model')\n",
    "        for intersection in intersections:\n",
    "            if 1 <= intersection <= 2.2:\n",
    "                plt.axvline(intersection, color='r', linestyle='--', label=f'Intersection at {intersection:.2f}')\n",
    "        plt.legend()\n",
    "        plt.xlabel('Dn4000')\n",
    "        plt.xticks(np.arange(1, 2.2, 0.05))\n",
    "        plt.ylabel('Density')\n",
    "        plt.title(f'Gaussian Mixture Model for logLgal Bin {i}')\n",
    "        plt.show()\n",
    "\n",
    "        print(f\"Bin {i}:\")\n",
    "        print(f\"Means: {means}\")\n",
    "        print(f\"Covariances: {covariances}\")\n",
    "        print(f\"Weights: {weights}\")\n",
    "        print(f\"Intersections: {intersections}\")\n",
    "\n",
    "    return midpoints\n",
    "\n",
    "# Number of logLgal bins\n",
    "num_bins = len(BGS_LOGLGAL_BINS) - 1\n",
    "\n",
    "# Fit the GMM and plot the results\n",
    "good = ~np.isnan(dn4000)\n",
    "midpoints = fit_gmm_and_plot(dn4000[good], logLgal_bin_idx[good], num_bins)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.array(BGS_LOGLGAL_BINS[1:]))\n",
    "print(np.array(BGS_LOGLGAL_BINS[:-1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming logLgal_bin_idx and log_L_gal are already defined\n",
    "logLgal_bin_idx = np.digitize(log_L_gal, BGS_LOGLGAL_BINS)\n",
    "\n",
    "# Calculate the mean value of LOGLGAL in each bin\n",
    "mean_logLgal_per_bin = []\n",
    "for i in range(1, len(BGS_LOGLGAL_BINS)):\n",
    "    bin_values = log_L_gal[logLgal_bin_idx == i]\n",
    "    mean_logLgal_per_bin.append(np.mean(bin_values))\n",
    "\n",
    "# Print the mean values\n",
    "for i, mean_value in enumerate(mean_logLgal_per_bin, 1):\n",
    "    print(f\"Mean LOGLGAL in bin {i}: {mean_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import erf\n",
    "from scipy.optimize import curve_fit\n",
    "import numpy as np\n",
    "\n",
    "values = [1.41675126, 1.46407341, 1.51939392, 1.58815607,1.63857298, 1.67328342, 1.70584468, 1.74753757]\n",
    "\n",
    "# Define the fitting function\n",
    "def fitting_function(logLgal, A, B, C, D):\n",
    "    return A + B*(1 + erf((logLgal - C) / D))\n",
    "\n",
    "priors = [1.42, .175, 9.9, 0.8]\n",
    "\n",
    "\n",
    "# Fit the function to the midpoints\n",
    "popt, pcov = curve_fit(fitting_function, mean_logLgal_per_bin, values, p0=priors, maxfev=10000)\n",
    "\n",
    "print(f\"Fitted parameters: A={popt[0]:.3f}, B={popt[1]:.3f}, C={popt[2]:.3f}, D={popt[3]:.3f}\")\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(dpi=80, figsize=(8, 6))\n",
    "plt.plot(mean_logLgal_per_bin, values, 'o', label='Midpoints')\n",
    "x = np.linspace(8.0, 11.3, 1000)\n",
    "y = fitting_function(x, *popt)\n",
    "#plt.plot(x, y, label='Fitted Function')\n",
    "plt.plot(x, get_SDSS_Dcrit(x), label='SDSS Dcrit')\n",
    "plt.plot(x, get_ian_Dcrit(x), label='New BGS Dcrit')\n",
    "plt.xlabel('logLgal')\n",
    "plt.ylabel('Dn4000 Midpoint')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now create a fitting function for the results with parameters A,B,C,D\n",
    "# A + (B) * (1 + special.erf((logLgal - C) / D))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Randoms Analysis for Footprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_sv3_clustering_randoms_files()\n",
    "build_sv3_full_randoms_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_y3_likesv3_clustering_randoms_files()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_y1_randoms_files()\n",
    "build_y3_randoms_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv3_randoms = pickle.load(open(MY_RANDOMS_SV3_MINI, 'rb'))\n",
    "tiles_BGS = read_tiles_Y3_sv3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the randoms positions\n",
    "fig=make_map(sv3_randoms['RA'].to_numpy(), sv3_randoms['DEC'].to_numpy())\n",
    "#make_map(ra, dec, fig=fig, alpha=0.05)\n",
    "\n",
    "\n",
    "# Some zoom-ins\n",
    "\n",
    "# These are the two regions we cut due to poor Y3 overlap\n",
    "#plot_positions(randoms_df0, randoms_df0[randoms_df0.NTILE_MINE >= 10], tiles_df=tiles_BGS, DEG_LONG=7, ra_min=192, dec_min=23, split=False)\n",
    "\n",
    "plot_positions(sv3_randoms, sv3_randoms[sv3_randoms.NTILE_MINE >= 10], DEG_LONG=5, ra_min=213, dec_min=50, split=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CORRECT FOOTPRINTS\n",
    "# These results copied into the groupcatalog.py file foorprints\n",
    "\n",
    "def frac_sky_from_randoms(randoms: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Provide a single randoms file at 2500 targets / square degrees for this, not a larger combined file.\n",
    "    \"\"\"\n",
    "    RANDOMS_DENSITY = 2500 # per square degree, Ashley Ross paper on LSS pipeline or elsewhere in docs\n",
    "\n",
    "    for i in range(1, 11):\n",
    "        n_pass_filter = randoms.NTILE_MINE >= i\n",
    "        n_pass_footprint = len(randoms[n_pass_filter].RA) / RANDOMS_DENSITY # in degrees squared\n",
    "        n_pass_frac_area = n_pass_footprint / DEGREES_ON_SPHERE\n",
    "        print(f\"BGS {i}pass Footprint calculated from randoms is {n_pass_footprint} square degrees or frac_area={n_pass_frac_area}\")\n",
    "\n",
    "print(\"-=- SV3 18 regions -=-\")\n",
    "frac_sky_from_randoms(pickle.load(open(MY_RANDOMS_SV3_MINI, 'rb')))\n",
    "\n",
    "print(\"-=- SV3 20 regions -=-\")\n",
    "frac_sky_from_randoms(pickle.load(open(MY_RANDOMS_SV3_MINI_20, 'rb')))\n",
    "\n",
    "print(\"-=- Y1 -=-\")\n",
    "frac_sky_from_randoms(pickle.load(open(MY_RANDOMS_Y1_MINI, 'rb')))\n",
    "\n",
    "print(\"-=- Y3 -=-\")\n",
    "frac_sky_from_randoms(pickle.load(open(MY_RANDOMS_Y3_MINI, 'rb')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
