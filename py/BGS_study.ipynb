{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is code for studying the BGS data from the LSS Catalogs prior to running group finding. There are blocks to create the \"Merged Files\" that are inputs for the preprocessing and groupfinding routines in the GroupCatalog class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import astropy.coordinates as coord\n",
    "import astropy.units as u\n",
    "import astropy.io.fits as fits\n",
    "from astropy.table import Table,join,vstack,unique,QTable\n",
    "import sys\n",
    "from urllib.parse import urljoin\n",
    "from scipy.ndimage import gaussian_filter\n",
    "import seaborn as sns\n",
    "from scipy.special import erf\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.spatial import KDTree\n",
    "\n",
    "if './SelfCalGroupFinder/py/' not in sys.path:\n",
    "    sys.path.append('./SelfCalGroupFinder/py/')\n",
    "from pyutils import *\n",
    "from dataloc import *\n",
    "from bgs_helpers import *\n",
    "import groupcatalog as gc\n",
    "from nnanalysis import *\n",
    "from plotting import *\n",
    "from footprintmanager import FootprintManager\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEEP_PASSES = 1\n",
    "APP_MAG_CUT = 19.5 # BGS BRIGHT, though 19.54 for some cameras. I don't know if FLUX_R has been corrected for this.\n",
    "#APP_MAG_CUT = 20.175 # BGS FAINT, and in SV3 it is 20.3 instead\n",
    "Z_MIN = 0.001\n",
    "Z_MAX = 0.5\n",
    "KEEP_ONLY_OBSERVED = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Empty Photo-z Ledger file\n",
    " \n",
    "See photoz.py for the code that populates this file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INITIALIZE LIGHTWEIGHT DESI BGS PHOTO-Z TABLE\n",
    "# Don't re-run! Will overwrite the file.\n",
    "\"\"\"\n",
    "desi_table = Table.read(IAN_BGS_Y3_MERGED_FILE, format='fits')\n",
    "desi_table2 = Table.read(IAN_BGS_SV3_MERGED_NOY3_FILE, format='fits')\n",
    "\n",
    "assert len(np.unique(desi_table['TARGETID'])) == len(desi_table), \"There are duplicate TARGETIDs in the Y3 file\"\n",
    "assert len(np.unique(desi_table2['TARGETID'])) == len(desi_table2), \"There are duplicate TARGETIDs in the SV3 file\"\n",
    "\n",
    "desi_table.keep_columns(['TARGETID', 'RA', 'DEC'])\n",
    "desi_table2.keep_columns(['TARGETID', 'RA', 'DEC'])\n",
    "\n",
    "desi_targets_table = vstack([desi_table, desi_table2], join_type='inner')\n",
    "desi_targets_table = unique(desi_targets_table, 'TARGETID')\n",
    "desi_targets_table['Z_LEGACY_BEST'] = NO_PHOTO_Z\n",
    "\n",
    "# add columns for 'RELEASE', 'BRICKID', 'OBJID', 'REF_CAT', 'MATCH_DIST' with no values\n",
    "desi_targets_table.add_column(np.zeros(len(desi_targets_table), dtype=int), name='RELEASE')\n",
    "desi_targets_table.add_column(np.zeros(len(desi_targets_table), dtype=int), name='BRICKID')\n",
    "desi_targets_table.add_column(np.zeros(len(desi_targets_table), dtype=int), name='OBJID')\n",
    "desi_targets_table.add_column(np.zeros(len(desi_targets_table), dtype='S2'), name='REF_CAT')\n",
    "desi_targets_table.add_column(np.full(len(desi_targets_table), 999999, dtype=float), name='MATCH_DIST')\n",
    "\n",
    "\n",
    "desi_targets_table = desi_targets_table.to_pandas()\n",
    "desi_targets_table.set_index('TARGETID', inplace=True)\n",
    "pickle.dump(desi_targets_table, open(IAN_PHOT_Z_FILE, 'wb'))\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read BGS_IMAGES_FOLDER + \"terminal.txt\" into an array of strings, 1 per line\n",
    "f = open(BGS_IMAGES_FOLDER + \"terminal.txt\", \"r\")\n",
    "lines = f.readlines()\n",
    "f.close()\n",
    "\n",
    "# Now filter it down to ones like this:\n",
    "#Start processing brick #X\n",
    "#Matched 0 out of Z\n",
    "#start_lines = [line for line in lines if \"Start processing brick\" in line]\n",
    "matched_lines = [line for line in lines if \"Matched\" in line]\n",
    "\n",
    "#print(len(start_lines))\n",
    "print(len(matched_lines))\n",
    "\n",
    "# Extract X, Y, Z from each line \n",
    "#start_lines = [line.split()[3] for line in start_lines]    \n",
    "matched_lines = [int(line.split()[1]) for line in matched_lines]\n",
    "\n",
    "# Strip away # and convert to int\n",
    "#start_lines = [int(line[1:]) for line in start_lines]\n",
    "\n",
    "# The index is the brick number for matched_lines\n",
    "# Find the brick numbers where the value is 0 and remember those indexes\n",
    "zero_indexes = [i for i in range(len(matched_lines)) if matched_lines[i] == 0]\n",
    "\n",
    "#pickle.dump(zero_indexes, open(BRICKS_TO_SKIP_S_FILE, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle.dump([], open(BRICKS_TO_SKIP_N_FILE, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build other support files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_sv3_clustering_randoms_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_sv3_full_randoms_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a file with just TARGETID and QUIESCENT columns to send to Joe for Clustering\n",
    "tbl = Table.read(IAN_BGS_Y3_MERGED_FILE_LOA, format='fits') \n",
    "tbl.keep_columns(['TARGETID', 'QUIESCENT'])\n",
    "tbl.write(BGS_Y3_FOLDER_LOA + \"COLOR_LOOKUP_LOA.fits\", format='fits', overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine data in a Merged BGS File (SV3, Y1, Y3, whatever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick one\n",
    "#table = Table.read(IAN_BGS_SV3_MERGED_FILE, format='fits')\n",
    "#table = Table.read(IAN_BGS_SV3_MERGED_NOY3_FILE, format='fits')\n",
    "table = Table.read(IAN_BGS_Y1_MERGED_FILE, format='fits')\n",
    "#table2 = Table.read(IAN_BGS_Y3_MERGED_FILE_LOA, format='fits')\n",
    "# Randomly downsample table2 to 1000 rows for testing\n",
    "#table2 = table2[np.random.choice(len(table2), size=100000, replace=False)]\n",
    "\n",
    "#table = Table.read(IAN_BGS_SV3_MERGED_FULL_FILE, format='fits')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feather experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import pyarrow.feather as feather\n",
    "names = [name for name in table.colnames if len(table[name].shape) <= 1]\n",
    "\n",
    "\n",
    "# Convert Astropy Table to pandas DataFrame if not already\n",
    "start = time.time()\n",
    "if not isinstance(table, pd.DataFrame):\n",
    "    df_arrow = table[names].to_pandas()\n",
    "else:\n",
    "    df_arrow = table\n",
    "end = time.time()\n",
    "print(f\"Conversion to pandas DataFrame time: {end - start:.4f} seconds)\n",
    "\n",
    "# Write to feather file\n",
    "start = time.time()\n",
    "feather_file = \"table_perf_test.feather\"\n",
    "df_arrow.reset_index(drop=True, inplace=True)\n",
    "feather.write_feather(df_arrow, feather_file)\n",
    "end = time.time()\n",
    "print(f\"Feather write time: {end - start:.4f} seconds\")\n",
    "\n",
    "# Perf test: read the feather file\n",
    "start = time.time()\n",
    "df_read = feather.read_feather(feather_file)\n",
    "end = time.time()\n",
    "print(f\"Feather read time: {end - start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "table_orig = Table.read(IAN_BGS_Y1_MERGED_FILE, format='fits')\n",
    "end = time.time()\n",
    "print(f\"Original FITS read time: {end - start:.4f} seconds\")\n",
    "print(f\"Rows read: {len(table_orig)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cut to the galaxy data we actually need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO this gets easilly out of sync with the .py file that does the 'production' filtering\n",
    "z_obs = get_tbl_column(table, 'Z', required=True)\n",
    "obj_type = get_tbl_column(table, 'SPECTYPE', required=True)\n",
    "unobserved = determine_unobserved_from_z(z_obs)\n",
    "deltachi2 = get_tbl_column(table, 'DELTACHI2', required=True)\n",
    "maskbits = get_tbl_column(table, 'MASKBITS')\n",
    "dec = table['DEC'].astype(\"<f8\")\n",
    "ra = table['RA'].astype(\"<f8\")\n",
    "z_phot = table['Z_PHOT'].astype(\"<f8\")\n",
    "target_id = table['TARGETID']\n",
    "app_mag_r = table['APP_MAG_R']\n",
    "app_mag_g = table['APP_MAG_G']\n",
    "g_r_apparent = app_mag_g - app_mag_r\n",
    "abs_mag_r = table['ABS_MAG_R']\n",
    "abs_mag_g = table['ABS_MAG_G']\n",
    "abs_mag_r_fsf = table['ABSMAG01_SDSS_R']\n",
    "abs_mag_g_fsf = table['ABSMAG01_SDSS_G']\n",
    "#sdss_g_r = table['ABSMAG_SDSS_G'] - table['ABSMAG_SDSS_R'] \n",
    "G_R_JM1 = table['ABSMAG01_SDSS_G'] - table['ABSMAG01_SDSS_R']\n",
    "p_obs = table['PROB_OBS'] \n",
    "ntiles = table['NTILE'].astype(\"<i8\")\n",
    "#tiles = table['TILES']\n",
    "tile_id = table['TILEID'] if 'TILEID' in table.columns else None\n",
    "#numobs = table['NUMOBS']\n",
    "#tile_locid = table['TILELOCID']\n",
    "ntiles_mine = table['NTILE_MINE']\n",
    "tileids = table['NEAREST_TILEIDS'][:,0].astype(\"<i8\") # TODO there are 10 here, we want NTILES_MINE many...\n",
    "#abs_mag_sdss = table['ABSMAG_SDSS_R']\n",
    "dn4000 = get_tbl_column(table, 'DN4000_MODEL')\n",
    "ref_cat = table['REF_CAT']\n",
    "quiescent = table['QUIESCENT']\n",
    "photsys = get_tbl_column(table, 'PHOTSYS', required=True)\n",
    "\n",
    "\n",
    "before_count = len(dec)\n",
    "print(before_count, \"objects in FITS file\")\n",
    "\n",
    "# TODO BUG Can we be mistaking STARS for GALAXIES?\n",
    "# Make filter array (True/False values)\n",
    "PASSES_REQUIRED = [1,2,3,4,10]\n",
    "\n",
    "galaxy_observed_filter = obj_type == b'GALAXY'\n",
    "app_mag_filter = np.where(photsys == b'N', app_mag_r < APP_MAG_CUT+0.04, app_mag_r < APP_MAG_CUT)\n",
    "redshift_filter = z_obs > Z_MIN\n",
    "redshift_hi_filter = z_obs < Z_MAX\n",
    "deltachi2_filter = deltachi2 > 40\n",
    "#abs_mag_sdss_filter = abs_mag_sdss < 100\n",
    "#observed_requirements = np.all([galaxy_observed_filter, app_mag_filter, redshift_filter, redshift_hi_filter, deltachi2_filter, abs_mag_sdss_filter], axis=0)\n",
    "observed_requirements = np.all([galaxy_observed_filter, app_mag_filter, redshift_filter, redshift_hi_filter, deltachi2_filter], axis=0)\n",
    "\n",
    "treat_as_unobserved = np.all([galaxy_observed_filter, app_mag_filter, np.invert(deltachi2_filter)], axis=0)\n",
    "\n",
    "unobserved = np.all([app_mag_filter, np.logical_or(unobserved, treat_as_unobserved)], axis=0)\n",
    "keep = np.all([np.logical_or(observed_requirements, unobserved)], axis=0)\n",
    "\n",
    "print(\"\\nWhole sample:\")\n",
    "print(f\"There are {len(obj_type):,} objects in the entire sample, of which {np.sum(galaxy_observed_filter):,} are observed galaxies.\") \n",
    "\n",
    "for n in PASSES_REQUIRED:\n",
    "    n_pass_filter = ntiles_mine >= n\n",
    "    n_pass_filter_old = ntiles >= n\n",
    "    unobserved_n = np.all([n_pass_filter, unobserved], axis=0)\n",
    "    observed_requirements_n = np.all([n_pass_filter, observed_requirements], axis=0)\n",
    "    keepn = np.all([np.logical_or(observed_requirements_n, unobserved_n)], axis=0)\n",
    "\n",
    "    print(f\"\\n{n}-pass analysis (NTILE_MINE):\")\n",
    "    print(f\"There are {np.sum(observed_requirements_n):,} galaxies in the <{APP_MAG_CUT} mag sample that pass our quality checks.\")\n",
    "    print(f\"There are {np.sum(unobserved_n):,} unobserved galaxies, including bad observed galaxies.\")\n",
    "    print(f\"This {n}-pass catalog would have {np.sum(keepn):,} galaxies ({np.sum(unobserved_n) / np.sum(keepn) * 100:.2f}% lost).\")\n",
    "\n",
    "    # We've demonstratred this is definetely not what we want\n",
    "    #unobserved_n_old = np.all([n_pass_filter_old, unobserved], axis=0)\n",
    "    #observed_requirements_n_old = np.all([n_pass_filter_old, observed_requirements], axis=0)\n",
    "    #keepn_old = np.all([np.logical_or(observed_requirements_n_old, unobserved_n_old)], axis=0)\n",
    "    #print(f\"\\n{n}-pass analysis (NTILE):\")\n",
    "    #print(f\"There are {np.sum(observed_requirements_n_old):,} galaxies in the bright (<{APP_MAG_CUT} mag) sample that pass our quality checks.\")\n",
    "    #print(f\"There are {np.sum(unobserved_n_old):,} unobserved galaxies, including bad observed galaxies.\")\n",
    "    #print(f\"This {n}-pass catalog would have {np.sum(keepn_old):,} galaxies ({np.sum(unobserved_n_old) / np.sum(keepn_old) * 100:.2f}% lost).\")\n",
    "\n",
    "if KEEP_ONLY_OBSERVED:\n",
    "    keep = np.all([keep, ntiles_mine >= KEEP_PASSES, ~unobserved], axis=0)\n",
    "else:\n",
    "    keep = np.all([keep, ntiles_mine >= KEEP_PASSES], axis=0)\n",
    "\n",
    "obj_type = obj_type[keep]\n",
    "dec = dec[keep]\n",
    "ra = ra[keep]\n",
    "z_phot = z_phot[keep]\n",
    "z_obs = z_obs[keep]\n",
    "target_id = target_id[keep] \n",
    "app_mag_r = app_mag_r[keep]\n",
    "app_mag_g = app_mag_g[keep]\n",
    "g_r_apparent = g_r_apparent[keep]\n",
    "p_obs = p_obs[keep]\n",
    "unobserved = unobserved[keep]\n",
    "deltachi2 = deltachi2[keep]\n",
    "ntiles = ntiles[keep]\n",
    "abs_mag_r = abs_mag_r[keep]\n",
    "abs_mag_g = abs_mag_g[keep]\n",
    "abs_mag_r_fsf = abs_mag_r_fsf[keep]\n",
    "abs_mag_g_fsf = abs_mag_g_fsf[keep]\n",
    "#tiles = tiles[keep]\n",
    "#ztileid = ztileid[keep]\n",
    "ntiles_mine = ntiles_mine[keep]\n",
    "tileids = tileids[keep]\n",
    "tile_id = tile_id[keep] if 'TILEID' in table.columns else None\n",
    "#numobs = numobs[keep]\n",
    "#tile_locid = tile_locid[keep]\n",
    "#abs_mag_sdss = abs_mag_sdss[keep]\n",
    "#sdss_g_r = sdss_g_r[keep]\n",
    "G_R_JM1 = G_R_JM1[keep]\n",
    "dn4000 = dn4000[keep]\n",
    "ref_cat = ref_cat[keep]\n",
    "maskbits = maskbits[keep]\n",
    "indexes_not_assigned = np.flatnonzero(unobserved)\n",
    "quiescent = quiescent[keep]\n",
    "\n",
    "after_count = len(dec)\n",
    "\n",
    "print(f\"\\nAfter all filters we have {after_count:,} of the original {before_count:,} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_to_df(table):\n",
    "    # This reproduces the above.\n",
    "    names = [name for name in table.colnames if len(table[name].shape) <= 1] \n",
    "    df = table[names].to_pandas()\n",
    "    df = df[np.where(df['PHOTSYS'] == b'N', df['APP_MAG_R'] < APP_MAG_CUT+0.04, df['APP_MAG_R'] < APP_MAG_CUT)]\n",
    "    df['UNOBSERVED'] = determine_unobserved_from_z(df['Z'])\n",
    "\n",
    "    galaxy_observed_filter = df['SPECTYPE'] == b'GALAXY'\n",
    "    deltachi2_filter = df['DELTACHI2'] > 40\n",
    "    observed_requirements = galaxy_observed_filter & (df['Z'] > Z_MIN) & (df['Z'] < Z_MAX) & deltachi2_filter\n",
    "    treat_as_unobserved = np.all([galaxy_observed_filter, np.invert(deltachi2_filter)], axis=0)\n",
    "\n",
    "    df['UNOBSERVED'] = df['UNOBSERVED'] | treat_as_unobserved\n",
    "    keep = np.all([np.logical_or(observed_requirements, df['UNOBSERVED'])], axis=0)\n",
    "\n",
    "    if KEEP_ONLY_OBSERVED:\n",
    "        keep = np.all([keep, df['NTILE_MINE'] >= KEEP_PASSES, ~df['UNOBSERVED']], axis=0)\n",
    "    else:\n",
    "        keep = np.all([keep, df['NTILE_MINE'] >= KEEP_PASSES], axis=0)\n",
    "\n",
    "    df = df[keep]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df2 = table_to_df(table2)\n",
    "df = table_to_df(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del table\n",
    "del table2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows in df2 that are in df2 (based on TARGETID)\n",
    "df2 = df2[~df2['TARGETID'].isin(df['TARGETID'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test that the overall target density in photsys N vs S is within 1% of each other\n",
    "mgr = FootprintManager()\n",
    "Ndeg = mgr.get_footprint('Y1', 'N', KEEP_PASSES)\n",
    "Sdeg = mgr.get_footprint('Y1', 'S', KEEP_PASSES)\n",
    "print(f\"Y1 N Footprint: {Ndeg} deg^2, S Footprint: {Sdeg} deg^2\")\n",
    "n_density = np.sum(df['PHOTSYS'] == b\"N\") / Ndeg\n",
    "s_density = np.sum(df['PHOTSYS'] == b\"S\") / Sdeg\n",
    "print(f\"Target density in N: {n_density:.4f} targets/deg^2\")\n",
    "print(f\"Target density in S: {s_density:.4f} targets/deg^2\")\n",
    "if np.abs(n_density - s_density) / ((n_density + s_density) / 2) > 0.01:\n",
    "    print(f\"WARNING: Target density in N and S is more than 1% different.\")\n",
    "\n",
    "Ndeg = mgr.get_footprint('Y3', 'N', KEEP_PASSES)\n",
    "Sdeg = mgr.get_footprint('Y3', 'S', KEEP_PASSES)\n",
    "print(f\"Y3 N Footprint: {Ndeg} deg^2, S Footprint: {Sdeg} deg^2\")\n",
    "n_density = np.sum(df2['PHOTSYS'] == b\"N\") / Ndeg\n",
    "s_density = np.sum(df2['PHOTSYS'] == b\"S\") / Sdeg\n",
    "print(f\"Target density in N: {n_density:.4f} targets/deg^2\")\n",
    "print(f\"Target density in S: {s_density:.4f} targets/deg^2\")\n",
    "if np.abs(n_density - s_density) / ((n_density + s_density) / 2) > 0.01:\n",
    "    print(f\"WARNING: Target density in N and S is more than 1% different.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLot the fraction quiescent in mag bins\n",
    "plt.figure(figsize=(10,6))\n",
    "bins = np.linspace(7, 11.6, 40)\n",
    "bin_centers = 0.5 * (bins[1:] + bins[:-1])\n",
    "quiescent_fraction = []\n",
    "for i in range(len(bins)-1):\n",
    "    in_bin = (df['LOG_L_GAL'] >= bins[i]) & (df['LOG_L_GAL'] < bins[i+1])\n",
    "    if np.sum(in_bin) > 0:\n",
    "        frac = np.sum(df['QUIESCENT'][in_bin]) / np.sum(in_bin)\n",
    "    else:\n",
    "        frac = np.nan\n",
    "    quiescent_fraction.append(frac)\n",
    "plt.plot(bin_centers, quiescent_fraction, marker='o', linestyle='-')\n",
    "\n",
    "if df2 is not None:\n",
    "    bin_centers = 0.5 * (bins[1:] + bins[:-1])\n",
    "    quiescent_fraction = []\n",
    "    for i in range(len(bins)-1):\n",
    "        in_bin = (df2['LOG_L_GAL'] >= bins[i]) & (df2['LOG_L_GAL'] < bins[i+1])\n",
    "        if np.sum(in_bin) > 0:\n",
    "            frac = np.sum(df2['QUIESCENT'][in_bin]) / np.sum(in_bin)\n",
    "        else:\n",
    "            frac = np.nan\n",
    "        quiescent_fraction.append(frac)\n",
    "    plt.plot(bin_centers, quiescent_fraction, marker='o', linestyle='-')\n",
    "\n",
    "plt.xlabel('Luminosity in R-band')\n",
    "plt.ylabel('Fraction Quiescent')\n",
    "plt.title('Fraction of Quiescent Galaxies in Magnitude Bins')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make maps\n",
    "three_pass_filter = ntiles_mine >= 3 \n",
    "\n",
    "ra3 = ra[three_pass_filter]\n",
    "dec3 = dec[three_pass_filter]\n",
    "tileids3 =  tileids[three_pass_filter]\n",
    "unobserved3 = unobserved[three_pass_filter]\n",
    "\n",
    "one_pass_df = pd.DataFrame({'RA': ra, 'DEC': dec, 'Z_ASSIGNED_FLAG': unobserved, 'TILEID': tileids})\n",
    "three_pass_df = pd.DataFrame({'RA': ra3, 'DEC': dec3, 'Z_ASSIGNED_FLAG': unobserved3, 'TILEID': tileids3})\n",
    "\n",
    "fig=make_map(ra, dec)\n",
    "fig=make_map(ra3, dec3, fig=fig, alpha=0.1)\n",
    "path = os.path.join(PAPER_PLOT_FOLDER, 'footprint.png')\n",
    "fig.savefig(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiles_BGS = read_tiles_Y1_main()\n",
    "plot_positions(one_pass_df, three_pass_df, tiles_df=tiles_BGS, DEG_LONG=5, split=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See where missing photo-z's are\n",
    "#idx_no_zphot = z_phot == NO_PHOTO_Z\n",
    "#no_photoz_df = pd.DataFrame({'RA': ra[idx_no_zphot], 'DEC': dec[idx_no_zphot], 'Z_ASSIGNED_FLAG': unobserved[idx_no_zphot], 'TILEID': tileids[idx_no_zphot]})\n",
    "#plot_positions(one_pass_df, no_photoz_df, tiles_df=tiles_BGS, DEG_LONG=3, split=False)\n",
    "\n",
    "fig=make_map(ra3, dec3, dpi=300, alpha=0.02)\n",
    "#ra_4 = ra[four_pass_filter]\n",
    "#dec_4 = dec[four_pass_filter]\n",
    "#print(f\"Number of 4-pass galaxies: {len(ra_4)}, number of 3-pass galaxies: {len(ra)}\")\n",
    "#fig=make_map(ra[idx_no_zphot], dec[idx_no_zphot], fig=fig, alpha=0.1)\n",
    "#fig=make_map(ra[ntiles_mine >= 10 ], dec[ntiles_mine >= 10 ], fig=fig, alpha=0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_map(ra, dec, fig=fig, alpha=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(table['ABSMAG01_SDSS_R'], bins=100, range=(-25, -15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Spectroscopic Properties Lookup Table\n",
    "For lost galaxies we want a Dn4000 value and stellar mass. But they have no spectra. So, using their k-corrected g-r color and their absolute magnitude (once a z is assigned to them), we can assigned a random Dn4000 based on similar observed galaxies. This approach helps keep whatever systematics are associated with out quiesence determination similar between lost and observed galaxies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This was run over the usual redshift range, 0.001 < z < 0.5, for Y3 Loa Observed Galaxies to a magnitude of 20.175\n",
    "# Prepare the data for KDTree\n",
    "goodidx = ~np.isnan(df['ABSMAG01_SDSS_R']) & ~np.isnan(df['G_R_BEST']) & ~np.isnan(df['DN4000_MODEL']) & ~np.isnan(df['LOG_L_GAL']) & ~np.isnan(df['LOGMSTAR']) & ~np.isnan(df['ABS_MAG_R'])\n",
    "METRIC_MAG = 1\n",
    "METRIC_GMR = 5\n",
    "\n",
    "print(\"Middle 50% range\")\n",
    "r1 = np.percentile(df.loc[goodidx, 'ABS_MAG_R'], [25, 75])\n",
    "r3 = np.percentile(df.loc[goodidx, 'ABSMAG01_SDSS_R'], [25, 75])\n",
    "r2 = np.percentile(df.loc[goodidx, 'G_R_BEST'], [25, 75])\n",
    "print((r1[1] - r1[0]) * METRIC_MAG)\n",
    "print((r2[1] - r2[0]) * METRIC_GMR)\n",
    "print((r3[1] - r3[0]) * METRIC_MAG)\n",
    "\n",
    "print(\"Middle 80% range\")\n",
    "r1 = np.percentile(df.loc[goodidx, 'ABS_MAG_R'], [10, 90])\n",
    "r3 = np.percentile(df.loc[goodidx, 'ABSMAG01_SDSS_R'], [10, 90])\n",
    "r2 = np.percentile(df.loc[goodidx, 'G_R_BEST'], [10, 90])\n",
    "print((r1[1] - r1[0]) * METRIC_MAG)\n",
    "print((r2[1] - r2[0]) * METRIC_GMR)\n",
    "print((r3[1] - r3[0]) * METRIC_MAG)\n",
    "\n",
    "print(\"Middle 95% range\")\n",
    "r1 = np.percentile(df.loc[goodidx, 'ABS_MAG_R'], [2.5, 97.5])\n",
    "r3 = np.percentile(df.loc[goodidx, 'ABSMAG01_SDSS_R'], [2.5, 97.5])\n",
    "r2 = np.percentile(df.loc[goodidx, 'G_R_BEST'], [2.5, 97.5])\n",
    "print((r1[1] - r1[0]) * METRIC_MAG)\n",
    "print((r2[1] - r2[0]) * METRIC_GMR)\n",
    "print((r3[1] - r3[0]) * METRIC_MAG)\n",
    "\n",
    "print(\"Middle 99.7% range\")\n",
    "r1 = np.percentile(df.loc[goodidx, 'ABS_MAG_R'], [0.15, 99.85])\n",
    "r3 = np.percentile(df.loc[goodidx, 'ABSMAG01_SDSS_R'], [0.15, 99.85])\n",
    "r2 = np.percentile(df.loc[goodidx, 'G_R_BEST'], [0.15, 99.85])\n",
    "print((r1[1] - r1[0]) * METRIC_MAG)\n",
    "print((r2[1] - r2[0]) * METRIC_GMR)\n",
    "print((r3[1] - r3[0]) * METRIC_MAG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_mag_scaled = df.loc[goodidx, 'ABSMAG01_SDSS_R'] * METRIC_MAG \n",
    "gmr_scaled = df.loc[goodidx, 'G_R_BEST'] * METRIC_GMR \n",
    "data_points = np.vstack((abs_mag_scaled, gmr_scaled)).T\n",
    "kdtree = KDTree(data_points)\n",
    "dn4000_lookup = df.loc[goodidx, 'DN4000_MODEL'].values\n",
    "logmstar_lookup = df.loc[goodidx, 'LOGMSTAR'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(BGS_Y3_DN4000_LOOKUP_FILE, 'wb') as f:\n",
    "    pickle.dump((kdtree, dn4000_lookup, logmstar_lookup), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "lookup = dn4000lookup()\n",
    "# Pick 10 random index from df2\n",
    "n = np.min([len(df2), 1000])\n",
    "idx_test = np.random.choice(len(df2), size=n, replace=False)\n",
    "example_z = df2['Z'].values[idx_test]\n",
    "example_abs_mag = app_mag_to_abs_mag(df2['APP_MAG_R'].values[idx_test], example_z)\n",
    "dn4000_truth = df2['DN4000_MODEL'].values[idx_test]\n",
    "example_gmr = df2['APP_MAG_G'].values[idx_test] - df2['APP_MAG_R'].values[idx_test]\n",
    "quiescent_truth = df2['QUIESCENT'].values[idx_test]\n",
    "\n",
    "#example_abs_mag = np.random.uniform(-25, -14, size=100000) \n",
    "#example_gmr = np.random.uniform(-0.1, 2.5, size=100000) \n",
    "nearest_dn4000, near_logmstar = lookup.query(example_abs_mag, example_gmr)\n",
    "example_q = is_quiescent_BGS_dn4000(abs_mag_r_to_log_solar_L(example_abs_mag), nearest_dn4000, example_gmr)\n",
    "# TODO gmr_kcorr = \n",
    "example_q2 = is_quiescent_BGS_gmr(abs_mag_r_to_log_solar_L(example_abs_mag), example_gmr)\n",
    "\n",
    "# Print % classification that agrees\n",
    "agreement = np.sum(quiescent_truth == example_q) / len(example_q) * 100\n",
    "print(f\"\\nQuiescent classification agreement (dn4000 method): {agreement:.2f}%\\n\") # 76%\n",
    "agreement2 = np.sum(quiescent_truth == example_q2) / len(example_q2) * 100\n",
    "print(f\"Quiescent classification agreement (g-r method): {agreement2:.2f}%\\n\") # 81%\n",
    "\n",
    "for i in range(10):\n",
    "    print(f\"Test {i}: M_r={example_abs_mag[i]:.2f}, g-r={example_gmr[i]:.2f}, T DN4000={dn4000_truth[i]:.3f}, lookup DN4000={nearest_dn4000[i]:.3f}, T Q={quiescent_truth[i].astype(int)}, lookup Q={example_q[i].astype(int)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, ['ABS_MAG_R', 'ABSMAG01_SDSS_R']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make another one for k-corrections. {Z, ABS_MAG_R (no k corr), ABS_MAG_G (no k corr), g-r (apparent)} => {k-corr-r, k-corr-g}\n",
    "# This was run over the usual redshift range, 0.001 < z < 0.5, for Y3 Loa Observed Galaxies to a magnitude of 20.175\n",
    "# Prepare the data for KDTree\n",
    "magr = app_mag_to_abs_mag(df['APP_MAG_R'], df['Z'])\n",
    "magg = app_mag_to_abs_mag(df['APP_MAG_G'], df['Z'])\n",
    "magr_k_gama = k_correct_gama(magr, df['Z'], magg - magr, band='r')\n",
    "magg_k_gama = k_correct_gama(magg, df['Z'], magg - magr, band='g')\n",
    "badmatch = (np.abs(magr_k_gama - df['ABSMAG01_SDSS_R']) > 1.0) | (np.abs(magg_k_gama - df['ABSMAG01_SDSS_G']) > 1.0)\n",
    "goodidx = ~np.isnan(magr) & ~np.isnan(magg) & ~np.isnan(df['Z']) & ~np.isnan(df['ABSMAG01_SDSS_R']) & ~np.isnan(df['ABSMAG01_SDSS_G']) & ~badmatch\n",
    "gmr = magg[goodidx] - magr[goodidx]\n",
    "\n",
    "print(\"Middle 50% range\")\n",
    "r2 = np.percentile(gmr, [25, 75])\n",
    "r4 = np.percentile(df.loc[goodidx, 'Z'], [25, 75])\n",
    "print((r2[1] - r2[0]) * kcorrlookup.METRIC_GMR)\n",
    "print((r4[1] - r4[0]) * kcorrlookup.METRIC_Z)\n",
    "\n",
    "print(\"Middle 80% range\")\n",
    "r2 = np.percentile(gmr, [10, 90])\n",
    "r4 = np.percentile(df.loc[goodidx, 'Z'], [10, 90])\n",
    "print((r2[1] - r2[0]) * kcorrlookup.METRIC_GMR)\n",
    "print((r4[1] - r4[0]) * kcorrlookup.METRIC_Z)\n",
    "\n",
    "print(\"Middle 95% range\")\n",
    "r2 = np.percentile(gmr, [2.5, 97.5])\n",
    "r4 = np.percentile(df.loc[goodidx, 'Z'], [2.5, 97.5])\n",
    "print((r2[1] - r2[0]) * kcorrlookup.METRIC_GMR)\n",
    "print((r4[1] - r4[0]) * kcorrlookup.METRIC_Z)\n",
    "\n",
    "print(\"Middle 99.7% range\")\n",
    "r2 = np.percentile(gmr, [2.5, 97.5])\n",
    "r4 = np.percentile(df.loc[goodidx, 'Z'], [0.15, 99.85])\n",
    "print((r2[1] - r2[0]) * kcorrlookup.METRIC_GMR)\n",
    "print((r4[1] - r4[0]) * kcorrlookup.METRIC_Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#abs_mag_r_scaled = magr[goodidx] * kcorrlookup.METRIC_ABSMAG_R \n",
    "#abs_mag_g_scaled = magg[goodidx] * kcorrlookup.METRIC_ABSMAG_G \n",
    "gmr_scaled = gmr * kcorrlookup.METRIC_GMR\n",
    "z_scaled = df.loc[goodidx, 'Z'].to_numpy() * kcorrlookup.METRIC_Z\n",
    "data_points = np.vstack((z_scaled, gmr_scaled)).T\n",
    "kdtree = KDTree(data_points)\n",
    "kcorr_r_lookup = (magr[goodidx] - df.loc[goodidx, 'ABSMAG01_SDSS_R']).values # difference is the k correction amount\n",
    "kcorr_g_lookup = (magg[goodidx] - df.loc[goodidx, 'ABSMAG01_SDSS_G']).values \n",
    "assert len(kcorr_r_lookup) == len(data_points)\n",
    "assert np.max(kcorr_r_lookup) < 2.0\n",
    "assert np.min(kcorr_r_lookup) > -2.0\n",
    "assert np.max(kcorr_g_lookup) < 2.5\n",
    "assert np.min(kcorr_g_lookup) > -2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(BGS_Y3_KCORR_LOOKUP_FILE, 'wb') as f:\n",
    "    pickle.dump((kdtree, kcorr_r_lookup, kcorr_g_lookup), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emcee\n",
    "import corner\n",
    "\n",
    "z_train = z_scaled = df.loc[goodidx, 'Z'].to_numpy()\n",
    "magr_train = magr[goodidx]\n",
    "magg_train = magg[goodidx]\n",
    "gmr_train = magg_train - magr_train\n",
    "\n",
    "n = np.min([len(df2), 10000])\n",
    "idx_test = np.arange(n)\n",
    "test_z = df2['Z'].values[idx_test]\n",
    "test_mag_r = app_mag_to_abs_mag(df2['APP_MAG_R'].values[idx_test], df2['Z'].values[idx_test])\n",
    "test_mag_g = app_mag_to_abs_mag(df2['APP_MAG_G'].values[idx_test], df2['Z'].values[idx_test])\n",
    "test_gmr = test_mag_g - test_mag_r\n",
    "\n",
    "true_abs_mag_r = df2['ABSMAG01_SDSS_R'].values[idx_test]\n",
    "true_abs_mag_g = df2['ABSMAG01_SDSS_G'].values[idx_test]\n",
    "true_gmr = true_abs_mag_g - true_abs_mag_r\n",
    "\n",
    "# Define the log likelihood function for optimizing k-correction lookup metrics\n",
    "def log_likelihood(params):\n",
    "    \"\"\"\n",
    "    Evaluate how well a given set of metric parameters performs\n",
    "    for k-correction lookup accuracy.\n",
    "    \n",
    "    params: [metric_z, metric_gmr]\n",
    "    metric_absmag_r is fixed to 1.0\n",
    "    \"\"\"\n",
    "    metric_z, metric_gmr = params\n",
    "    \n",
    "    # Rebuild KDTree with new metrics\n",
    "    z_scaled = z_train * metric_z\n",
    "    gmr_scaled = gmr_train * metric_gmr\n",
    "    abs_mag_r_scaled = magr_train\n",
    "    \n",
    "    train_points = np.vstack((z_scaled, gmr_scaled, abs_mag_r_scaled)).T\n",
    "    kdtree_test = KDTree(train_points)\n",
    "    \n",
    "    # Query nearest neighbors\n",
    "    test_points = np.vstack((test_z * metric_z, \n",
    "                             test_gmr * metric_gmr,\n",
    "                             test_mag_r)).T\n",
    "    distances, indices = kdtree_test.query(test_points)\n",
    "    \n",
    "    # Get predicted k-corrections\n",
    "    pred_kcorr_r = kcorr_r_lookup[indices]\n",
    "    pred_kcorr_g = kcorr_g_lookup[indices]\n",
    "    \n",
    "    # Calculate predicted k-corrected magnitudes\n",
    "    pred_abs_mag_r_kcorr = test_mag_r - pred_kcorr_r\n",
    "    pred_abs_mag_g_kcorr = test_mag_g - pred_kcorr_g\n",
    "    pred_gmr_kcorr = pred_abs_mag_g_kcorr - pred_abs_mag_r_kcorr\n",
    "    \n",
    "    # Calculate errors\n",
    "    abs_mag_r_errors = np.abs(pred_abs_mag_r_kcorr - true_abs_mag_r)\n",
    "    gmr_errors = np.abs(pred_gmr_kcorr - true_gmr)\n",
    "    \n",
    "    # Log likelihood: negative weighted mean absolute error\n",
    "    # Weight abs_mag_r error more heavily than g-r error\n",
    "    log_like = -(3.0 * np.mean(abs_mag_r_errors) + 1.0 * np.mean(gmr_errors))\n",
    "    \n",
    "    return log_like\n",
    "\n",
    "def log_prior(params):\n",
    "    \"\"\"Uniform prior on metric parameters\"\"\"\n",
    "    metric_z, metric_gmr = params\n",
    "    \n",
    "    # Reasonable bounds for metrics\n",
    "    if 1.0 < metric_z < 100.0 and 1.0 < metric_gmr < 50.0:\n",
    "        return 0.0\n",
    "    return -np.inf\n",
    "\n",
    "def log_probability(params):\n",
    "    lp = log_prior(params)\n",
    "    if not np.isfinite(lp):\n",
    "        return -np.inf\n",
    "    return lp + log_likelihood(params)\n",
    "\n",
    "# Initialize walkers\n",
    "n_walkers = 8\n",
    "n_dim = 2  # Now only 2 parameters\n",
    "\n",
    "# Starting position: current kcorrlookup metrics (excluding metric_absmag_r)\n",
    "initial_params = np.array([kcorrlookup.METRIC_Z, kcorrlookup.METRIC_GMR])\n",
    "pos = initial_params + 1e-2 * np.random.randn(n_walkers, n_dim)\n",
    "\n",
    "# Set up sampler\n",
    "sampler = emcee.EnsembleSampler(n_walkers, n_dim, log_probability)\n",
    "\n",
    "# Run MCMC\n",
    "print(\"Running MCMC...\")\n",
    "n_steps = 50\n",
    "sampler.run_mcmc(pos, n_steps, progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get results\n",
    "samples = sampler.get_chain(discard=0, thin=1, flat=True)\n",
    "\n",
    "# Print results\n",
    "print(\"\\nOptimal parameters:\")\n",
    "print(f\"METRIC_Z: {np.median(samples[:, 0]):.2f} (16-84%: {np.percentile(samples[:, 0], [16, 84])})\")\n",
    "print(f\"METRIC_GMR: {np.median(samples[:, 1]):.2f} (16-84%: {np.percentile(samples[:, 1], [16, 84])})\")\n",
    "print(f\"METRIC_ABSMAG_R: 1.00 (fixed)\")\n",
    "\n",
    "# Plot corner plot\n",
    "fig = corner.corner(samples, labels=[\"METRIC_Z\", \"METRIC_GMR\"],\n",
    "                   truths=initial_params)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "klookup = kcorrlookup()\n",
    "# Pick 10 random index from df2\n",
    "n = np.min([len(df2), 1000])\n",
    "idx_test = np.random.choice(len(df2), size=n, replace=False)\n",
    "example_z = df2['Z'].values[idx_test]\n",
    "example_abs_mag_r = app_mag_to_abs_mag(df2['APP_MAG_R'].values[idx_test], example_z)\n",
    "example_abs_mag_g = app_mag_to_abs_mag(df2['APP_MAG_G'].values[idx_test], example_z)\n",
    "true_abs_mag_r = df2['ABSMAG01_SDSS_R'].values[idx_test]\n",
    "true_abs_mag_g = df2['ABSMAG01_SDSS_G'].values[idx_test]\n",
    "\n",
    "#example_z = np.random.uniform(0.001, 0.5, size=1000)  # Replace with your actual data\n",
    "#example_abs_mag_r = np.random.uniform(-25, -15, size=1000)  # Replace with your actual data\n",
    "#example_abs_mag_g = np.random.uniform(-25, -15, size=1000)   # Replace with your actual data\n",
    "nearest_kcorr_r, nearest_kcorr_g = klookup.query(example_abs_mag_r, example_abs_mag_g, example_z)\n",
    "calculated_abs_mag_r, calculated_abs_mag_g = k_correct_fromlookup(example_abs_mag_r, example_abs_mag_g, example_z)\n",
    "\n",
    "# What % have a k-corrected g-r within 0.1 of the true value?\n",
    "true_gmr = true_abs_mag_g - true_abs_mag_r\n",
    "calc_gmr = calculated_abs_mag_g - calculated_abs_mag_r\n",
    "agreement = np.sum(np.abs(true_gmr - calc_gmr) < 0.1) / len(true_gmr) * 100\n",
    "print(f\"\\nK-correction g-r agreement within 0.1 mag: {agreement:.2f}%\\n\")\n",
    "\n",
    "for i in range(10):\n",
    "    print(f\"Test {i}: z={example_z[i]:.3f}, M_r={example_abs_mag_r[i]:.2f}, M_g={example_abs_mag_g[i]:.2f}, True M_r^0.1={true_abs_mag_r[i]:.2f}, Calc M_r^0.1={calculated_abs_mag_r[i]:.2f}, True M_g={true_abs_mag_g[i]:.2f}, Calc M_g={calculated_abs_mag_g[i]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the random redshift hashtables\n",
    "This was run over the usual redshift range, 0.001 < z < 0.5, for Y3 Loa Observed Galaxies to a magnitude of 20.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For comparison\n",
    "with open(IAN_MXXL_LOST_APP_TO_Z_FILE, 'rb') as f:\n",
    "    mxxl_app_mag_bins, mxxl_the_map = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is how we did it for MXXL\n",
    "mag_bins, the_map = build_app_mag_to_z_map(app_mag_r[~unobserved], z_obs[~unobserved])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New better way I think\n",
    "app_mag_bins2, the_map2 = build_app_mag_to_z_map_2(app_mag_r[~unobserved], z_obs[~unobserved])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in the_map2.keys():\n",
    "    print(f\"App mag bin {key} has {len(the_map2[key])} galaxies.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    m = app_mag_r[~unobserved][i]\n",
    "    zp = z_phot[~unobserved][i]\n",
    "    distribution = the_map2[np.digitize(m, app_mag_bins2)]\n",
    "    percentiles = np.percentile(distribution, [5, 16, 50, 84, 95])\n",
    "    mean = np.mean(distribution)\n",
    "    std = np.std(distribution)\n",
    "    # what sigma does zp lie at?\n",
    "    sigma = (zp - mean) / std\n",
    "    print(f\"App mag: {m:.2f}, z_p: {zp:.4f}, sigma: {sigma:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And now the photo-z infused way (there is a version 3 as well which fills empty/small bins with neighborin values)\n",
    "app_mag_bins3, z_phot_bins, the_map3 = build_app_mag_to_z_map_4(app_mag_r[~unobserved].copy(), z_phot[~unobserved].copy(),  z_obs[~unobserved].copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.printoptions(precision=4, linewidth=200):\n",
    "    for k in the_map3.keys():\n",
    "        print(f\"Key {k} has {len(the_map3[k])} galaxies\")\n",
    "        #print(f\"Key {k} has {len(the_map3[k])} galaxies: {np.percentile(the_map3[k], [5, 16, 50, 84, 95])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(0, 0.5, 50)\n",
    "\n",
    "#print(bins)\n",
    "plt.figure(figsize=(10, 6))\n",
    "trash=plt.hist(the_map2[33],bins=bins, color='green', density=True, histtype='step', label=app_mag_bins2[33], linestyle='dashed')\n",
    "trash=plt.hist(the_map2[284],bins=bins, color='darkred', density=True, histtype='step', label=app_mag_bins2[284], linestyle='dashed')\n",
    "\n",
    "trash=plt.hist(the_map3[39, 10], bins=bins, color=[0.0, 1.0, 0.0], density=True, histtype='step', label=f\"z={z_phot_bins[10]:.3f} r={app_mag_bins3[39]:.2f}\", linestyle='dotted')\n",
    "trash=plt.hist(the_map3[30, 10], bins=bins, color=[0.1, 0.8, 0.0], density=True, histtype='step', label=f\"z={z_phot_bins[10]:.3f} r={app_mag_bins3[30]:.2f}\", linestyle='dotted')\n",
    "trash=plt.hist(the_map3[22, 10], bins=bins, color=[0.2, 0.6, 0.0], density=True, histtype='step', label=f\"z={z_phot_bins[10]:.3f} r={app_mag_bins3[22]:.2f}\", linestyle='dotted')\n",
    "trash=plt.hist(the_map3[15, 10], bins=bins, color=[0.3, 0.4, 0.0], density=True, histtype='step', label=f\"z={z_phot_bins[10]:.3f} r={app_mag_bins3[15]:.2f}\", linestyle='dotted')\n",
    "\n",
    "#trash=plt.hist(mxxl_the_map[29],bins=bins, color='green', density=True, histtype='step', label=mxxl_app_mag_bins[29])\n",
    "#trash=plt.hist(app_mag_bins_read[50],bins=bins, color='orange', density=True, histtype='step', label=app_mag_bins_read[50], linestyle='dotted')\n",
    "#trash=plt.hist(mxxl_the_map[90],bins=bins, color='darkred', density=True, histtype='step', label=mxxl_app_mag_bins[90])\n",
    "plt.legend()\n",
    "\n",
    "plt.xlim(0, 0.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(BGS_Y3_LOST_APP_TO_Z_FILE, 'wb') as f:\n",
    "    pickle.dump((app_mag_bins2, the_map2), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(BGS_Y3_LOST_APP_AND_ZPHOT_TO_Z_FILE, 'wb') as f:\n",
    "    pickle.dump((app_mag_bins3, z_phot_bins, the_map3), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Luminosity Thresholds Number Density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = [-17, -18, -19, -20, -21, -22]\n",
    "df['ABS_MAG_R_K_E'] = df['ABS_MAG_R'] + 0.8 * (0.1 - df['Z'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO copied from Ashley, broken\n",
    "def getnz(datcut):\n",
    "    dz = 0.01\n",
    "    zmax = 0.5\n",
    "    z = 0.0\n",
    "    fullsky = 360*360/np.pi\n",
    "    area = 8656 # unsure\n",
    "    zl = []\n",
    "    nl = []\n",
    "    while z < zmax:\n",
    "        sel = datcut['Z'] > z\n",
    "        sel &= datcut['Z'] < z+dz\n",
    "        #ngal = np.sum(datcut[sel]['WEIGHT_COMP'])\n",
    "        ngal = datcut.loc[sel].shape[0]\n",
    "        fsky = area/fullsky\n",
    "        #disl = distance([z,z+dz])\n",
    "        disl = z_to_ldist(z), z_to_ldist(z+dz)\n",
    "        vshell = 4/3.*np.pi*(disl[1]**3.-disl[0]**3.)\n",
    "        vtot = vshell*fsky\n",
    "        nbar = ngal/vtot\n",
    "        zl.append(z+dz/2.)\n",
    "        nl.append(nbar)\n",
    "        z += dz\n",
    "    return zl,nl\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for i, threshold in enumerate(thresholds):\n",
    "    mask = df['ABS_MAG_R_K'] < threshold\n",
    "    mask2 = df['ABS_MAG_R_K_E'] < threshold\n",
    "    zlc,nlc = getnz(df.loc[mask])\n",
    "    zlc2,nlc2 = getnz(df.loc[mask2])\n",
    "\n",
    "    #z_values = df.loc[mask, 'Z'].to_numpy()\n",
    "    #z_values2 = df.loc[mask2, 'Z'].to_numpy()\n",
    "    #hist = np.histogram(z_values, bins=np.linspace(0, 0.5, 50), density=True)\n",
    "    #hist2 = np.histogram(z_values2, bins=np.linspace(0, 0.5, 50), density=True)\n",
    "    color = sns.color_palette(\"tab10\", len(thresholds))[i]\n",
    "    #plt.plot(hist[1][:-1], hist[0], label=f'$R < {threshold}$', color=color, linestyle='-')\n",
    "    #plt.plot(hist2[1][:-1], hist2[0], color=color, linestyle='--')\n",
    "\n",
    "    plt.plot(zlc, nlc, label=f'$R < {threshold}$', color=color, linestyle='-')\n",
    "    plt.plot(zlc2, nlc2, color=color, linestyle='--')\n",
    "\n",
    "plt.xlabel('Redshift $z$')\n",
    "plt.ylabel('Number Density $n(z)$')\n",
    "plt.title('Redshift Distribution of BGS ANY')\n",
    "plt.legend(title=\"Solid: raw, Dashed: e-corr\", fontsize=11)\n",
    "plt.grid('both')\n",
    "plt.xlim(0, 0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quality Cuts Research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fracflux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(MASKBITS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interesting = df.loc[df['MASKBITS'] & MASKBITS['NPRIMARY'] != 0]\n",
    "print(f\"Number of interesting objects: {len(interesting)}\")\n",
    "interesting.reset_index().loc[0:10, ['RA', 'DEC', 'MASKBITS']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(np.isnan(df['FRACFLUX_R']).sum() + np.isnan(df['FRACFLUX_G']).sum() + np.isnan(df['FRACFLUX_Z']).sum())\n",
    "\n",
    "#orig_table = Table.read(BGS_ANY_FULL_FILE, format='fits')\n",
    "FF_CUT = 0.35\n",
    "# LOW Z folks used 0.35 for this. I think we can get away with cutting less than that.\n",
    "ff_g = df['FRACFLUX_G'] < FF_CUT\n",
    "ff_r = df['FRACFLUX_R'] < FF_CUT\n",
    "ff_z = df['FRACFLUX_Z'] < FF_CUT\n",
    "\n",
    "print(f\"There are {len(df):,} galaxies in the sample.\")\n",
    "print(f\"There are {np.sum(~ff_g):,} galaxies with FRACFLUX_G > {FF_CUT}.\")\n",
    "print(f\"There are {np.sum(~ff_r):,} galaxies with FRACFLUX_R > {FF_CUT}.\")\n",
    "print(f\"There are {np.sum(~ff_z):,} galaxies with FRACFLUX_Z > {FF_CUT}.\")\n",
    "ff_mask = np.sum([ff_g, ff_r, ff_z], axis=0) >= 2\n",
    "\n",
    "# I manually inspected the first 50 of these and I'm only unhappy about cutting 7 of them.\n",
    "# More than 7 did look like galaxies but were quite complicated overlappin situations.\n",
    "removed = df[~ff_mask]\n",
    "print(f\"Removing {len(removed):,} galaxies ({len(removed) / len(df) * 100:.2f}%) with 2 or more FRACFLUX values > {FF_CUT}.\")\n",
    "\n",
    "# How many were spectroscopic ones?\n",
    "spectro_removed = removed[~np.isnan(removed['Z'])]\n",
    "print(f\"Of the removed galaxies, {len(spectro_removed):,} ({len(spectro_removed) / len(removed) * 100:.2f}%) had spectra and survived other cuts.\")\n",
    "\n",
    "print(np.isnan(df['Z']).sum() / len(df))\n",
    "print(np.isnan(removed['Z']).sum() / len(removed))\n",
    "\n",
    "removed.reset_index().loc[0:10, ['FRACFLUX_G','FRACFLUX_Z','FRACFLUX_R']]\n",
    "removed.reset_index().loc[0:100,['RA', 'DEC']].to_csv(\"removed3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Photo-Z vs Spec-Z Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"There are {(z_phot != NO_PHOTO_Z).sum():,} ({(z_phot != NO_PHOTO_Z).sum() / len(z_phot):.1%}) targets with phot-z\")\n",
    "\n",
    "good_idx = np.flatnonzero((z_phot != NO_PHOTO_Z) & ~unobserved) #& ~(np.isclose(z_phot, z_obs, atol=0.00025, rtol=0.000001)))\n",
    "print(f\"Amongst observed galaxies there are {len(good_idx):,} ({len(good_idx) / np.sum(~unobserved) * 100:.2f}%) galaxies with photo-z.\")\n",
    "\n",
    "unobserved_with_photz = np.flatnonzero((z_phot != NO_PHOTO_Z) & unobserved)\n",
    "print(f\"Amongst unobserved galaxies there are {len(unobserved_with_photz):,} ({len(unobserved_with_photz) / np.sum(unobserved) * 100:.2f}%) with photo-z.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_z = z_phot[good_idx] - z_obs[good_idx]\n",
    "plt.hist(delta_z, bins=600, range=(-0.2, 0.2))\n",
    "#plt.yscale(\"log\")\n",
    "plt.title(\"Photo-z Quality\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xlabel(\"z_phot - z_spec\")\n",
    "plt.ylim(0, 70000)\n",
    "\n",
    "\n",
    "# add bars for my z_thresh\n",
    "plt.axvline(-SIM_Z_THRESH, color='red')\n",
    "plt.axvline(SIM_Z_THRESH, color='red')\n",
    "\n",
    "percentiles = np.percentile(delta_z, [16, 50, 84])\n",
    "print(f\"Median delta z: {percentiles[1]:.4f}, 16th percentile: {percentiles[0]:.4f}, 84th percentile: {percentiles[2]:.4f}\")\n",
    "percentiles = np.percentile(delta_z, [2.5, 97.5])\n",
    "print(f\"2.5th percentile: {percentiles[0]:.4f}, 97.5th percentile: {percentiles[1]:.4f}\")\n",
    "# add bars for the percentiles\n",
    "plt.axvline(percentiles[0], color='green')\n",
    "plt.axvline(percentiles[1], color='green')\n",
    "\n",
    "# What % fall within 0.005 of the true redshift?\n",
    "within_5_milli = np.abs(delta_z) < SIM_Z_THRESH\n",
    "print(f\"{np.sum(within_5_milli) / len(delta_z) * 100:.2f}% of galaxies have a photometric redshift within {SIM_Z_THRESH} of the spectroscopic redshift.\")\n",
    "\n",
    "# Now look only at quiescent galaxies less than 10^9 solar luminosities\n",
    "# TODO \n",
    "#luminosity = abs_mag_r_to_log_solar_L(app_mag_to_abs_mag_k(app_mag_r, z_obs, g_r_apparent))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define Gaussian and Lorentzian functions\n",
    "def gaussian(x, amp, mean, stddev):\n",
    "    return amp * np.exp(-((x - mean) ** 2) / (2 * stddev ** 2))\n",
    "\n",
    "def lorentzian(x, amp, mean, gamma, exp):\n",
    "    return amp * gamma**2 / (np.power(np.abs(x - mean), exp) + gamma**2)\n",
    "\n",
    "# Prepare data\n",
    "abs_delta_z = delta_z\n",
    "hist, bin_edges = np.histogram(abs_delta_z, bins=1000, range=(-0.2, 0.2), density=True)\n",
    "bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "\n",
    "# Fit Gaussian\n",
    "popt_gauss, _ = curve_fit(gaussian, bin_centers, hist, p0=[1, 0, 0.01], maxfev=100000)\n",
    "# Poor fit\n",
    "\n",
    "# Fit Lorentzian\n",
    "popt_lorentz, _ = curve_fit(lorentzian, bin_centers, hist, p0=[24, 0.0001, 0.0153, 2.0], maxfev=100000)\n",
    "#Lorentzian fit parameters: amp=22.902018525656057, mean=8.151405674056218e-05, gamma=0.009227129158691942, exp=2.2857148452956757\n",
    "\n",
    "\n",
    "# Plot histogram and fits\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(abs_delta_z, bins=1000, range=(-0.2, 0.2), density=True, alpha=0.6, label='Data')\n",
    "plt.plot(bin_centers, gaussian(bin_centers, *popt_gauss), label='Gaussian fit', color='red')\n",
    "plt.plot(bin_centers, lorentzian(bin_centers, *popt_lorentz), label='~Lorentzian fit', color='green')\n",
    "plt.xlabel('z_phot - z_spec')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.title('Fitting delta_z with Gaussian and ~Lorentzian')\n",
    "plt.show()\n",
    "\n",
    "# Print fit parameters\n",
    "print(f\"Gaussian fit parameters: amp={popt_gauss[0]}, mean={popt_gauss[1]}, stddev={popt_gauss[2]}\")\n",
    "print(f\"Lorentzian fit parameters: amp={popt_lorentz[0]}, mean={popt_lorentz[1]}, gamma={popt_lorentz[2]}, exp={popt_lorentz[3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SV3 Analysis\n",
    "\n",
    "SV3 is composed of 20 regions where 10 or 11 exposures eacj were taken, almost completely on top of each other.  Our SV3 analysis takes the inner part of these patches (NTILE_MINE >= 10) of these regions as the data set.  \n",
    "\n",
    "Then, we can eliminate 1 tile from each of these regions to make test sets in order to view our systematics as a function of NTILE_MINE. The order they are eliminated in matters; we need to go backwards in time.}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a DataFrame filtered down to the galaxies we want to keep\n",
    "sv3_merged_table = Table.read(IAN_BGS_SV3_MERGED_FILE, format='fits')\n",
    "sv3_merged_table.remove_column('NEAREST_TILEIDS')\n",
    "sv3_df = sv3_merged_table.to_pandas()\n",
    "print(len(sv3_df))\n",
    "sv3_df['APP_MAG_R'] = get_app_mag(sv3_df['FLUX_R'])\n",
    "unobserved = sv3_merged_table['Z'].astype(\"<i8\") == 999999\n",
    "galaxy_observed_filter = sv3_df['SPECTYPE'] == b'GALAXY'\n",
    "redshift_filter = sv3_df['Z'] > Z_MIN\n",
    "redshift_hi_filter = sv3_df['Z'] < Z_MAX\n",
    "deltachi2_filter = sv3_df['DELTACHI2'] > 40\n",
    "app_mag_filter = sv3_df['APP_MAG_R'] < 20.3\n",
    "observed_requirements = np.all([galaxy_observed_filter, app_mag_filter, redshift_filter, redshift_hi_filter, deltachi2_filter], axis=0)\n",
    "treat_as_unobserved = np.all([galaxy_observed_filter, app_mag_filter, np.invert(deltachi2_filter)], axis=0)\n",
    "\n",
    "unobserved = np.all([app_mag_filter, np.logical_or(unobserved, treat_as_unobserved)], axis=0)\n",
    "sv3_df['OBSERVED'] = np.invert(unobserved)\n",
    "keep = np.all([np.logical_or(observed_requirements, unobserved)], axis=0)\n",
    "keep = np.all([keep, sv3_df['NTILE_MINE'] >= 10], axis=0)\n",
    "\n",
    "sv3_df = sv3_df.loc[keep] \n",
    "sv3_df.reset_index(drop=True, inplace=True)\n",
    "print(len(sv3_df))\n",
    "\n",
    "# Initialize new columns for observed as function of N pass\n",
    "for i in range(0, 12):\n",
    "    sv3_df[f'OBSERVED_{i}'] = sv3_df['OBSERVED']\n",
    "\n",
    "for FAINT in [False, True]:\n",
    "\n",
    "    if not FAINT:\n",
    "        mag_filter = sv3_df['APP_MAG_R'] < 19.5\n",
    "    else:\n",
    "        mag_filter = sv3_df['APP_MAG_R'] > 19.5\n",
    "        \n",
    "    print(f\"{len(sv3_df[mag_filter]) / 138.192} galaxies per sq degree\")\n",
    "\n",
    "    for patch_number in range(len(sv3_regions_sorted)):\n",
    "        tilelist = sv3_regions_sorted[patch_number]\n",
    "        #print(f'Patch {patch_number} - TILE IDs: {tilelist}')\n",
    "        \n",
    "        row_selector = np.logical_and(sv3_df['TILEID'].isin(tilelist), mag_filter)\n",
    "\n",
    "        #one_patch_df = sv3_df[sv3_df['TILEID'].isin(tilelist)]\n",
    "        #print(f\"{len(one_patch_df)} galaxies, {np.sum(one_patch_df['OBSERVED']) / len(one_patch_df) :.1%} of the targets are observed\")\n",
    "        #one_patch_df[f'OBSERVED_{len(tilelist)}'] = one_patch_df['OBSERVED']\n",
    "        \n",
    "        #print (\"Remove tiles in reverse TILEID order:\")\n",
    "        for i in np.flip(np.arange(0, len(tilelist))):\n",
    "            tileid = tilelist[i]\n",
    "            observed_by_this_tile = sv3_df.loc[row_selector, 'TILEID'] == tileid\n",
    "            #print(f'{np.sum(observed_by_this_tile)} galaxies were observed by tile {tileid} ({i+1}/{len(tilelist)})')\n",
    "            prev = sv3_df.loc[row_selector, f'OBSERVED_{i+1}']\n",
    "            sv3_df.loc[row_selector, f'OBSERVED_{i}'] = np.where(observed_by_this_tile, False, prev)\n",
    "            \n",
    "        #for i in np.flip(np.arange(0, len(tilelist)+1)):\n",
    "        #    if FAINT:\n",
    "        #        totals_observed_faint[i] += np.sum(sv3_df.loc[row_selector, f'OBSERVED_{i}'])\n",
    "        #        totals_all_faint[i] += len(sv3_df.loc[row_selector])\n",
    "        #    else:\n",
    "        #        totals_observed_bright[i] += np.sum(sv3_df.loc[row_selector, f'OBSERVED_{i}'])\n",
    "        #        totals_all_bright[i] += len(sv3_df.loc[row_selector])\n",
    "\n",
    "            #print(f\"{np.sum(one_patch_df[f'OBSERVED_{i}']) / len(one_patch_df) :.1%} of the targets are observed with {i} passes\")\n",
    "                \n",
    "    #for i in range(1, 12):\n",
    "    #    if FAINT:\n",
    "    #        print(f\"{totals_observed_faint[i]:,} ({totals_observed_faint[i] / totals_all_faint[i]:.1%}) faint galaxies are observed with {i} passes\")\n",
    "    #    else: \n",
    "    #        print(f\"{totals_observed_bright[i]:,} ({totals_observed_bright[i] / totals_all_bright[i]:.1%}) bright galaxies are observed with {i} passes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observed_faint = np.zeros(12, dtype=int)\n",
    "observed_bright = np.zeros(12, dtype=int)\n",
    "\n",
    "total_faint = np.sum(sv3_df['APP_MAG_R'] > 19.5)\n",
    "total_bright = np.sum(sv3_df['APP_MAG_R'] < 19.5)\n",
    "\n",
    "for i in range(0, 12):\n",
    "    observed_faint[i] = np.sum(sv3_df.loc[sv3_df['APP_MAG_R'] > 19.5, f'OBSERVED_{i}'])\n",
    "    observed_bright[i] = np.sum(sv3_df.loc[sv3_df['APP_MAG_R'] < 19.5, f'OBSERVED_{i}'])\n",
    "\n",
    "\n",
    "plt.plot(observed_bright / total_bright, color='b', label=\"BGS BRIGHT ME\")\n",
    "plt.plot(observed_faint / total_faint, color='orange', label=\"BGS FAINT ME\")\n",
    "plt.plot([1,2,3,4], [.29, .52, 0.68, .81], '--', color='b', label=\"BGS BRIGHT PAPER\")\n",
    "plt.plot([1,2,3,4], [.15, .32, 0.47, .62], '--', color='orange', label=\"BGS FAINT PAPER\")\n",
    "plt.xlabel(\"Number of passes\")\n",
    "plt.ylabel(\"Fraction of targets observed\")\n",
    "plt.title(\"SV3 BGS Completeness\")\n",
    "plt.xticks(np.arange(0, 12))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sv3_df.loc[sv3_df['APP_MAG_R'] < 19.5, 'OBSERVED'].count() - sv3_df.loc[sv3_df['APP_MAG_R'] < 19.5, 'OBSERVED'].sum())\n",
    "print(sv3_df.loc[sv3_df['APP_MAG_R'] < 19.5, 'OBSERVED'].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above numbers do not seem to track with the Y1 data. In Y1 no region has more than 4 passes so how do I have a fiber incompleteness better than the above number?\n",
    "\n",
    "Also Figure 17 of https://iopscience.iop.org/article/10.3847/1538-3881/accff8/pdf disagrees with my above analysis. So what is above is wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nearest Neighbor Analysis (for SV3 Merged File)\n",
    "Run this on SV3, no Y3 supplement, over the usual redshift range, observed galaxies only. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create bin files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percentiles for 10 equal bins\n",
    "percentiles = np.linspace(0, 100, 12)\n",
    "bin_edges = np.percentile(z_obs, percentiles)\n",
    "\n",
    "with np.printoptions(precision=3, linewidth=200):\n",
    "    print(f\"Even number z bin: {np.array(bin_edges[1:])}\")\n",
    "    print(f\"The Z Bins we use: {Z_BINS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAKE SURE that above KEEP_PASSES is set to 1 and you filter out unobserved galaxies as we need a source of 'truth'\n",
    "assert KEEP_PASSES == 1 and np.sum(unobserved) == 0\n",
    "assert Z_MIN == 0.001 and Z_MAX == 0.5\n",
    "\n",
    "# We will only consider the observed galaxies in SV3 as we need a source of truth for the analysis.\n",
    "# This is ~98% so this analysis should be representative.\n",
    "# As usual for SV3, only want galaxies that were fully covered by the rosetting pattern. (in_10p_zone)\n",
    "# We then pretend to have not observed ~20% of galaxies, as is the case in the main survey. (obs_7p)\n",
    "# TODO BUG Ashley says my method is wrong for doing this.\n",
    "in_10p_zone = ntiles_mine >= 10\n",
    "#obs_7p = ~gc.drop_SV3_passes(3, tile_id, unobserved)\n",
    "\n",
    "df['OBSERVED'] = True # Could go to Y3Loa-SV3Cut and match_coord_sky to get which ones to mark as unobserved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_id.mask.sum() # BUG prevents using LOST_GALAXIES_ONLY=True. Is it because of Y3 supplement has no TILE_ID?\n",
    "# Decided just to use with LOST_GALAXIES_ONLY=False, in the 98% complete SV3 sample this shouldn't matter much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "### Create NEIGHBOR_ANALYSIS_SV3_BINS_FILE\n",
    "###\n",
    "newobj = NNAnalyzer_cic.from_df(df)\n",
    "#newobj.set_row_locator( np.logical_and(obs_10p, app_mag_r < 19.5) ) # 10p inner regions and BRIGHT only\n",
    "newobj.set_row_locator(in_10p_zone)\n",
    "newobj.find_nn_properties(LOST_GALAXIES_ONLY=False) \n",
    "newobj.make_bins()\n",
    "\n",
    "print(np.sum(newobj.all_ang_bincounts))\n",
    "print(np.sum(newobj.all_sim_z_bincounts))\n",
    "\n",
    "newobj.save(NEIGHBOR_ANALYSIS_SV3_BINS_FILE_V2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian smoothing version\n",
    "loaded_nna = NNAnalyzer_cic.from_results_file(NEIGHBOR_ANALYSIS_SV3_BINS_FILE_V2)\n",
    "\n",
    "# Apply filling\n",
    "loaded_nna.fill_bins()\n",
    "loaded_nna.save('new_bins_test_1_filled')\n",
    "\n",
    "# Apply Gaussian filter\n",
    "loaded_nna.apply_gaussian_smoothing(0.75)\n",
    "loaded_nna.save(NEIGHBOR_ANALYSIS_SV3_BINS_SMOOTHED_FILE_V2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at bin results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#orig_nna = NNAnalyzer_cic.from_results_file(NEIGHBOR_ANALYSIS_SV3_BINS_FILE)\n",
    "#smoothed_nna = NNAnalyzer_cic.from_results_file(NEIGHBOR_ANALYSIS_SV3_BINS_SMOOTHED_FILE)\n",
    "orig_nna = NNAnalyzer_cic.from_results_file(NEIGHBOR_ANALYSIS_SV3_BINS_FILE_V2)\n",
    "filled_nna = NNAnalyzer_cic.from_results_file('new_bins_test_1_filled')\n",
    "smoothed_nna = NNAnalyzer_cic.from_results_file(NEIGHBOR_ANALYSIS_SV3_BINS_SMOOTHED_FILE_V2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare to previous\n",
    "new = NNAnalyzer_cic.from_results_file(NEIGHBOR_ANALYSIS_SV3_BINS_SMOOTHED_FILE)\n",
    "newresults1 = new.integrate_out_dimension((0,6))\n",
    "newresults2 = smoothed_nna.integrate_out_dimension((0,6))\n",
    "\n",
    "to_compare = [newresults1, newresults2]\n",
    "\n",
    "for newresults in to_compare:\n",
    "    old = NNAnalyzer_cic.from_results_file(NEIGHBOR_ANALYSIS_SV3_BINS_SMOOTHED_FILE + \"~\")\n",
    "    oldresults = old.integrate_out_dimension((0,6))\n",
    "    print(f\"Total Pairs considered: old is {np.sum(oldresults[2]):,}; new is {np.sum(newresults[2]):,}\")\n",
    "\n",
    "    good_data = oldresults[2] >= 5\n",
    "    print(f\"Number of good data points: {np.sum(good_data):,} out of {np.prod(np.shape(old.all_ang_bincounts))}\")\n",
    "\n",
    "    j=plt.hist(newresults[0][good_data] - oldresults[0][good_data], bins=200, range=(-0.5, 0.5), alpha=0.6)\n",
    "    #plt.axvline(np.percentile(newresults[0][good_data] - oldresults[0][good_data], 95), color='green', linestyle='--', label='95%')\n",
    "    #plt.axvline(np.percentile(newresults[0][good_data] - oldresults[0][good_data], 5), color='blue', linestyle='--', label='5%')\n",
    "    plt.xlabel('Difference in Sim Z Bincounts')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Difference in Sim Z Bincounts between New and Old')\n",
    "\n",
    "    # Write a bunch of percentiles\n",
    "    percentiles = np.percentile(newresults[0][good_data] - oldresults[0][good_data], [2.5, 16, 50, 84, 97.5])\n",
    "    print(f\"95% Interval: {percentiles[0]:.4f} to {percentiles[4]:.4f}\")\n",
    "    print(f\"68% Interval: {percentiles[1]:.4f} to {percentiles[3]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothed_nna.all_ang_bincounts.shape\n",
    "# Add nn{n}_delta_z using zphot from the original data\n",
    "# Remove ABS MAG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.append(np.logspace(np.log10(3), np.log10(900), ANG_DIST_BIN_COUNT - 1), np.log10(3600)) # upper bound is higher than any data, lower bound is not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(smoothed_nna.get_score(\n",
    "    np.array([0,1,2,3,4,5,6,7,8,9]),\n",
    "    np.repeat(19.0, 10), \n",
    "    np.repeat(1.0, 10), \n",
    "    np.repeat(0.25, 10),\n",
    "    np.repeat(30.0, 10),\n",
    "    np.repeat(0.0, 10),\n",
    "))\n",
    "print(smoothed_nna.get_score(\n",
    "    None, 19.0, 1.0, 0.25, 30.0, 0.0\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_nna.plot_angdist_appmag_per_zbin_cc(z_bin_numbers_to_plot=[2,4,6,8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How does the filling and smoothing work out?\n",
    "zbin = 3\n",
    "neighbors = 5\n",
    "orig_nna.plot_angdist_appmag_per_zbin_cc(z_bin_numbers_to_plot=[zbin], neighbors=neighbors, nn_c=[1], t_c=[0])\n",
    "filled_nna.plot_angdist_appmag_per_zbin_cc(z_bin_numbers_to_plot=[zbin], neighbors=neighbors, nn_c=[1], t_c=[0])\n",
    "smoothed_nna.plot_angdist_appmag_per_zbin_cc(z_bin_numbers_to_plot=[zbin], neighbors=neighbors, nn_c=[1], t_c=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothed_nna.plot_angdist_appmag_per_zbin_cc(z_bin_numbers_to_plot=[2], neighbors=8, nn_c=[1], t_c=[0])\n",
    "smoothed_nna.plot_angdist_appmag_per_zbin_cc(z_bin_numbers_to_plot=[2], nn_c=[1], t_c=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(smoothed_nna.all_ang_bincounts))\n",
    "print(np.prod(np.shape(smoothed_nna.all_ang_bincounts)))\n",
    "print(np.median(smoothed_nna.all_ang_bincounts))\n",
    "print(np.mean(smoothed_nna.all_ang_bincounts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_nna.plot_angdist_neighbor_per_zbin_cc(t_c=[0], nn_c=[0], z_bin_numbers_to_plot=[1,3,5,7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothed_nna.plot_angdist_neighbor_per_zbin_cc(t_c=[1], nn_c=[0], z_bin_numbers_to_plot=[1,3,5,7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_nna.plot_angdist_neighbor_per_zbin_cc(z_bin_numbers_to_plot=[1,4,7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothed_nna.plot_angdist_neighbor_per_zbin_cc(z_bin_numbers_to_plot=[1,4,7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS IS ALL OLD AND BROKEN\n",
    "\n",
    "# TODO de-dupe with pre_process_BGS(...)\n",
    "obs_7p = ~gc.drop_SV3_passes(3, tile_id, unobserved) # drop 3rd pass, which is the 7th pass in the full\n",
    "in_10p_zone = ntiles_mine >= 10 # at least 10 tiles observed\n",
    "# Examine photo-z NN relation\n",
    "NUM_NEIGHBORS = 30\n",
    "unobs_7p = ~obs_7p\n",
    "bright = app_mag_r[in_10p_zone] < 19.5\n",
    "use = bright & unobs_7p\n",
    "neighbor_indexes = np.zeros(shape=(NUM_NEIGHBORS, use.sum()), dtype=np.int32) # indexes point to CATALOG locations\n",
    "ang_distances = np.zeros(shape=(NUM_NEIGHBORS, use.sum()))\n",
    "\n",
    "catalog = coord.SkyCoord(ra=ra[in_10p_zone][obs_7p]*u.degree, dec=dec[in_10p_zone][obs_7p]*u.degree, frame='icrs')\n",
    "\n",
    "print(f\"Finding nearest {NUM_NEIGHBORS} neighbors... \", end='\\r')   \n",
    "for n in range(0, NUM_NEIGHBORS):\n",
    "    to_match = coord.SkyCoord(ra=ra[in_10p_zone][use]*u.degree, dec=dec[in_10p_zone][use]*u.degree, frame='icrs')\n",
    "    idx, d2d, d3d = coord.match_coordinates_sky(to_match, catalog, nthneighbor=n+1, storekdtree='sv3')\n",
    "    neighbor_indexes[n] = idx\n",
    "    ang_distances[n] = d2d.to(u.arcsec).value\n",
    "print(f\"Finding nearest {NUM_NEIGHBORS} neighbors... done!\")   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ztruth = z_obs[in_10p_zone][use]\n",
    "# fill zphot_fake with ztruth + a random gaussian draw around 0.0 with sigma of 0.01\n",
    "zphot = z_phot[in_10p_zone][use]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_pass_df.Z_ASSIGNED_FLAG = ~obs_7p\n",
    "plot_positions(one_pass_df, tiles_df=None, DEG_LONG=2.5, split=True, ra_min=one_pass_df.RA[0], dec_min=one_pass_df['DEC'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial look at Photo-z only matching to neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramspace = np.arange(0.001, 0.05, 0.001)\n",
    "                       \n",
    "cumulative_percent_correct_by_n_zp_firstonly = np.zeros((NUM_NEIGHBORS, len(paramspace)) , dtype=float)\n",
    "\n",
    "i = 0\n",
    "for PHOTOZ_MATCHING_THRESHOLD in paramspace:\n",
    "    \n",
    "    this_neighbor_correct = np.zeros(shape=(NUM_NEIGHBORS, len(ztruth)), dtype=bool)\n",
    "    z_phot_neighbor_match = np.zeros(shape=(NUM_NEIGHBORS, len(ztruth)), dtype=bool)\n",
    "    cumulative_percent_z_phot_neighbor_match = []\n",
    "    z_phot_first_neighbor_match_idx = np.ones(len(ztruth), dtype=int) * 999 # sentinal value for no match\n",
    "    z_phot_match_correct = np.zeros(shape=(NUM_NEIGHBORS, len(ztruth)), dtype=bool)\n",
    "    delta_z = np.zeros(shape=(NUM_NEIGHBORS, len(ztruth)), dtype=float)\n",
    "\n",
    "    percent_correct_by_n = []\n",
    "    percent_correct_by_n_zp = []\n",
    "    cumulative_percent_correct_by_n = []\n",
    "    cumulative_percent_correct_by_n_zp = []\n",
    "    cumulative_percent_correct_by_n_zp_closestonly = []\n",
    "    first_matched_but_incorrect = []\n",
    "\n",
    "    for n in range(0, NUM_NEIGHBORS):\n",
    "\n",
    "        z_neighbor = z_obs[in_10p_zone][obs_7p][neighbor_indexes[n]]\n",
    "        this_neighbor_correct[n] = close_enough(ztruth, z_neighbor)\n",
    "        percent_correct_by_n.append(this_neighbor_correct[n].sum() / len(ztruth))\n",
    "        any_neighbor_correct = this_neighbor_correct[0:n+1].max(axis=0) # max will be True if any neighbor is True\n",
    "        cumulative_percent_correct_by_n.append(any_neighbor_correct.sum() / len(ztruth))\n",
    "\n",
    "        z_phot_neighbor_match[n] = close_enough(zphot, z_neighbor, threshold=PHOTOZ_MATCHING_THRESHOLD)\n",
    "        any_z_phot_match = z_phot_neighbor_match[0:n+1].max(axis=0) # will be True if z_phot matches a neighbor\n",
    "        cumulative_percent_z_phot_neighbor_match.append(any_z_phot_match.sum() / len(ztruth))\n",
    "        z_phot_match_correct[n] = z_phot_neighbor_match[n] & this_neighbor_correct[n]\n",
    "        any_neighbor_zp_correct = z_phot_match_correct[0:n+1].max(axis=0) # will be True if z_phot matches a neighbor\n",
    "        percent_correct_by_n_zp.append(z_phot_match_correct[n].sum() / len(ztruth))\n",
    "        cumulative_percent_correct_by_n_zp.append(any_neighbor_zp_correct.sum() / len(ztruth))\n",
    "\n",
    "\n",
    "        # Now only consider the closest neighbor, by photo-z matching\n",
    "        delta_z[n] = zphot - z_neighbor\n",
    "        best_match_idx = np.argmin(delta_z[0:n+1], axis=0, keepdims=True)\n",
    "        closest_delta_z_correct = np.take_along_axis(z_phot_match_correct, best_match_idx, axis=0)[0] # will be True bset zphot match is correct\n",
    "        cumulative_percent_correct_by_n_zp_closestonly.append(closest_delta_z_correct.sum() / len(ztruth))\n",
    "\n",
    "        # Now only consider the first neighbor in order of angular distance\n",
    "        z_phot_first_neighbor_match_idx = np.minimum(z_phot_first_neighbor_match_idx, np.where(z_phot_neighbor_match[n],n,999))\n",
    "        has_match = z_phot_first_neighbor_match_idx != 999\n",
    "\n",
    "        # if z_phot_first_neighbor_match_idx is 999 for  row, first_correct will be False\n",
    "        # if z_phot_first_neighbor_match_idx is an index for row, first_correct will be z_phot_match_correct value for that row at that index\n",
    "        first_correct = np.repeat(False, len(ztruth))\n",
    "        first_correct[has_match] = z_phot_match_correct[z_phot_first_neighbor_match_idx[has_match], np.arange(len(ztruth))[has_match]]\n",
    "        cumulative_percent_correct_by_n_zp_firstonly[n,i] = (first_correct.sum() / len(ztruth))\n",
    "\n",
    "        first_matched_but_incorrect.append(cumulative_percent_z_phot_neighbor_match[n] - cumulative_percent_correct_by_n_zp_firstonly[n,i])\n",
    "\n",
    "    if i == 26:\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(range(1, NUM_NEIGHBORS+1), percent_correct_by_n, label=\"This Neighbor is correct z\")\n",
    "        plt.plot(range(1, NUM_NEIGHBORS+1), percent_correct_by_n_zp, label=\"This Neighbor photo-z matched and correct (thresh={PHOTOZ_MATCHING_THRESHOLD:.3})\")\n",
    "        plt.plot(range(1, NUM_NEIGHBORS+1), cumulative_percent_correct_by_n, label=\"Any Neighbor has correct z\")\n",
    "        plt.plot(range(1, NUM_NEIGHBORS+1), cumulative_percent_correct_by_n_zp, label=\"Any neighbor photo-z matched and correct (thresh={PHOTOZ_MATCHING_THRESHOLD:.3})\")\n",
    "        plt.plot(range(1, NUM_NEIGHBORS+1), cumulative_percent_correct_by_n_zp_closestonly, label=\"Minimum delta-z matched neighbor correct (thresh={PHOTOZ_MATCHING_THRESHOLD:.3})\")\n",
    "        plt.plot(range(1, NUM_NEIGHBORS+1), cumulative_percent_correct_by_n_zp_firstonly[:,i], label=f\"First photo-z matched neighbor correct (thresh={PHOTOZ_MATCHING_THRESHOLD:.3})\")\n",
    "        plt.plot(range(1, NUM_NEIGHBORS+1), cumulative_percent_z_phot_neighbor_match, label=\"Any Neighbor photo-z matched (thresh={PHOTOZ_MATCHING_THRESHOLD:.3})\")\n",
    "        plt.ylabel(\"Fraction\")\n",
    "        plt.xlabel(\"Nth Nearest Neighbor\")\n",
    "        plt.xticks(np.arange(1, NUM_NEIGHBORS+1))\n",
    "        plt.ylim(0, 0.8)\n",
    "        #plt.axhline(y=0.0125, color='r', linestyle='--', label=\"Random Chance\")\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.figure(figsize=(10, 5))\n",
    "        # stacked bar chart of \n",
    "        plt.bar(range(1, NUM_NEIGHBORS+1), cumulative_percent_correct_by_n_zp_firstonly[:,i], label=f\"First photo-z matched neighbor correct (thresh={PHOTOZ_MATCHING_THRESHOLD:.3})\")\n",
    "        plt.bar(range(1, NUM_NEIGHBORS+1), first_matched_but_incorrect, label=f\"First photo-z matched neighbor incorrect (thresh={PHOTOZ_MATCHING_THRESHOLD:.3})\", bottom = cumulative_percent_correct_by_n_zp_firstonly[:,i])\n",
    "\n",
    "        plt.ylabel(\"Fraction\")\n",
    "        plt.xlabel(\"Nth Nearest Neighbor\")\n",
    "        plt.xticks(np.arange(1, NUM_NEIGHBORS+1))\n",
    "        plt.ylim(0, 1.0)\n",
    "        plt.legend()\n",
    "    \n",
    "    i = i + 1\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the maximum point\n",
    "best = np.max(cumulative_percent_correct_by_n_zp_firstonly)\n",
    "best_idx = np.unravel_index(np.argmax(cumulative_percent_correct_by_n_zp_firstonly), cumulative_percent_correct_by_n_zp_firstonly.shape)\n",
    "print(cumulative_percent_correct_by_n_zp_firstonly.shape)\n",
    "print(best_idx)\n",
    "print(f\"Best photo-z match threshold: {paramspace[best_idx[1]]:.3f}, Neighbors={best_idx[0]+1}, {best:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, NUM_NEIGHBORS+1), cumulative_percent_correct_by_n_zp_firstonly[:,4],        label=f\"thresh={paramspace[4]:.3})\", color=[0.1, 0.8, 0])\n",
    "plt.plot(range(1, NUM_NEIGHBORS+1), cumulative_percent_correct_by_n_zp_firstonly[:,9],        label=f\"thresh={paramspace[9]:.3})\", color=[0.2, 0.7, 0])\n",
    "plt.plot(range(1, NUM_NEIGHBORS+1), cumulative_percent_correct_by_n_zp_firstonly[:,19],       label=f\"thresh={paramspace[19]:.3})\", color=[0.3, 0.6, 0])\n",
    "plt.plot(range(1, NUM_NEIGHBORS+1), cumulative_percent_correct_by_n_zp_firstonly[:,26], '--', label=f\"thresh={paramspace[26]:.3})\", color=[0.4, 0.5, 0])\n",
    "plt.plot(range(1, NUM_NEIGHBORS+1), cumulative_percent_correct_by_n_zp_firstonly[:,39],       label=f\"thresh={paramspace[39]:.3})\", color=[0.5, 0.4, 0])\n",
    "plt.plot(range(1, NUM_NEIGHBORS+1), cumulative_percent_correct_by_n_zp_firstonly[:,49],       label=f\"thresh={paramspace[49]:.3})\", color=[0.6, 0.3, 0])\n",
    "plt.plot(range(1, NUM_NEIGHBORS+1), cumulative_percent_correct_by_n_zp_firstonly[:,59],       label=f\"thresh={paramspace[59]:.3})\", color=[0.7, 0.2, 0])\n",
    "plt.plot(range(1, NUM_NEIGHBORS+1), cumulative_percent_correct_by_n_zp_firstonly[:,99],       label=f\"thresh={paramspace[99]:.3})\", color=[0.8, 0.1, 0])\n",
    "plt.ylabel(\"Fraction Correct\")\n",
    "plt.xlabel(\"Nth Nearest Neighbor\")\n",
    "plt.xticks(np.arange(1, NUM_NEIGHBORS+1))\n",
    "plt.ylim(0, 0.45)\n",
    "#plt.axhline(y=0.0125, color='r', linestyle='--', label=\"Random Chance\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a figure showing the best photo-z match for each galaxy\n",
    "plt.figure(figsize=(10, 5))\n",
    "for i in range(0, len(paramspace)):\n",
    "    color = [i/(len(paramspace)+1),0.5,0]\n",
    "    plt.plot(range(1, NUM_NEIGHBORS+1), cumulative_percent_correct_by_n_zp_firstonly[:,i], label=f\"Photo-z Match Threshold {paramspace[i]:.3f}\", color=color)\n",
    "plt.ylabel(\"Fraction Correct\")\n",
    "plt.xlabel(\"Nth Nearest Neighbor\")\n",
    "plt.xticks(np.arange(1, NUM_NEIGHBORS+1))\n",
    "plt.ylim(0, 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Magnitudes, k-corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Projected Clustering measurements are taken using the FSF mags\n",
    "# We want to use those for spectroscopic. For lost galaxies, we use the polynomial fit ones. \n",
    "# It looks like there are some systematic differences but we can live with it.\n",
    "\n",
    "df['MAG_DELTA'] = df['ABS_MAG_R_K_BEST'] - df['ABS_MAG_R_K'] # should be FSF ones minus polynomial ones\n",
    "df_obs = df.loc[df['UNOBSERVED'] == False]\n",
    "\n",
    "print(len(df_obs.loc[df_obs['MAG_DELTA'] == 0])) # should be ~0\n",
    "\n",
    "junk=plt.hist(df_obs['MAG_DELTA'], bins=100, range=(-0.4, 0.4), density=True, histtype='step', label='MAG_DELTA')\n",
    "plt.xlabel(\"MAG_DELTA (FSF - Polynomial)\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Distribution of MAG_DELTA for Observed Galaxies\")\n",
    "plt.grid()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logLgal_bin_idx = np.digitize(df['LOG_L_GAL'], BGS_LOGLGAL_BINS)\n",
    "num_bins = len(BGS_LOGLGAL_BINS) - 1\n",
    "# 0 is less than the lowest, len(BGS_LOGLGAL_BINS) is greater than the highest entry in BGS_LOGLGAL_BINS\n",
    "\n",
    "print(np.array(BGS_LOGLGAL_BINS))\n",
    "\n",
    "# Calculate the mean value of LOGLGAL in each bin\n",
    "mean_logLgal_per_bin = []\n",
    "for i in range(1, len(BGS_LOGLGAL_BINS)):\n",
    "    bin_values = df['LOG_L_GAL'][logLgal_bin_idx == i]\n",
    "    mean_logLgal_per_bin.append(np.mean(bin_values))\n",
    "\n",
    "# Print the mean values\n",
    "for i, mean_value in enumerate(mean_logLgal_per_bin, 1):\n",
    "    print(f\"Mean LOGLGAL in bin {i}: {mean_value:.2f} (N= {len(df['LOG_L_GAL'][logLgal_bin_idx == i]):,})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It doesn't matter if you use g-r or G-R as the difference between the is the same (before k-corrections of course)\n",
    "abs_mag_r_NEWCOSMO = app_mag_to_abs_mag(df['APP_MAG_R'], df['Z'])\n",
    "abs_mag_g_NEWCOSMO = app_mag_to_abs_mag(df['APP_MAG_G'], df['Z'])\n",
    "gmra = df['APP_MAG_G'] - df['APP_MAG_R']\n",
    "\n",
    "Gk = k_correct_bgs(abs_mag_g_NEWCOSMO, df['Z'], gmra, df['PHOTSYS'], band='g')\n",
    "Rk = k_correct_bgs(abs_mag_r_NEWCOSMO, df['Z'], gmra, df['PHOTSYS'], band='r')\n",
    "G_R_k_BGS1 = Gk - Rk\n",
    "\n",
    "Gk_GAMA = k_correct_gama(abs_mag_g_NEWCOSMO, df['Z'], gmra, band='g')\n",
    "Rk_GAMA = k_correct_gama(abs_mag_r_NEWCOSMO, df['Z'], gmra, band='r')\n",
    "G_R_k_GAMA = Gk_GAMA - Rk_GAMA\n",
    "\n",
    "Rk_lookup, Gk_lookup = k_correct_fromlookup(abs_mag_r_NEWCOSMO, abs_mag_g_NEWCOSMO, df['Z'])\n",
    "\n",
    "#Gk_BGS2 = k_correct_bgs_v2(abs_mag_g, z_obs, g_r_apparent, band='g')\n",
    "#Rk_BGS2 = k_correct_bgs_v2(abs_mag_r, z_obs, g_r_apparent, band='r')\n",
    "#G_R_k_BGS2 = Gk_BGS2 - Rk_BGS2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison of g-r computed a few ways\n",
    "bins = np.linspace(-0.3, 2.0, 200)\n",
    "\n",
    "# TODO look at published paper and see how that does\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "#junk=plt.hist(g_r_apparent, bins=bins, label=\"g-r\", histtype='step', density=True)\n",
    "#junk=plt.hist(sdss_g_r, bins=bins, label='From LSS Pipeline (JM?)', histtype='step', density=True)\n",
    "junk=plt.hist(df['ABS_MAG_G'] - df['ABS_MAG_R'], bins=bins, label=\"(G-R) Old Raw\", histtype='step', density=True)\n",
    "junk=plt.hist(abs_mag_g_NEWCOSMO - abs_mag_r_NEWCOSMO, bins=bins, label=\"(G-R) Raw\", histtype='step', density=True)\n",
    "junk=plt.hist(G_R_k_BGS1, bins=bins, label=\"(G-R)^0.1 BGS poly v1\", histtype='step', density=True)\n",
    "junk=plt.hist(G_R_k_GAMA, bins=bins, label=\"(G-R)^0.1 GAMA poly\", histtype='step', density=True)\n",
    "junk=plt.hist(df['ABSMAG01_SDSS_G'] - df['ABSMAG01_SDSS_R'], bins=bins, label=\"(G-R)^0.1 FSF\", histtype='step', density=True)\n",
    "junk=plt.hist(Gk_lookup - Rk_lookup, bins=bins, label=\"(G-R)^0.1 From Lookup\", histtype='step', density=True)\n",
    "\n",
    "#junk=plt.hist(G_R_k_BGS2, bins=bins, label=\"0.1^(G-R) BGS poly v2\", histtype='step', density=True)\n",
    "plt.xlabel(\"g-r\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.legend()\n",
    "plt.xlim(0.1, 1.5)\n",
    "plt.title(\"Comparison of g-r computed a few ways\")\n",
    "plt.tight_layout()\n",
    "plt.ylim(0,3.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show Abs mag R distribution \n",
    "magbins = np.linspace(-25, -10, 100)\n",
    "plt.figure(figsize=(8,6))\n",
    "junk=plt.hist(df['ABS_MAG_R'], bins=magbins, histtype='step', label='Old, no k', density=True)\n",
    "junk=plt.hist(abs_mag_r_NEWCOSMO, bins=magbins, histtype='step', label='Correct, no k', density=True)\n",
    "junk=plt.hist(Rk, bins=magbins, histtype='step', label='Correct, with BGS k', density=True)\n",
    "junk=plt.hist(Rk_GAMA, bins=magbins, histtype='step', label='Correct, with GAMA k', density=True)\n",
    "junk=plt.hist(df['ABSMAG01_SDSS_R'], bins=magbins, histtype='step', label='FSF', density=True)\n",
    "junk=plt.hist(Rk_lookup, bins=magbins, histtype='step', label='From Lookup', density=True)\n",
    "plt.legend()\n",
    "plt.title(\"R band Absolute Magnitude Histogram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate differences from FSF magnitudes (ABSMAG01_SDSS_R) for each method\n",
    "diff_old_raw = df['ABS_MAG_R'] - df['ABSMAG01_SDSS_R']\n",
    "diff_new_raw = abs_mag_r_NEWCOSMO - df['ABSMAG01_SDSS_R']\n",
    "diff_bgs_poly = Rk - df['ABSMAG01_SDSS_R']\n",
    "diff_gama_poly = Rk_GAMA - df['ABSMAG01_SDSS_R']\n",
    "diff_lookup = Rk_lookup - df['ABSMAG01_SDSS_R']\n",
    "\n",
    "# Create histogram\n",
    "bins = np.linspace(-2, 2, 200)\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.hist(diff_bgs_poly, bins=bins, alpha=0.6, label='BGS poly k-corr - FSF', density=True, histtype='step')\n",
    "plt.hist(diff_gama_poly, bins=bins, alpha=0.6, label='GAMA poly k-corr - FSF', density=True, histtype='step')\n",
    "plt.hist(diff_lookup, bins=bins, alpha=0.6, label='Lookup k-corr - FSF', density=True, histtype='step')\n",
    "\n",
    "plt.xlabel('$\\Delta M_r$ (Method - ABSMAG01_SDSS_R)')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Difference in R-band Absolute Magnitude from FSF Method')\n",
    "plt.legend()\n",
    "plt.xlim(-1.0, 1.0)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"Summary statistics (Method - ABSMAG01_SDSS_R):\")\n",
    "print(f\"BGS poly: mean={np.mean(diff_bgs_poly):.4f}, median={np.median(diff_bgs_poly):.4f}, std={np.std(diff_bgs_poly):.4f}\")\n",
    "print(f\"GAMA poly: mean={np.mean(diff_gama_poly):.4f}, median={np.median(diff_gama_poly):.4f}, std={np.std(diff_gama_poly):.4f}\")\n",
    "print(f\"Lookup: mean={np.mean(diff_lookup):.4f}, median={np.median(diff_lookup):.4f}, std={np.std(diff_lookup):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show Abs mag G distribution \n",
    "magbins = np.linspace(-25, -10, 100)\n",
    "plt.figure(figsize=(8,6))\n",
    "junk=plt.hist(df['ABS_MAG_G'], bins=magbins, histtype='step', label='Old, no k', density=True)\n",
    "junk=plt.hist(abs_mag_g_NEWCOSMO, bins=magbins, histtype='step', label='Correct, no k', density=True)\n",
    "junk=plt.hist(Gk, bins=magbins, histtype='step', label='Correct, with BGS k', density=True)\n",
    "junk=plt.hist(Gk_GAMA, bins=magbins, histtype='step', label='Correct, with GAMA k', density=True)\n",
    "junk=plt.hist(df['ABSMAG01_SDSS_G'], bins=magbins, histtype='step', label='FSF', density=True)\n",
    "junk=plt.hist(Gk_lookup, bins=magbins, histtype='step', label='From Lookup', density=True)\n",
    "plt.legend()\n",
    "plt.title(\"G band Absolute Magnitude Histogram\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can see global GLOBAL_RED_COLOR_CUT=0.76 here\n",
    "junk=plt.hist(G_R_k_GAMA, bins=300, alpha=0.5, label=\"0.1^(G-R) GAMA-style\")\n",
    "junk=plt.hist(G_R_JM1, bins=300, alpha=0.5, label=\"0.1^(G-R) JM\")\n",
    "plt.legend()\n",
    "plt.xlim(0.5, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(is_quiescent_lost_gal_guess(g_r_apparent).sum() / len(g_r_apparent))\n",
    "assert len(G_R_k_GAMA) == len(g_r_apparent)\n",
    "print(is_quiescent_BGS_gmr(None, G_R_k_GAMA).sum() / len(G_R_k_GAMA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyutils import *\n",
    "print(BGS_LOGLGAL_BINS)\n",
    "print(BINWISE_RED_COLOR_CUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_quiescent_BGS_gmr(np.array([5.8, 9.0, 14.5]), np.array([0.5, 0.9, 0.9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get logLgal bins\n",
    "log_L_gal = abs_mag_r_to_log_solar_L(Rk) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define 8 bins that have equal number of galaxies\n",
    "log_L_gal_bins = np.percentile(log_L_gal, np.linspace(0, 100, 9))\n",
    "print(log_L_gal_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.min(log_L_gal))\n",
    "print(np.max(log_L_gal))\n",
    "print(np.min(logLgal_bin_idx))\n",
    "print(np.max(logLgal_bin_idx))\n",
    "plt.hist(log_L_gal, bins=BGS_LOGLGAL_BINS, align='mid')\n",
    "#plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a plot of G_R_k in each logLgal bin\n",
    "for i in range(0, len(BGS_LOGLGAL_BINS)+1):\n",
    "    galaxy_idx_for_this_bin = logLgal_bin_idx == i\n",
    "\n",
    "    plt.figure(dpi=80, figsize=(5, 3))\n",
    "    junk=plt.hist(G_R_k_GAMA[galaxy_idx_for_this_bin], bins=np.arange(0,1.3,0.02), label=f\"0.1^(G-R) Bin {i}\", align='mid')\n",
    "    plt.legend()\n",
    "    plt.xlim(0.4, 1.2)\n",
    "    plt.xticks(np.arange(0.4, 1.2, 0.04))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quiescent vs Star-Forming Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dn4000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DN4000_MODEL take a look\n",
    "plt.figure()\n",
    "junk=plt.hist(df['DN4000_MODEL'], bins=np.linspace(1, 2.5, 200), histtype='step')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('DN4000_MODEL')\n",
    "plt.ylabel('Count')\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the GMM and plot the results\n",
    "good = ~np.isnan(df['DN4000_MODEL'])\n",
    "values = [1.47, 1.5, 1.55, 1.57, 1.61, 1.63, 1.65, 1.67, 1.68, 1.69] # Chosen from past runs\n",
    "#intersections = fit_gmm_and_plot(df.loc[good, 'DN4000_MODEL'].to_numpy(), logLgal_bin_idx[good], num_bins, 1.0, 2.2, 'Dn4000', means_init=[[1.2], [1.8]], manual_thresholds=values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import erf\n",
    "from scipy.optimize import curve_fit\n",
    "import numpy as np\n",
    "\n",
    "# Use the intersection if the GMM is a great fit, otherwise need to adjust a little by eye.\n",
    "\n",
    "# Define the fitting function\n",
    "def fitting_function(logLgal, A, B, C, D):\n",
    "    return A + B*(1 + erf((logLgal - C) / D))\n",
    "\n",
    "priors = [1.42, .175, 9.9, 0.8]\n",
    "# Fit the function to the midpoints\n",
    "popt, pcov = curve_fit(fitting_function, mean_logLgal_per_bin, values, p0=priors, maxfev=10000)\n",
    "\n",
    "print(f\"Fitted parameters: A={popt[0]:.3f}, B={popt[1]:.3f}, C={popt[2]:.3f}, D={popt[3]:.3f}\")\n",
    "\n",
    "# Plot the results\n",
    "# INcrease fontsize a bit\n",
    "plt.figure(figsize=(8, 6), dpi=DPI_PAPER)\n",
    "plt.plot(mean_logLgal_per_bin, values, 'o', label='Thresholds Chosen')\n",
    "x = np.linspace(6.5, 11.5, 1000)\n",
    "y = fitting_function(x, *popt)\n",
    "plt.plot(x, get_Dn4000_crit(x), label='Fitted Threshold Used Here')\n",
    "plt.plot(x, get_SDSS_Dcrit(x), label='Tinker 2021 SDSS Threshold')\n",
    "#plt.plot(x, y, '--', label='Newly Fitted Function')\n",
    "plt.xlabel('log($L_{\\\\rm gal}) [L_\\odot h^{-2}]$')\n",
    "plt.ylabel('$D_n4000$ Threshold')\n",
    "#plt.title('Quiescent Classification')\n",
    "#plt.text(6.4, 1.675, f\"Uses DN4000_MODEL from fastspecfit\")\n",
    "#plt.text(6.4, 1.65, f\"$g-r$ < 0.65 always Star-Forming\")\n",
    "plt.legend()\n",
    "plt.twiny()\n",
    "plt.xlim(log_solar_L_to_abs_mag_r(6.5), log_solar_L_to_abs_mag_r(11.5))\n",
    "plt.xticks(np.arange(-23, -12, 2))\n",
    "plt.xlabel(\"$M_r$ - 5log(h)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HAlpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HALPHA_EW take a look\n",
    "plt.figure()\n",
    "junk=plt.hist(df['HALPHA_EW'], bins=np.logspace(-3, 3.0, 200), histtype='step')\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('HALPHA_EW')\n",
    "plt.ylabel('Count')\n",
    "plt.plot()\n",
    "\n",
    "# HALPHA_EW with inverse-variance weightings\n",
    "plt.figure()\n",
    "junk=plt.hist(df['HALPHA_EW'], bins=np.logspace(-3, 3.0, 200), histtype='step', weights=df['HALPHA_EW_IVAR'])\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('HALPHA_EW')\n",
    "plt.ylabel('Count')\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_halpha = np.log10(df['HALPHA_EW'])\n",
    "no_halpha =(c_halpha < -2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the GMM and plot the results\n",
    "c_halpha = np.log10(df['HALPHA_EW']).to_numpy()\n",
    "good = np.logical_and(~no_halpha, ~np.isnan(c_halpha))\n",
    "values = [0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55] # Chosen from past runs\n",
    "intersections = fit_gmm_and_plot(c_halpha[good],logLgal_bin_idx[good], num_bins, -1, 2.3, \"Halpha\", manual_thresholds=values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import erf\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "# Define the fitting function\n",
    "def fitting_function(logLgal, A, B, C, D):\n",
    "    return A + B*(1 + erf((logLgal - C) / D))\n",
    "\n",
    "priors = [0.9, -0.175, 9.9, 0.8]\n",
    "\n",
    "# Fit the function to the midpoints\n",
    "popt, pcov = curve_fit(fitting_function, mean_logLgal_per_bin, values, p0=priors, maxfev=10000)\n",
    "\n",
    "print(f\"Fitted parameters: A={popt[0]:.3f}, B={popt[1]:.3f}, C={popt[2]:.3f}, D={popt[3]:.3f}\")\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(dpi=80, figsize=(8, 6))\n",
    "plt.plot(mean_logLgal_per_bin, values, 'o', label='Midpoints')\n",
    "x = np.linspace(8.0, 11.3, 1000)\n",
    "y = fitting_function(x, *popt)\n",
    "z = get_halpha_crit(x)\n",
    "plt.plot(x, z, label='Saved Fitted Function')\n",
    "plt.plot(x, y, '--', label='New Fitted Function')\n",
    "plt.xlabel('log(L)')\n",
    "plt.ylabel('Log(Halpha) Threshold')\n",
    "plt.title(\"Halpha Threshold\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SSFR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SSFR take a look\n",
    "plt.figure()\n",
    "junk=plt.hist(df['SSFR'], bins=np.logspace(-14, -8.5, 200), histtype='step')\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('SSFR')\n",
    "plt.ylabel('Count')\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_ssfr = np.log10(df['SSFR']).to_numpy()\n",
    "no_ssfr = c_ssfr <-14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the GMM and plot the results\n",
    "good = np.logical_and(~no_ssfr, ~np.isnan(c_ssfr))\n",
    "intersectinos = fit_gmm_and_plot(c_ssfr[good], np.digitize(df['LOG_L_GAL'][good], BGS_LOGLGAL_BINS), num_bins, -14, -8.5, \"SSFR\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this case the GMM doesn't make as much sense Arjun says. Just use -11 as cut.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## g-r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GMR take a look\n",
    "plt.figure()\n",
    "junk=plt.hist(df['G_R_BEST'], bins=np.linspace(-2, 2, 200), histtype='step')\n",
    "plt.xlabel('G_R_BEST')\n",
    "plt.ylabel('Count')\n",
    "plt.yscale('log')\n",
    "plt.title(\"All Galaxies G-R\")\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_gmr = df['G_R_BEST'].to_numpy()\n",
    "good = ~np.isnan(c_gmr)\n",
    "good = np.logical_and(good, c_gmr < 1.4)\n",
    "# Fit the GMM and plot the results\n",
    "values = [0.72, 0.72, 0.73, 0.76, 0.80, 0.82, 0.83, 0.84, 0.85, 0.85] # Chosen from past runs\n",
    "values = None\n",
    "intersections = fit_gmm_and_plot(c_gmr[good], logLgal_bin_idx[good], num_bins, 0, 1.25, \"G-R\", manual_thresholds=values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the fitting function\n",
    "def fitting_function(logLgal, A, B, C, D):\n",
    "    return A + B*(1 + erf((logLgal - C) / D))\n",
    "\n",
    "priors = [0.42, .175, 9.9, 0.8]\n",
    "\n",
    "# Fit the function to the midpoints\n",
    "popt, pcov = curve_fit(fitting_function, mean_logLgal_per_bin, values, p0=priors, maxfev=10000)\n",
    "\n",
    "print(f\"Fitted parameters: A={popt[0]:.3f}, B={popt[1]:.3f}, C={popt[2]:.3f}, D={popt[3]:.3f}\")\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(dpi=80, figsize=(8, 6))\n",
    "plt.plot(mean_logLgal_per_bin, values, 'o', label='Midpoints')\n",
    "x = np.linspace(8.0, 11.3, 1000)\n",
    "y = fitting_function(x, *popt)\n",
    "plt.plot(x, y, label='Fitted Function')\n",
    "plt.plot(x, get_gmr_crit(x), label='Saved Fitted Function')\n",
    "plt.xlabel('logLgal')\n",
    "plt.ylabel('Bin Midpoint Value')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KMeans Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "# Trained with BGS BRIGHT OBSERVED 0.001 < z < 0.5 galaxies.\n",
    "# Unobserved don't use KMeans anyway. Don't want to train on FAINT since that's not our goal.\n",
    "#x, y, z, zz, classification, missing = is_quiescent_BGS_kmeans(df['LOG_L_GAL'].to_numpy(), df['DN4000_MODEL'].to_numpy(), df['HALPHA_EW'].to_numpy(), df['SSFR'].to_numpy(), df['G_R_BEST'].to_numpy())\n",
    "#df['KMEANS_LABEL'] = classification\n",
    "\n",
    "classification_gmronly = is_quiescent_BGS_gmr(df['LOG_L_GAL'].to_numpy(), df['G_R_BEST'].to_numpy())\n",
    "df['GMR_LABEL'] = classification_gmronly\n",
    "\n",
    "classification_dn4000 = is_quiescent_BGS_dn4000(df['LOG_L_GAL'].to_numpy(), df['DN4000_MODEL'].to_numpy(), df['G_R_BEST'].to_numpy())\n",
    "df['DN4000_LABEL'] = classification_dn4000\n",
    "\n",
    "#x, y, z, zz, classification, missing = is_quiescent_BGS_kmeans(df['LOG_L_GAL'].to_numpy(), df['DN4000_MODEL'].to_numpy(), df['HALPHA_EW'].to_numpy(), df['SSFR'].to_numpy(), df['G_R_BEST'].to_numpy(), model=QUIESCENT_MODEL)\n",
    "#df['KMEANS_LABEL'] = classification\n",
    "\n",
    "x, y, z, zz, classification, missing = is_quiescent_BGS_kmeans(df['LOG_L_GAL'].to_numpy(), df['DN4000_MODEL'].to_numpy(), df['HALPHA_EW'].to_numpy(), df['SSFR'].to_numpy(), df['G_R_BEST'].to_numpy(), model=QUIESCENT_MODEL_V2)\n",
    "df['KMEANS_LABEL_V2'] = classification\n",
    "\n",
    "#x, y, z, zz, classification, missing = is_quiescent_BGS_kmeans(df['LOG_L_GAL'].to_numpy(), df['DN4000_MODEL'].to_numpy(), df['HALPHA_EW'].to_numpy(), df['SSFR'].to_numpy(), df['G_R_BEST'].to_numpy(), model=QUIESCENT_MODEL_V1)\n",
    "#df['KMEANS_LABEL_V1'] = classification\n",
    "\n",
    "# Check that KMEANS_LABEL and KMEANS_LABEL_1 are exactly the same\n",
    "#print(np.sum(df['KMEANS_LABEL'] != df['KMEANS_LABEL_1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observed_df = df.loc[~unobserved]\n",
    "unobserved_df = df.loc[unobserved] # mostly don't have spectra (but some do with a bad z fit or something)\n",
    "#q_col = 'KMEANS_LABEL_V2'\n",
    "q_col = 'DN4000_LABEL'\n",
    "\n",
    "# Tests of results\n",
    "extreme_blue = df['G_R_BEST'] < 0.65\n",
    "blue_but_q = df.loc[extreme_blue, q_col].mean()\n",
    "assert blue_but_q < 0.01, f\"< 1% of very blue galaxies should be labeled as quiescent, but got {blue_but_q:.2%}\"\n",
    "\n",
    "heavyhalpha = df['HALPHA_EW'] > 25\n",
    "heavyhalpha_but_q = df.loc[heavyhalpha, q_col].mean()\n",
    "assert heavyhalpha_but_q < 0.01, f\"< 1% of very strong Halpha galaxies should be labeled as quiescent, but got {heavyhalpha_but_q:.2%}\"\n",
    "\n",
    "dusty_sf = (observed_df['G_R_BEST'] > 0.75) & (observed_df['G_R_BEST'] < 0.95) # Reddish\n",
    "dusty_sf = (dusty_sf & (observed_df['SSFR'] > 3E-11)) & ((observed_df['HALPHA_EW'] > 4) & (observed_df['DN4000_MODEL'] < 1.4)) # Lots of SF indicators\n",
    "dusty_sf_but_q = observed_df.loc[dusty_sf, q_col].mean()\n",
    "assert dusty_sf_but_q < 0.01, f\"< 1% of reddish galaxies with clear spectroscopic signals of star-formation should be labeled as quiescent, but got {dusty_sf_but_q:.2%}\"\n",
    "\n",
    "dim = df['LOG_L_GAL'] < 8.0\n",
    "dim_q = df.loc[dim, q_col].mean()\n",
    "assert dim_q < 0.1, f\"< 10% of galaxies at L < 10^8 should be labeled as quiescent, but got {dim_q:.2%}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(unobserved_df))\n",
    "print(len(observed_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot quiescent fraction as a function of logLgal\n",
    "plt.figure()\n",
    "for df_to_use in [observed_df]:\n",
    "    color = 'green' if df_to_use is observed_df else 'red'\n",
    "    label = 'Observed' if df_to_use is observed_df else 'Unobserved'\n",
    "    lbin = np.linspace(6, 11.4, 25)\n",
    "    lbin_idx = np.digitize(df_to_use['LOG_L_GAL'], lbin)\n",
    "    lbin_idx = np.clip(lbin_idx, 1, len(lbin)-1)\n",
    "    lbin_means = np.array([df_to_use['LOG_L_GAL'][lbin_idx == i].mean() for i in range(1, len(lbin))])\n",
    "    lbin_counts = np.array([df_to_use['LOG_L_GAL'][lbin_idx == i].count() for i in range(1, len(lbin))])\n",
    "    lbin_q = np.array([df_to_use['KMEANS_LABEL_V2'][lbin_idx == i].mean() for i in range(1, len(lbin))])\n",
    "    lbin_q_dn4000 = np.array([df_to_use['DN4000_LABEL'][lbin_idx == i].mean() for i in range(1, len(lbin))])\n",
    "    lbin_q_gmronly = np.array([df_to_use['GMR_LABEL'][lbin_idx == i].mean() for i in range(1, len(lbin))])\n",
    "    if df_to_use is unobserved_df:\n",
    "        plt.plot(lbin_means, lbin_q, color, linestyle=':', label=f'{label} KMeans (G-R only)')\n",
    "    else:\n",
    "        #plt.plot(lbin_means, lbin_q, color, linestyle='-', label=f\"{label} Kmeans\")\n",
    "        plt.plot(lbin_means, lbin_q_dn4000, color, linestyle='--', label=f'{label} Dn4000 Method')\n",
    "        plt.plot(lbin_means, lbin_q_gmronly, color, linestyle=':', label=f'{label} G-R Only')\n",
    "\n",
    "    print(lbin_counts)\n",
    "\n",
    "plt.xlabel('logLgal')\n",
    "plt.ylabel('Quiescent Fraction')\n",
    "plt.title(\"Quiescent Fraction as a function of logLgal\")\n",
    "plt.legend()\n",
    "plt.xlim(6.5, 11.4)\n",
    "plt.ylim(0.0, 0.9)\n",
    "plt.grid()\n",
    "plt.yticks(np.arange(0, 0.9, 0.1))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATE PLOTS OF KMEANS CLASSIFICATION IN THE MODIFIED FITTING SPACE\n",
    "def plot_density(x, y, q, xlabel, ylabel, scatter=False):\n",
    "    \"\"\"\n",
    "    Create a density plot for the given x and y properties, classified by the provided labels.\n",
    "\n",
    "    Parameters:\n",
    "    x (array-like): Data for the x-axis.\n",
    "    y (array-like): Data for the y-axis.\n",
    "    q (array-like): Boolean array for classification (True/False).\n",
    "    xlabel (str): Label for the x-axis.\n",
    "    ylabel (str): Label for the y-axis.\n",
    "    alpha (float): Transparency level for the points.\n",
    "    \"\"\"\n",
    "    if x is pd.Series:\n",
    "        x = x.to_numpy()\n",
    "    if y is pd.Series:\n",
    "        y = y.to_numpy()\n",
    "    if q is pd.Series:\n",
    "        q = q.to_numpy()\n",
    "\n",
    "    # First randomy downsample to 1500 points of each class\n",
    "    if len(x) > 4000:\n",
    "        x_q = x[q]\n",
    "        y_q = y[q]\n",
    "        x_sf = x[~q]\n",
    "        y_sf = y[~q]\n",
    "\n",
    "        if len(x_q) > 2000:\n",
    "            idx = np.random.choice(len(x_q), 2000, replace=False)\n",
    "            x_q = x_q[idx]\n",
    "            y_q = y_q[idx]\n",
    "\n",
    "        if len(x_sf) > 2000:\n",
    "            idx = np.random.choice(len(x_sf), 2000, replace=False)\n",
    "            x_sf = x_sf[idx]\n",
    "            y_sf = y_sf[idx]\n",
    "\n",
    "        x = np.concatenate((x_q, x_sf))\n",
    "        y = np.concatenate((y_q, y_sf))\n",
    "        q = np.concatenate((np.repeat(True, len(x_q)), np.repeat(False, len(x_sf))))\n",
    "\n",
    "    plt.figure()\n",
    "    sns.kdeplot(x=x, y=y, hue=q, fill=True, levels=10, palette=[\"blue\", \"red\"], alpha=0.7, bw_adjust=1.0)\n",
    "    plt.axhline(0.0, color='k', linestyle='--')\n",
    "    plt.axvline(0.0, color='k', linestyle='--')\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.show()\n",
    "\n",
    "    if scatter:\n",
    "        plt.figure()\n",
    "        plt.scatter(x_q, y_q, c='red', alpha=0.15, label='Q')\n",
    "        plt.scatter(x_sf, y_sf, c='blue', alpha=0.15, label='SF')\n",
    "        plt.xlabel(xlabel)\n",
    "        plt.ylabel(ylabel)\n",
    "        plt.axhline(0.0, color='k', linestyle='--')\n",
    "        plt.axvline(0.0, color='k', linestyle='--')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "# Plot each combination of x, y, z, zz\n",
    "#dim = df['LOG_L_GAL'][~dead] < 9.0\n",
    "#arrays = [x[dim], y[dim], z[dim], zz[dim]]\n",
    "arrays = [x, y, z, zz]\n",
    "names = ['~Dn4000', '~log(Halpha)', '~log(SSFR)', '~G-R']\n",
    "for i in range(len(arrays)):\n",
    "    for j in range(i+1, len(arrays)):\n",
    "        plot_density(arrays[i], arrays[j], classification[~missing].astype(bool), names[i], names[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHOW RESULTS PLOTS OF KMEANS CLASSIFICATION IN THE SPACE OF THE ORIGINAL VARIABLES\n",
    "def quiescent_classification_plots(df):\n",
    "    alpha = 0.7\n",
    "\n",
    "    # Downsample the data to make it easier to plot\n",
    "    df_q = df.loc[df['KMEANS_LABEL'] != 0]\n",
    "    df_sf = df.loc[df['KMEANS_LABEL'] == 0]\n",
    "    idx_q = np.random.choice(len(df_q), np.min([1500, len(df_q)]), replace=False)\n",
    "    idx_sf = np.random.choice(len(df_sf), np.min([1500, len(df_sf)]), replace=False)\n",
    "    idx = np.concatenate((idx_q, idx_sf))\n",
    "    sample = df.iloc[idx]\n",
    "    colors = np.where(sample['KMEANS_LABEL'] == 0, 0, 1) # darkred => red\n",
    "    #colors = np.where(sample['KMEANS_LABEL'] == 2, 'darkred', colors)\n",
    "\n",
    "    names = ['DN4000', 'HALPHA_EW', 'SSFR', 'G_R_BEST']\n",
    "    for i in range(len(names)):\n",
    "        for j in range(i+1, len(names)):\n",
    "            plt.figure()\n",
    "            sns.kdeplot(\n",
    "                x=sample[names[i]], \n",
    "                y=sample[names[j]], \n",
    "                hue=colors, \n",
    "                fill=True, \n",
    "                levels=10, \n",
    "                palette=[\"blue\", \"red\"], \n",
    "                alpha=alpha,\n",
    "            )\n",
    "            if names[i] == 'HALPHA_EW':\n",
    "                #plt.xscale('log')\n",
    "                #plt.xlim(0.001, 100)\n",
    "                plt.xlim(0, 60)\n",
    "            if names[j] == 'HALPHA_EW':\n",
    "                #plt.yscale('log')\n",
    "                #plt.ylim(0.001, 100)\n",
    "                plt.ylim(0, 60)\n",
    "            if names[i] == 'SSFR':\n",
    "                plt.xscale('log')\n",
    "                plt.xlim(1e-14, 1e-9)\n",
    "            if names[j] == 'SSFR':\n",
    "                plt.yscale('log')\n",
    "                plt.ylim(1e-14, 1e-9)\n",
    "            if names[i] == 'G_R_BEST':\n",
    "                plt.xlim(0, 1.5)\n",
    "            if names[j] == 'G_R_BEST':\n",
    "                plt.ylim(0, 1.5)\n",
    "            if names[i] == 'DN4000':\n",
    "                plt.xlim(1.0, 2.5)\n",
    "            if names[j] == 'DN4000':\n",
    "                plt.ylim(1.0, 2.5)\n",
    "            plt.plot()\n",
    "\n",
    "quiescent_classification_plots(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiescent_classification_plots(df.loc[df['LOG_L_GAL'] < 9.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_df_quiescence_to_sdss(df, 'DN4000_LABEL')\n",
    "#compare_df_quiescence_to_sdss(df, 'QUIESCENT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Randoms Analysis for Footprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Broken?\n",
    "build_sv3_clustering_randoms_files()\n",
    "build_y3_likesv3_clustering_randoms_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_sv3_full_randoms_files()\n",
    "build_y1_randoms_files()\n",
    "build_y3_randoms_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv3_randoms = pickle.load(open(MY_RANDOMS_SV3_MINI, 'rb'))\n",
    "tiles_BGS = read_tiles_Y3_sv3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the randoms positions\n",
    "fig=make_map(sv3_randoms['RA'].to_numpy(), sv3_randoms['DEC'].to_numpy())\n",
    "#make_map(ra, dec, fig=fig, alpha=0.05)\n",
    "\n",
    "\n",
    "# Some zoom-ins\n",
    "\n",
    "# These are the two regions we cut due to poor Y3 overlap\n",
    "#plot_positions(randoms_df0, randoms_df0[randoms_df0.NTILE_MINE >= 10], tiles_df=tiles_BGS, DEG_LONG=7, ra_min=192, dec_min=23, split=False)\n",
    "\n",
    "plot_positions(sv3_randoms, sv3_randoms[sv3_randoms.NTILE_MINE >= 10], DEG_LONG=5, ra_min=213, dec_min=50, split=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pickle.load(open(RANDOMS_Y1_0_WITHMYNTILE, 'rb'))\n",
    "# filter to NTILE_MINE >= 3\n",
    "df = df.loc[df.NTILE_MINE >= 3]\n",
    "fig=make_map(df['RA'].to_numpy(), df['DEC'].to_numpy())\n",
    "# Save as a FITS file\n",
    "t = Table.from_pandas(df)\n",
    "t.write(OUTPUT_FOLDER + \"FOOTPRINT_MATCHED_RANDOMS_0_Y1.fits\", format='fits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pickle.load(open(RANDOMS_Y3_0_WITHMYNTILE, 'rb'))\n",
    "# filter to NTILE_MINE >= 3\n",
    "df = df.loc[df.NTILE_MINE >= 3]\n",
    "fig=make_map(df['RA'].to_numpy(), df['DEC'].to_numpy())\n",
    "# Save as a FITS file\n",
    "t = Table.from_pandas(df)\n",
    "t.write(OUTPUT_FOLDER + \"FOOTPRINT_MATCHED_RANDOMS_0_Y3.fits\", format='fits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from footprintmanager import FootprintManager\n",
    "fp_manager = FootprintManager()\n",
    "#fp_manager.reload_all() # If randoms have changed\n",
    "\n",
    "def frac_sky_from_randoms(survey):\n",
    "    for i in range(1, 11):\n",
    "        footprint_all = fp_manager.get_footprint(survey, 'all', i)\n",
    "        footprint_N = fp_manager.get_footprint(survey, 'N', i)\n",
    "        footprint_S = fp_manager.get_footprint(survey, 'S', i)\n",
    "        print(f\"{i}-pass: {footprint_all/DEGREES_ON_SPHERE:.4%} of sky. {footprint_all:.4f} deg (all), {footprint_N:.4f} deg (N), {footprint_S:.4f} deg (S)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CORRECT FOOTPRINTS\n",
    "# These results copied into the groupcatalog.py file footprints\n",
    "\n",
    "print(\"-=- SV3 18 regions -=-\")\n",
    "frac_sky_from_randoms('SV3-18')\n",
    "\n",
    "print(\"-=- SV3 20 regions -=-\")\n",
    "frac_sky_from_randoms('SV3-20')\n",
    "\n",
    "print(\"-=- Y1 -=-\")\n",
    "frac_sky_from_randoms('Y1')\n",
    "\n",
    "print(\"-=- Y3 -=-\")\n",
    "frac_sky_from_randoms('Y3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fluxlim Correction Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitsio\n",
    "\n",
    "# Use the BGS Luminosity Function from the Nordberg group\n",
    "data = fitsio.read(BGS_Y3_FOLDER_LOA + 'Y3_z0P002_0P6_Q0P78_LF_N_all_rband.fits')\n",
    "df_N = pd.DataFrame(data.byteswap().newbyteorder())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = df_N[df_N['LOG_PHI'] > -9.9]\n",
    "y_err = plot_df['LOG_PHI'] - plot_df['LOG_PHI_LOW'], plot_df['LOG_PHI_HI'] - plot_df['LOG_PHI']\n",
    "\n",
    "def get_log_phi(abs_mag):\n",
    "    \"\"\"\n",
    "    Looks up the log(Phi) value for a given absolute magnitude from the BGS Y3 N LF data,\n",
    "    using linear interpolation. Throws an error if the magnitude is outside the data range.\n",
    "\n",
    "    Parameters:\n",
    "    abs_mag (float or array-like): The absolute magnitude(s) to look up.\n",
    "\n",
    "    Returns:\n",
    "    float or array-like: The interpolated log(Phi) value(s).\n",
    "    \"\"\"\n",
    "    # np.interp requires x-coordinates to be increasing. df_N['BIN_CEN'] is decreasing.\n",
    "    xp = df_N['BIN_CEN'].values\n",
    "    fp = df_N['LOG_PHI'].values\n",
    "\n",
    "    # Check if the input is outside the valid range\n",
    "    if np.any(abs_mag < -24.5) or np.any(abs_mag > -10.0):\n",
    "        raise ValueError(f\"Absolute magnitude must be between {xp[0]:.2f} and {xp[-1]:.2f}.\")\n",
    "\n",
    "    return np.interp(abs_mag, xp, fp)\n",
    "\n",
    "# Plot to show the interpolation\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.errorbar(plot_df['BIN_CEN'], plot_df['LOG_PHI'], yerr=y_err, fmt='.', capsize=3, label='BGS Y3 N LF Data')\n",
    "test_mags = np.linspace(-24.5, -10.0, 100)\n",
    "interpolated_log_phi = get_log_phi(test_mags)\n",
    "plt.plot(test_mags, interpolated_log_phi, 'r--', label='Linear Interpolation')\n",
    "plt.xlabel('Absolute Magnitude (r-band)')\n",
    "plt.ylabel('log($\\Phi$ / Mpc$^{-3}$ mag$^{-1}$)')\n",
    "plt.title('BGS Luminosity Function and Interpolation')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def current_fluxlim_correction(z):\n",
    "    return np.power(10.0, np.power(z / 0.40, 4.0) * 0.4); # from rho_lum(z) BGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.cosmology import FlatLambdaCDM\n",
    "from scipy.integrate import quad\n",
    "\n",
    "# --- 1. Setup Cosmology and Constants ---\n",
    "# Use a standard cosmology. You can adjust this if your LF was derived with different parameters.\n",
    "cosmo = FlatLambdaCDM(H0=70, Om0=0.3)\n",
    "\n",
    "# --- 2. Define the Integrand ---\n",
    "# The function we want to integrate is the number density Phi(M) times the luminosity L(M).\n",
    "def luminosity_integrand(M, get_log_phi_func):\n",
    "    \"\"\"\n",
    "    Returns Phi(M) * L(M) for integration.\n",
    "    M: Absolute magnitude\n",
    "    get_log_phi_func: The function that returns log10(Phi) for a given M.\n",
    "    \"\"\"\n",
    "    log_phi = get_log_phi_func(M)\n",
    "    phi = 10**log_phi\n",
    "    lum = abs_mag_r_to_solar_L(M)\n",
    "    return phi * lum\n",
    "\n",
    "# --- 3. Perform the Integration ---\n",
    "# Define the magnitude range for integration. This should cover the entire range of your LF data.\n",
    "m_min, m_max = -24.5, -10.0\n",
    "\n",
    "# Calculate the total possible luminosity density by integrating over all magnitudes.\n",
    "# The result of quad is (value, error_estimate). We only need the value.\n",
    "total_lum_density, _ = quad(luminosity_integrand, m_min, m_max, args=(get_log_phi,))\n",
    "\n",
    "# --- 4. Calculate Completeness vs. Redshift ---\n",
    "flux_limit = 19.5\n",
    "redshifts = np.linspace(0.001, 0.5, 50)\n",
    "lum_completeness = []\n",
    "factor_needed = []\n",
    "corrected_completeness = []\n",
    "\n",
    "for z in redshifts:\n",
    "    # Calculate luminosity distance in Mpc\n",
    "    #D_L_Mpc = cosmo.luminosity_distance(z).value\n",
    "    # Calculate distance modulus\n",
    "    dist_mod = distmod(z)\n",
    "    \n",
    "    # Find the absolute magnitude limit at this redshift for our flux limit\n",
    "    M_lim = flux_limit - dist_mod\n",
    "    M_lim = np.clip(M_lim, m_min, m_max)  # Ensure within LF range\n",
    "    \n",
    "    # Integrate luminosity down to the observable limit M_lim\n",
    "    # We integrate from m_min (brightest) to M_lim.\n",
    "    observed_lum_density, _ = quad(luminosity_integrand, m_min, M_lim, args=(get_log_phi,))\n",
    "    corrected = observed_lum_density * current_fluxlim_correction(z) / total_lum_density\n",
    "    \n",
    "    # The completeness is the ratio of observed to total luminosity\n",
    "    completeness = observed_lum_density / total_lum_density\n",
    "\n",
    "    factor_needed.append(total_lum_density / observed_lum_density)\n",
    "    lum_completeness.append(completeness)\n",
    "    corrected_completeness.append(corrected)\n",
    "\n",
    "lum_completeness = np.array(lum_completeness)\n",
    "factor_needed = np.array(factor_needed)\n",
    "corrected_completeness = np.array(corrected_completeness)\n",
    "\n",
    "# --- 5. Plot the Results ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(redshifts, lum_completeness, label=f'Luminosity Completeness (r < {flux_limit})')\n",
    "#plt.plot(redshifts, corrected_completeness, label='Corrected Completeness')\n",
    "plt.xlabel('Redshift (z)')\n",
    "plt.ylabel('Luminosity Completeness Fraction')\n",
    "plt.title('Fraction of Total Luminosity Observed vs. Redshift')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This shows that we are weakly dependent on the minimum magnitude limit chosen for the LF integration.\n",
    "\n",
    "min_mag = np.linspace(-22, -10, 50)\n",
    "# Plot the luminosity density as a function of min magnitude\n",
    "lum_densities = []\n",
    "for M_lim in min_mag:\n",
    "    lum_density, _ = quad(luminosity_integrand, m_min, M_lim, args=(get_log_phi,))\n",
    "    lum_densities.append(lum_density)   \n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(min_mag, lum_densities)\n",
    "plt.xlabel('Minimum Absolute Magnitude (r-band)')\n",
    "plt.ylabel('Luminosity Density [L_sun h^2 Mpc^-3]')\n",
    "plt.title('Luminosity Density vs. Minimum Absolute Magnitude')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the needed correction factor with a similar functional form\n",
    "from scipy.optimize import curve_fit    \n",
    "\n",
    "def fit_function(z, p1, p2, p3):\n",
    "    return np.power(10.0, np.power(z / p1, p2) * p3)\n",
    "\n",
    "# Perform the curve fitting\n",
    "popt, pcov = curve_fit(fit_function, redshifts, factor_needed, p0=[0.4, 4.0, 0.4], maxfev=10000)\n",
    "print(f\"Fitted parameters: p1={popt[0]:.3f}, p2={popt[1]:.3f}, p3={popt[2]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLot it\n",
    "plt.figure()\n",
    "plt.plot(redshifts, current_fluxlim_correction(redshifts), label='Current Flux Limit Correction')\n",
    "plt.plot(redshifts, factor_needed, label='Needed Correction Factor')\n",
    "plt.plot(redshifts, fit_function(redshifts, *popt), '--', label='Fitted Correction Factor')\n",
    "plt.xlabel('Redshift z')\n",
    "plt.ylabel('Flux Limit Correction Factor')\n",
    "plt.title('Current Flux Limit Correction as a function of Redshift')\n",
    "plt.grid()\n",
    "plt.axhline(1.0, color='k', linestyle='--')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Property Binned Clustering \n",
    "Look at galaxy properties to may want to calculate clustering based on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.isnan(df['SERSIC']).sum() == 0, \"SERSIC column has NaN values!\"\n",
    "assert np.isnan(df['SHAPE_R']).sum() == 0, \"SHAPE_R column has NaN values!\"\n",
    "nopsf = df.loc[df['SERSIC'] > 0]\n",
    "print(np.min(nopsf['SERSIC']), np.max(nopsf['SERSIC']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = df.loc[df['MORPHTYPE'] ==b'EXP']\n",
    "dev[['MORPHTYPE', 'SERSIC', 'SHAPE_R',  'Z', 'QUIESCENT', 'ABS_MAG_R_K_BEST']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reds = df.loc[df['QUIESCENT'] == 1]\n",
    "blues = df.loc[df['QUIESCENT'] == 0]\n",
    "reds_19 = reds.loc[reds['ABS_MAG_R_K_BEST'] < -19]\n",
    "reds_19 = reds_19.loc[reds_19['ABS_MAG_R_K_BEST'] > -20]\n",
    "blues_19 = blues.loc[blues['ABS_MAG_R_K_BEST'] < -19]\n",
    "blues_19 = blues_19.loc[blues_19['ABS_MAG_R_K_BEST'] > -20]\n",
    "print(len(reds_19), len(blues_19))\n",
    "\n",
    "plt.hist(reds_19['SERSIC'], bins=[0.5, 0.9, 1.1, 1.5, 2.5, 3.5, 4.5, 5.5, 6.01])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(blues['SERSIC'], bins=[0.5, 0.9, 1.1, 1.5, 2.5, 3.5, 4.5, 5.5, 6.01])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(blues_19['SERSIC'], bins=[0.5, 0.9, 1.1, 1.5, 2.5, 3.5, 4.5, 5.5, 6.01])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df['SHAPE_R'], bins=np.linspace(0, 10.0, 400))\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df['SHAPE_E1'], bins=np.linspace(0, 1.0, 100))\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "photo_table = Table.read(f'{IRON_PHOTO_VAC_ROOT}potential-targets/targetphot-potential-nside2-hp02-main-iron.fits', format='fits')\n",
    "\n",
    "print(photo_table.colnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will automaticalyl drop PSF galaxies from bin definitions. Thye have SERSIC=0\n",
    "# Bins: 0.5 to 2.0, 2.0 to 4.5, 4.5 to 6.0. So each big bump is different bin\n",
    "\n",
    "# Use Sersic Index"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
