{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as c\n",
    "from astropy.wcs import WCS\n",
    "import astropy.coordinates as coord\n",
    "import astropy.units as u\n",
    "import syslog\n",
    "import astropy.io.fits as fits\n",
    "import healpy as hp\n",
    "from astropy.table import Table,join,vstack,unique\n",
    "import types\n",
    "import requests\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "if './SelfCalGroupFinder/py/' not in sys.path:\n",
    "    sys.path.append('./SelfCalGroupFinder/py/')\n",
    "from pyutils import *\n",
    "from dataloc import *\n",
    "from photoz import *\n",
    "import groupcatalog as gc\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEEP_PASSES = 1\n",
    "APP_MAG_CUT = 19.5 # BGS BRIGHT, though 19.54 for some cameras. I don't know if FLUX_R has been corrected for this.\n",
    "#APP_MAG_CUT = 20.175 # BGS FAINT, and in SV3 it is 20.3 instead\n",
    "Z_MIN = 0.001\n",
    "Z_MAX = 0.8\n",
    "\n",
    "def get_app_mag(flux):\n",
    "    \"\"\"This converts nanomaggies into Pogson magnitudes\"\"\"\n",
    "    return 22.5 - 2.5*np.log10(flux)\n",
    "\n",
    "# This corresponds to 8.33 square degrees and empircally makes sense by looking at the randoms\n",
    "TILE_RADIUS = 5862.0 * u.arcsec # arcsec\n",
    "\n",
    "def find_tiles_for_galaxies(tiles_df, gals_df, num_tiles_to_find):\n",
    "    num_galaxies = len(gals_df.RA)\n",
    "    num_tiles = len(tiles_df.RA)\n",
    "\n",
    "    tiles_coord = coord.SkyCoord(ra=tiles_df.RA.to_numpy()*u.degree, dec=tiles_df.Dec.to_numpy()*u.degree, frame='icrs')\n",
    "    gals_coord = coord.SkyCoord(ra=gals_df.RA.to_numpy()*u.degree, dec=gals_df.Dec.to_numpy()*u.degree, frame='icrs')\n",
    "\n",
    "    # Structure for resultant data\n",
    "    nearest_tile_ids = np.zeros((num_galaxies, num_tiles_to_find), dtype=int)\n",
    "    ntiles_inside = np.zeros((num_galaxies), dtype=int)\n",
    "\n",
    "    for n in range(num_tiles_to_find):\n",
    "        idx, d2d, d3d = coord.match_coordinates_sky(gals_coord, tiles_coord, nthneighbor=n+1, storekdtree='kdtree_tiles')\n",
    "        nearest_tile_ids[:,n] = tiles_df.iloc[idx].TILEID\n",
    "        ntiles_inside += (d2d < TILE_RADIUS).astype(int)\n",
    "\n",
    "    \n",
    "    return ntiles_inside, nearest_tile_ids\n",
    "\n",
    "def table_to_df(table: Table):\n",
    "    \"\"\"\n",
    "    This does not work for all purposes yet.\n",
    "    \"\"\"\n",
    "    # TODO why not use to_pandas()?\n",
    "    #df = table.to_pandas()\n",
    "    \n",
    "    obj_type = table['SPECTYPE'].data.data\n",
    "    dec = table['DEC'].astype(\"<f8\") # Big endian vs little endian regression in pandas. Convert more of these fields like this\n",
    "    ra = table['RA'].astype(\"<f8\") # as needed if using pandas with this data\n",
    "    z_obs = table['Z'].data.data\n",
    "    target_id = table['TARGETID']\n",
    "    #flux_r = table['FLUX_R']\n",
    "    #flux_g = table['FLUX_G']\n",
    "    app_mag_r = get_app_mag(table['FLUX_R'])\n",
    "    app_mag_g = get_app_mag(table['FLUX_G'])\n",
    "    g_r_apparent = app_mag_g - app_mag_r\n",
    "    #sdss_g_r = table['ABSMAG_SDSS_G'] - table['ABSMAG_SDSS_R'] \n",
    "    #G_R_JM1 = table['ABSMAG01_SDSS_G'] - table['ABSMAG01_SDSS_R']\n",
    "    p_obs = table['PROB_OBS'] \n",
    "    unobserved = table['Z'].mask\n",
    "    deltachi2 = table['DELTACHI2'].data.data\n",
    "    ntiles = table['NTILE']\n",
    "    #abs_mag_sdss = table['ABSMAG_SDSS_R']\n",
    "    dn4000 = table['DN4000'].data.data\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'SPECTYPE': obj_type,\n",
    "        'Dec': dec,\n",
    "        'RA': ra,\n",
    "        'z': z_obs,\n",
    "        'TARGETID': target_id,\n",
    "        #'FLUX_R': flux_r,\n",
    "        #'FLUX_G': flux_g,\n",
    "        'APP_MAG_R': app_mag_r,\n",
    "        'APP_MAG_G': app_mag_g,\n",
    "        'G_R_APPARENT': g_r_apparent,\n",
    "        #'SDSS_G_R': sdss_g_r,\n",
    "        #'G_R_JM1': G_R_JM1,\n",
    "        'PROB_OBS': p_obs,\n",
    "        'UNOBSERVED': unobserved,\n",
    "        'DELTACHI2': deltachi2,\n",
    "        'NTILE': ntiles,\n",
    "        #'ABS_MAG_SDSS': abs_mag_sdss,\n",
    "        'DN4000': dn4000\n",
    "        })\n",
    "\n",
    "    return df\n",
    "\n",
    "def read_tiles_Y1_file():\n",
    "    tiles_table = Table.read(BGS_TILES_FILE, format='csv')\n",
    "    tiles_table.keep_columns(['TILEID', 'FAFLAVOR', 'TILERA', 'TILEDEC'])\n",
    "    tiles_df = pd.DataFrame({'RA': tiles_table['TILERA'].astype(\"<f8\"), 'Dec': tiles_table['TILEDEC'].astype(\"<f8\"), 'FAFLAVOR': tiles_table['FAFLAVOR'], 'TILEID': tiles_table['TILEID']})\n",
    "    tiles_df = tiles_df[tiles_df.FAFLAVOR == 'mainbright']\n",
    "    tiles_df.reset_index(drop=True, inplace=True)\n",
    "    return tiles_df\n",
    "\n",
    "def read_tiles_file():\n",
    "    tiles_table = Table.read(BGS_Y3_TILES_FILE, format='csv')\n",
    "    tiles_table.keep_columns(['TILEID', 'FAFLAVOR', 'TILERA', 'TILEDEC'])\n",
    "    tiles_df = pd.DataFrame({'RA': tiles_table['TILERA'].astype(\"<f8\"), 'Dec': tiles_table['TILEDEC'].astype(\"<f8\"), 'FAFLAVOR': tiles_table['FAFLAVOR'], 'TILEID': tiles_table['TILEID']})\n",
    "    return tiles_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Merged SV3 File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv3_table = Table.read(BGS_SV3_ANY_FULL_FILE, format='fits')\n",
    "tiles = read_tiles_file()\n",
    "SV3_tiles = tiles.loc[tiles.FAFLAVOR == 'sv3bright']\n",
    "print(sv3_table.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to needed columns only and save\n",
    "sv3_table.keep_columns(['TARGETID', 'SPECTYPE', 'DEC', 'RA', 'Z_not4clus', 'ZTILEID', 'NUMOBS', 'FLUX_R', 'FLUX_G', 'PROB_OBS', 'ZWARN', 'DELTACHI2', 'NTILE', 'TILES', 'TILEID', 'TILELOCID'])\n",
    "sv3_table.rename_column('Z_not4clus', 'Z')\n",
    "\n",
    "sv3_df = sv3_table.to_pandas()\n",
    "sv3_df.rename(columns={'DEC': 'Dec'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntiles_inside, nearest_tile_ids = find_tiles_for_galaxies(SV3_tiles, sv3_df, 15)\n",
    "sv3_table.add_column(ntiles_inside, name=\"NTILE_MINE\")\n",
    "sv3_table.add_column(nearest_tile_ids, name=\"NEAREST_TILEIDS\")\n",
    "sv3_table.write(IAN_BGS_SV3_MERGED_FILE, format='fits', overwrite='True')\n",
    "del(sv3_table)\n",
    "del(sv3_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Photo-z\n",
    " \n",
    "See photoz.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fits_links_pz, fits_links_main = get_photoz_file_lists()\n",
    "print(len(fits_links_pz))\n",
    "print(len(fits_links_main))\n",
    "\n",
    "# Network Bandwidth:\n",
    "# 445 GB for pz sweeps\n",
    "# more for main sweeps..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INITIALIZE LIGHTWEIGHT DESI BGS PHOTO-Z TABLE\n",
    "# Don't re-run! Will overwrite the file.\n",
    "\"\"\"\n",
    "desi_table = Table.read(IAN_BGS_Y3_MERGED_FILE, format='fits')\n",
    "desi_table2 = Table.read(IAN_BGS_SV3_MERGED_FILE, format='fits')\n",
    "\n",
    "assert len(np.unique(desi_table['TARGETID'])) == len(desi_table), \"There are duplicate TARGETIDs in the Y3 file\"\n",
    "assert len(np.unique(desi_table2['TARGETID'])) == len(desi_table2), \"There are duplicate TARGETIDs in the SV3 file\"\n",
    "\n",
    "desi_table.keep_columns(['TARGETID', 'RA', 'DEC'])\n",
    "desi_table2.keep_columns(['TARGETID', 'RA', 'DEC'])\n",
    "\n",
    "desi_targets_table = vstack([desi_table, desi_table2], join_type='inner')\n",
    "desi_targets_table = unique(desi_targets_table, 'TARGETID')\n",
    "desi_targets_table['Z_LEGACY_BEST'] = -99.0\n",
    "\n",
    "# add columns for 'RELEASE', 'BRICKID', 'OBJID' with no values\n",
    "desi_targets_table.add_column(np.zeros(len(desi_targets_table), dtype=int), name='RELEASE')\n",
    "desi_targets_table.add_column(np.zeros(len(desi_targets_table), dtype=int), name='BRICKID')\n",
    "desi_targets_table.add_column(np.zeros(len(desi_targets_table), dtype=int), name='OBJID')\n",
    "\n",
    "\n",
    "desi_targets_table = desi_targets_table.to_pandas()\n",
    "desi_targets_table.set_index('TARGETID', inplace=True)\n",
    "pickle.dump(desi_targets_table, open(IAN_PHOT_Z_FILE, 'wb'))\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brick is 5 degrees wide in both RA and DEC TODO can filter DESI targets by this before matching? faster? Maybe...\n",
    "#plt.hist(df.RA) \n",
    "#plt.hist(df.DEC)\n",
    "\n",
    "# Checking the matched objects\n",
    "#plot_positions(\n",
    "#    pd.DataFrame({'RA': df[matched2].RA, 'Dec': df[matched2].DEC}), \n",
    "#    pd.DataFrame({'RA': desi_table['RA'].astype('<f8'), 'Dec': desi_table['DEC'].astype('<f8')}), \n",
    "#    tiles_df=None, \n",
    "#    DEG_LONG=1,\n",
    "#    ra_min=np.min(df['RA']), \n",
    "#    dec_min=np.min(df['DEC']+0.5), \n",
    "#    split=False\n",
    "#)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a merged master BGS data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdul = fits.open(BGS_FASTSPEC_FILE, memmap=True)\n",
    "#print(hdul[1].columns)\n",
    "data = hdul[1].data\n",
    "fastspecfit_id = data['TARGETID']\n",
    "DN4000 = data['DN4000'] # TODO there is also DN4000_OBS and DN4000_MODEL (and inverse variance)\n",
    "FSF_G = data['ABSMAG01_SDSS_G']\n",
    "FSF_R = data['ABSMAG01_SDSS_R']\n",
    "hdul.close()\n",
    "\n",
    "print(len(fastspecfit_id))\n",
    "print(len(DN4000))\n",
    "\n",
    "fastspecfit_table = Table([fastspecfit_id, DN4000, FSF_G, FSF_R], names=('TARGETID', 'DN4000', 'ABSMAG01_SDSS_G', 'ABSMAG01_SDSS_R'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_table = Table.read(BGS_ANY_FULL_FILE, format='fits')\n",
    "print(main_table.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALREADY DONE FOR US; only needed to do this in Iron v1.2 due to a bug.\n",
    "# Prob obs file\n",
    "#p_table = Table.read(BGS_PROB_OBS_FILE, format='fits')\n",
    "#print(len(p_table))\n",
    "\n",
    "# Join them all on TARGETID\n",
    "#joined_table = join(main_table, p_table, keys=\"TARGETID\")\n",
    "#print(len(joined_table))\n",
    "\n",
    "to_join = main_table\n",
    "#to_join = p_table\n",
    "\n",
    "# The lost galaxies will not have fastspecfit rows I think\n",
    "final_table = join(to_join, fastspecfit_table, join_type='left', keys=\"TARGETID\")\n",
    "print(len(final_table))\n",
    "\n",
    "# Sanity check that everything went as intended\n",
    "assert len(final_table) == len(main_table)\n",
    "\n",
    "# Filter to needed columns only and save\n",
    "final_table.keep_columns(['TARGETID', 'SPECTYPE', 'DEC', 'RA', 'Z_not4clus', 'FLUX_R', 'FLUX_G', 'BITWEIGHTS', 'PROB_OBS', 'ZWARN', 'DELTACHI2', 'NTILE', 'TILES', 'DN4000', 'ABSMAG01_SDSS_G', 'ABSMAG01_SDSS_R', 'MASKBITS'])\n",
    "final_table.rename_column('Z_not4clus', 'Z')\n",
    "final_table.write(IAN_BGS_MERGED_FILE, format='fits', overwrite='True')\n",
    "\n",
    "del(main_table)\n",
    "#del(p_table)\n",
    "del(fastspecfit_table)\n",
    "del(final_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augment with my version of NTILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_NTILE_MINE_to_table(table_file):\n",
    "    tiles_df = read_tiles_Y1_file()\n",
    "    table = Table.read(table_file, format='fits')\n",
    "    galaxies_df = table_to_df(table)\n",
    "    \n",
    "    ntiles_inside, nearest_tile_ids = find_tiles_for_galaxies(tiles_df, galaxies_df, 15)\n",
    "    if 'NTILE_MINE' in table.columns:\n",
    "        table.remove_columns(['NTILE_MINE', 'NEAREST_TILEIDS'])\n",
    "    table.add_column(ntiles_inside, name=\"NTILE_MINE\")\n",
    "    table.add_column(nearest_tile_ids, name=\"NEAREST_TILEIDS\")\n",
    "\n",
    "    table.write(table_file, format='fits', overwrite='True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add_NTILE_MINE_to_table(IAN_BGS_MERGED_FILE)\n",
    "add_NTILE_MINE_to_table(IAN_BGS_MERGED_FILE_OLD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jura Quick Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't have all files needed to use the above pipline yet, so just working with what we have\n",
    "\n",
    "# Main file\n",
    "main_table = Table.read(BGS_Y3_ANY_FULL_FILE, format='fits')\n",
    "print(len(main_table))\n",
    "\n",
    "# Filter to needed columns only and save\n",
    "main_table.keep_columns(['TARGETID', 'SPECTYPE', 'DEC', 'RA', 'Z', 'FLUX_R', 'FLUX_G', 'ZWARN', 'DELTACHI2', 'NTILE', 'TILES'])\n",
    "\n",
    "galaxies_df = pd.DataFrame({\n",
    "    'Dec': main_table['DEC'],\n",
    "    'RA': main_table['RA'],\n",
    "    })\n",
    "\n",
    "tiles_BGS = read_tiles_file()\n",
    "\n",
    "ntiles_inside, nearest_tile_ids = find_tiles_for_galaxies(tiles_BGS, galaxies_df, 10)\n",
    "\n",
    "main_table.add_column(ntiles_inside, name=\"NTILE_MINE\")\n",
    "main_table.add_column(nearest_tile_ids, name=\"NEAREST_TILEIDS\")\n",
    "\n",
    "main_table.write(IAN_BGS_Y3_MERGED_FILE, format='fits', overwrite='True')\n",
    "\n",
    "del(main_table)\n",
    "del(tiles_BGS)\n",
    "del(galaxies_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine data in Merged BGS File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick one\n",
    "table = Table.read(IAN_BGS_MERGED_FILE, format='fits')\n",
    "#table = Table.read(IAN_BGS_SV3_MERGED_FILE, format='fits')\n",
    "#table = Table.read(IAN_BGS_Y3_MERGED_FILE, format='fits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiles_BGS = read_tiles_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See two equivalent ways of determining which rows are for unobserved galaxies\n",
    "one=table['ZWARN'] == 999999\n",
    "two=table['Z'].mask\n",
    "three=table['Z'] == 999999.0\n",
    "assert(np.all(one == two))\n",
    "assert(np.all(one == three))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(table['Z'], bins=50)\n",
    "plt.title(\"Z\")\n",
    "plt.yscale('log')\n",
    "print(np.min(table['Z']), np.max(table['Z']))\n",
    "print(table['Z'].mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.unique(table['ZWARN']))\n",
    "#print(np.unique(table['ZWARN_MTL']))\n",
    "print(np.unique(table['SPECTYPE']))\n",
    "print(np.unique(table['NTILE']))\n",
    "#print(np.unique(table['TARGET_STATE']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cut to the galaxy data we actually need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO this gets easilly out of sync with the .py file that does the 'production' filtering\n",
    "\n",
    "if np.ma.is_masked(table['Z']):\n",
    "    print(\"Masked table\")\n",
    "    z_obs = table['Z'].data.data\n",
    "    obj_type = table['SPECTYPE'].data.data\n",
    "    unobserved = table['Z'].mask # the masked values are what is unobserved\n",
    "    deltachi2 = table['DELTACHI2'].data.data  \n",
    "    maskbits = table['MASKBITS'].data.data\n",
    "else:\n",
    "    print(\"Unmasked table\")\n",
    "    # SV3 version didn't do this\n",
    "    z_obs = table['Z']\n",
    "    obj_type = table['SPECTYPE']\n",
    "    unobserved = table['Z'].astype(\"<i8\") == 999999\n",
    "    deltachi2 = table['DELTACHI2']\n",
    "    maskbits = table['MASKBITS']\n",
    "    \n",
    "dec = table['DEC']\n",
    "ra = table['RA']\n",
    "target_id = table['TARGETID']\n",
    "app_mag_r = get_app_mag(table['FLUX_R'])\n",
    "app_mag_g = get_app_mag(table['FLUX_G'])\n",
    "flux_r = table['FLUX_R']\n",
    "flux_g = table['FLUX_G']\n",
    "g_r_apparent = app_mag_g - app_mag_r\n",
    "#sdss_g_r = table['ABSMAG_SDSS_G'] - table['ABSMAG_SDSS_R'] \n",
    "#G_R_JM1 = table['ABSMAG01_SDSS_G'] - table['ABSMAG01_SDSS_R']\n",
    "#p_obs = table['PROB_OBS'] \n",
    "ntiles = table['NTILE']\n",
    "#tiles = table['TILES']\n",
    "#ztileid = table['ZTILEID']\n",
    "#tile_id = table['TILEID']\n",
    "#numobs = table['NUMOBS']\n",
    "#tile_locid = table['TILELOCID']\n",
    "ntiles_mine = table['NTILE_MINE']\n",
    "tileids = table['NEAREST_TILEIDS'][:,0].astype(\"<i8\") # TODO there are 10 here, we want NTILES_MINE many...\n",
    "#abs_mag_sdss = table['ABSMAG_SDSS_R']\n",
    "#dn4000 = table['DN4000'].data.data\n",
    "\n",
    "\n",
    "before_count = len(dec)\n",
    "print(before_count, \"objects in FITS file\")\n",
    "\n",
    "# TODO BUG Can we be mistaking STARS for GALAXIES?\n",
    "# Make filter array (True/False values)\n",
    "PASSES_REQUIRED = [1,2,3,4,10]\n",
    "\n",
    "galaxy_observed_filter = obj_type == b'GALAXY'\n",
    "app_mag_filter = app_mag_r < APP_MAG_CUT\n",
    "redshift_filter = z_obs > Z_MIN\n",
    "redshift_hi_filter = z_obs < Z_MAX\n",
    "deltachi2_filter = deltachi2 > 40\n",
    "#abs_mag_sdss_filter = abs_mag_sdss < 100\n",
    "#observed_requirements = np.all([galaxy_observed_filter, app_mag_filter, redshift_filter, redshift_hi_filter, deltachi2_filter, abs_mag_sdss_filter], axis=0)\n",
    "observed_requirements = np.all([galaxy_observed_filter, app_mag_filter, redshift_filter, redshift_hi_filter, deltachi2_filter], axis=0)\n",
    "\n",
    "treat_as_unobserved = np.all([galaxy_observed_filter, app_mag_filter, np.invert(deltachi2_filter)], axis=0)\n",
    "\n",
    "unobserved = np.all([app_mag_filter, np.logical_or(unobserved, treat_as_unobserved)], axis=0)\n",
    "keep = np.all([np.logical_or(observed_requirements, unobserved)], axis=0)\n",
    "\n",
    "print(\"\\nWhole sample:\")\n",
    "print(f\"There are {len(obj_type):,} objects in the entire sample, of which {np.sum(galaxy_observed_filter):,} are observed galaxies.\") \n",
    "\n",
    "for n in PASSES_REQUIRED:\n",
    "    n_pass_filter = ntiles_mine >= n\n",
    "    n_pass_filter_old = ntiles >= n\n",
    "    unobserved_n = np.all([n_pass_filter, unobserved], axis=0)\n",
    "    observed_requirements_n = np.all([n_pass_filter, observed_requirements], axis=0)\n",
    "    keepn = np.all([np.logical_or(observed_requirements_n, unobserved_n)], axis=0)\n",
    "\n",
    "    print(f\"\\n{n}-pass analysis (NTILE_MINE):\")\n",
    "    print(f\"There are {np.sum(observed_requirements_n):,} galaxies in the bright (<{APP_MAG_CUT} mag) sample that pass our quality checks.\")\n",
    "    print(f\"There are {np.sum(unobserved_n):,} unobserved galaxies, including bad observed galaxies.\")\n",
    "    print(f\"This {n}-pass catalog would have {np.sum(keepn):,} galaxies ({np.sum(unobserved_n) / np.sum(keepn) * 100:.2f}% lost).\")\n",
    "\n",
    "    # We've demonstratred this is definetely not what we want\n",
    "    #unobserved_n_old = np.all([n_pass_filter_old, unobserved], axis=0)\n",
    "    #observed_requirements_n_old = np.all([n_pass_filter_old, observed_requirements], axis=0)\n",
    "    #keepn_old = np.all([np.logical_or(observed_requirements_n_old, unobserved_n_old)], axis=0)\n",
    "    #print(f\"\\n{n}-pass analysis (NTILE):\")\n",
    "    #print(f\"There are {np.sum(observed_requirements_n_old):,} galaxies in the bright (<{APP_MAG_CUT} mag) sample that pass our quality checks.\")\n",
    "    #print(f\"There are {np.sum(unobserved_n_old):,} unobserved galaxies, including bad observed galaxies.\")\n",
    "    #print(f\"This {n}-pass catalog would have {np.sum(keepn_old):,} galaxies ({np.sum(unobserved_n_old) / np.sum(keepn_old) * 100:.2f}% lost).\")\n",
    "\n",
    "# FOR PARTS BELOW SET WHAT YOU WANT TO KEEP!\n",
    "keep = np.all([keep, ntiles_mine >= KEEP_PASSES], axis=0)\n",
    "\n",
    "obj_type = obj_type[keep]\n",
    "dec = dec[keep]\n",
    "ra = ra[keep]\n",
    "z_obs = z_obs[keep]\n",
    "target_id = target_id[keep] \n",
    "flux_r = flux_r[keep]\n",
    "app_mag_r = app_mag_r[keep]\n",
    "app_mag_g = app_mag_g[keep]\n",
    "g_r_apparent = g_r_apparent[keep]\n",
    "#p_obs = p_obs[keep]\n",
    "unobserved = unobserved[keep]\n",
    "deltachi2 = deltachi2[keep]\n",
    "ntiles = ntiles[keep]\n",
    "#tiles = tiles[keep]\n",
    "#ztileid = ztileid[keep]\n",
    "ntiles_mine = ntiles_mine[keep]\n",
    "tileids = tileids[keep]\n",
    "#tile_id = tile_id[keep]\n",
    "#numobs = numobs[keep]\n",
    "#tile_locid = tile_locid[keep]\n",
    "#abs_mag_sdss = abs_mag_sdss[keep]\n",
    "#sdss_g_r = sdss_g_r[keep]\n",
    "#G_R_JM1 = G_R_JM1[keep]\n",
    "#dn4000 = dn4000[keep]\n",
    "maskbits = maskbits[keep]\n",
    "indexes_not_assigned = np.argwhere(unobserved)\n",
    "\n",
    "after_count = len(dec)\n",
    "\n",
    "print(f\"\\nAfter all filters we have {after_count} of the original {before_count} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiles_split = np.zeros((len(tiles)), dtype=list)\n",
    "counts = np.zeros(15, dtype=int)\n",
    "for i in range(len(tiles)):\n",
    "    tiles_split[i] = list(tiles[i].split('-'))\n",
    "    counts[len(tiles_split[i])] += 1\n",
    "\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiles_split[9000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make maps\n",
    "two_pass_filter = ntiles_mine >= 2 \n",
    "three_pass_filter = ntiles_mine >= 3 \n",
    "four_pass_filter = ntiles_mine >= 4 \n",
    "\n",
    "ra2 = ra[two_pass_filter]\n",
    "dec2 = dec[two_pass_filter]\n",
    "tileids2 =  tileids[two_pass_filter]\n",
    "unobserved2 = unobserved[two_pass_filter]\n",
    "\n",
    "ra3 = ra[three_pass_filter]\n",
    "dec3 = dec[three_pass_filter]\n",
    "tileids3 =  tileids[three_pass_filter]\n",
    "unobserved3 = unobserved[three_pass_filter]\n",
    "\n",
    "ra4 = ra[four_pass_filter]\n",
    "dec4 = dec[four_pass_filter]\n",
    "tileids4 =  tileids[four_pass_filter]\n",
    "unobserved4 = unobserved[four_pass_filter]\n",
    "\n",
    "one_pass_df = pd.DataFrame({'RA': ra, 'Dec': dec, 'z_assigned_flag': unobserved, 'TILEID': tileids})\n",
    "two_pass_df = pd.DataFrame({'RA': ra2, 'Dec': dec2, 'z_assigned_flag': unobserved2, 'TILEID': tileids2})\n",
    "three_pass_df = pd.DataFrame({'RA': ra3, 'Dec': dec3, 'z_assigned_flag': unobserved3, 'TILEID': tileids3})\n",
    "four_pass_df = pd.DataFrame({'RA': ra4, 'Dec': dec4, 'z_assigned_flag': unobserved4, 'TILEID': tileids4})\n",
    "\n",
    "plot_positions(one_pass_df, three_pass_df, tiles_df=tiles_BGS, DEG_LONG=5, split=False)\n",
    "\n",
    "#fig=make_map(ra, dec)\n",
    "#ra_4 = ra[four_pass_filter]\n",
    "#dec_4 = dec[four_pass_filter]\n",
    "#print(f\"Number of 4-pass galaxies: {len(ra_4)}, number of 3-pass galaxies: {len(ra)}\")\n",
    "#fig=make_map(ra_4, dec_4, fig=fig, alpha=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Siena Galaxy Atlas Analysis (SGA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understand how many galaxies are affected by Siena Galaxy Atlas (SGA) Masks\n",
    "has_a_maskbit = maskbits != 0\n",
    "idx_with_masks = np.nonzero(maskbits)\n",
    "print(f\"{np.sum(has_a_maskbit):,} galaxies ({np.sum(has_a_maskbit) / len(maskbits) * 100:.2f}%) have a maskbit set.\")\n",
    "\n",
    "unobserved_with_maskbits = np.logical_and(has_a_maskbit, unobserved)\n",
    "print(f\"{np.sum(unobserved_with_maskbits):,} galaxies ({np.sum(unobserved_with_maskbits) / len(maskbits) * 100:.2f}%) have a maskbit set and are unobserved.\")\n",
    "\n",
    "# See https://www.legacysurvey.org/dr9/bitmasks/\n",
    "# https://github.com/legacysurvey/legacypipe/blob/master/py/legacypipe/bits.py\n",
    "BITMASK_SGA = 0x1000 \n",
    "sga_collision = (maskbits & BITMASK_SGA) != 0\n",
    "print(f\"{np.sum(sga_collision):,} galaxies ({np.sum(sga_collision) / len(maskbits) * 100:.2f}%) have a SGA collision.\")\n",
    "\n",
    "to_remove = np.logical_and(sga_collision, unobserved)\n",
    "print(f\"{np.sum(to_remove):,} galaxies ({np.sum(to_remove) / len(maskbits) * 100:.2f}%) have a SGA collision and are unobserved.\")\n",
    "\n",
    "# TODO well it looks like the maskbits are only set on targets with spectra for some reason.\n",
    "# They are photometric so I don't know why this would be.\n",
    "# But also looking at the images, more often than not the masked target is the SGA one itself, not one inside its ellipse.\n",
    "# Thus removing all these seems worse than leaving them.\n",
    "\n",
    "sga_ra = ra[to_remove]\n",
    "sga_dec = dec[to_remove]\n",
    "df = pd.DataFrame({'RA': sga_ra, 'Dec': sga_dec})\n",
    "df.to_csv(OUTPUT_FOLDER + f'sga_collisions.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SV3 Analysis\n",
    "\n",
    "SV3 is composed of 20 regions where 10 or 11 exposures eacj were taken, almost completely on top of each other.  Our SV3 analysis takes the inner part of these patches (NTILE_MINE >= 10) of these regions as the data set.  \n",
    "\n",
    "Then, we can eliminate 1 tile from each of these regions to make test sets in order to view our systematics as a function of NTILE_MINE. The order they are eliminated in matters; we need to go backwards in time.}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a DataFrame filtered down to the galaxies we want to keep\n",
    "sv3_merged_table = Table.read(IAN_BGS_SV3_MERGED_FILE, format='fits')\n",
    "sv3_merged_table.remove_column('NEAREST_TILEIDS')\n",
    "sv3_df = sv3_merged_table.to_pandas()\n",
    "print(len(sv3_df))\n",
    "sv3_df['app_mag'] = get_app_mag(sv3_df['FLUX_R'])\n",
    "unobserved = sv3_merged_table['Z'].astype(\"<i8\") == 999999\n",
    "galaxy_observed_filter = sv3_df['SPECTYPE'] == b'GALAXY'\n",
    "redshift_filter = sv3_df['Z'] > Z_MIN\n",
    "redshift_hi_filter = sv3_df['Z'] < Z_MAX\n",
    "deltachi2_filter = sv3_df['DELTACHI2'] > 40\n",
    "app_mag_filter = sv3_df['app_mag'] < 20.3\n",
    "observed_requirements = np.all([galaxy_observed_filter, app_mag_filter, redshift_filter, redshift_hi_filter, deltachi2_filter], axis=0)\n",
    "treat_as_unobserved = np.all([galaxy_observed_filter, app_mag_filter, np.invert(deltachi2_filter)], axis=0)\n",
    "\n",
    "unobserved = np.all([app_mag_filter, np.logical_or(unobserved, treat_as_unobserved)], axis=0)\n",
    "sv3_df['OBSERVED'] = np.invert(unobserved)\n",
    "keep = np.all([np.logical_or(observed_requirements, unobserved)], axis=0)\n",
    "keep = np.all([keep, sv3_df['NTILE_MINE'] >= 10], axis=0)\n",
    "\n",
    "sv3_df = sv3_df.loc[keep] \n",
    "sv3_df.reset_index(drop=True, inplace=True)\n",
    "print(len(sv3_df))\n",
    "\n",
    "# Initialize new columns for observed as function of N pass\n",
    "for i in range(0, 12):\n",
    "    sv3_df[f'OBSERVED_{i}'] = sv3_df['OBSERVED']\n",
    "\n",
    "for FAINT in [False, True]:\n",
    "\n",
    "    if not FAINT:\n",
    "        mag_filter = sv3_df['app_mag'] < 19.5\n",
    "    else:\n",
    "        mag_filter = sv3_df['app_mag'] > 19.5\n",
    "        \n",
    "    print(f\"{len(sv3_df[mag_filter]) / 138.192} galaxies per sq degree\")\n",
    "\n",
    "    for patch_number in range(len(gc.sv3_regions_sorted)):\n",
    "        tilelist = gc.sv3_regions_sorted[patch_number]\n",
    "        #print(f'Patch {patch_number} - TILE IDs: {tilelist}')\n",
    "        \n",
    "        row_selector = np.logical_and(sv3_df['TILEID'].isin(tilelist), mag_filter)\n",
    "\n",
    "        #one_patch_df = sv3_df[sv3_df['TILEID'].isin(tilelist)]\n",
    "        #print(f\"{len(one_patch_df)} galaxies, {np.sum(one_patch_df['OBSERVED']) / len(one_patch_df) :.1%} of the targets are observed\")\n",
    "        #one_patch_df[f'OBSERVED_{len(tilelist)}'] = one_patch_df['OBSERVED']\n",
    "        \n",
    "        #print (\"Remove tiles in reverse TILEID order:\")\n",
    "        for i in np.flip(np.arange(0, len(tilelist))):\n",
    "            tileid = tilelist[i]\n",
    "            observed_by_this_tile = sv3_df.loc[row_selector, 'TILEID'] == tileid\n",
    "            #print(f'{np.sum(observed_by_this_tile)} galaxies were observed by tile {tileid} ({i+1}/{len(tilelist)})')\n",
    "            prev = sv3_df.loc[row_selector, f'OBSERVED_{i+1}']\n",
    "            sv3_df.loc[row_selector, f'OBSERVED_{i}'] = np.where(observed_by_this_tile, False, prev)\n",
    "            \n",
    "        #for i in np.flip(np.arange(0, len(tilelist)+1)):\n",
    "        #    if FAINT:\n",
    "        #        totals_observed_faint[i] += np.sum(sv3_df.loc[row_selector, f'OBSERVED_{i}'])\n",
    "        #        totals_all_faint[i] += len(sv3_df.loc[row_selector])\n",
    "        #    else:\n",
    "        #        totals_observed_bright[i] += np.sum(sv3_df.loc[row_selector, f'OBSERVED_{i}'])\n",
    "        #        totals_all_bright[i] += len(sv3_df.loc[row_selector])\n",
    "\n",
    "            #print(f\"{np.sum(one_patch_df[f'OBSERVED_{i}']) / len(one_patch_df) :.1%} of the targets are observed with {i} passes\")\n",
    "                \n",
    "    #for i in range(1, 12):\n",
    "    #    if FAINT:\n",
    "    #        print(f\"{totals_observed_faint[i]:,} ({totals_observed_faint[i] / totals_all_faint[i]:.1%}) faint galaxies are observed with {i} passes\")\n",
    "    #    else: \n",
    "    #        print(f\"{totals_observed_bright[i]:,} ({totals_observed_bright[i] / totals_all_bright[i]:.1%}) bright galaxies are observed with {i} passes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observed_faint = np.zeros(12, dtype=int)\n",
    "observed_bright = np.zeros(12, dtype=int)\n",
    "\n",
    "total_faint = np.sum(sv3_df['app_mag'] > 19.5)\n",
    "total_bright = np.sum(sv3_df['app_mag'] < 19.5)\n",
    "\n",
    "for i in range(0, 12):\n",
    "    observed_faint[i] = np.sum(sv3_df.loc[sv3_df['app_mag'] > 19.5, f'OBSERVED_{i}'])\n",
    "    observed_bright[i] = np.sum(sv3_df.loc[sv3_df['app_mag'] < 19.5, f'OBSERVED_{i}'])\n",
    "\n",
    "\n",
    "plt.plot(observed_bright / total_bright, color='b', label=\"BGS BRIGHT ME\")\n",
    "plt.plot(observed_faint / total_faint, color='orange', label=\"BGS FAINT ME\")\n",
    "plt.plot([1,2,3,4], [.29, .52, 0.68, .81], '--', color='b', label=\"BGS BRIGHT PAPER\")\n",
    "plt.plot([1,2,3,4], [.15, .32, 0.47, .62], '--', color='orange', label=\"BGS FAINT PAPER\")\n",
    "plt.xlabel(\"Number of passes\")\n",
    "plt.ylabel(\"Fraction of targets observed\")\n",
    "plt.title(\"SV3 BGS Completeness\")\n",
    "plt.xticks(np.arange(0, 12))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sv3_df.loc[sv3_df['app_mag'] < 19.5, 'OBSERVED'].count() - sv3_df.loc[sv3_df['app_mag'] < 19.5, 'OBSERVED'].sum())\n",
    "print(sv3_df.loc[sv3_df['app_mag'] < 19.5, 'OBSERVED'].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above numbers do not seem to track with the Y1 data. In Y1 no region has more than 4 passes so how do I have a fiber incompleteness better than the above number?\n",
    "\n",
    "Also Figure 17 of https://iopscience.iop.org/article/10.3847/1538-3881/accff8/pdf disagrees with my above analysis. So what is above is wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Color Analysis\n",
    "\n",
    "Lesson from this analysis: the BGS data, workign with my 0.1^G-R with GAMA k-corrections, does not distribute a per logLgal bin G-R; the global 0.76 split seems to work for all bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = app_mag_to_abs_mag(app_mag_g, z_obs)\n",
    "R = app_mag_to_abs_mag(app_mag_r, z_obs)\n",
    "\n",
    "G_R = G - R\n",
    "\n",
    "Gk = k_correct_bgs(G, z_obs, g_r_apparent, band='g')\n",
    "Rk = k_correct_bgs(R, z_obs, g_r_apparent, band='r')\n",
    "\n",
    "G_R_k = Gk - Rk\n",
    "\n",
    "Gk_GAMA = k_correct_gama(G, z_obs, g_r_apparent, band='g')\n",
    "Rk_GAMA = k_correct_gama(R, z_obs, g_r_apparent, band='r')\n",
    "\n",
    "G_R_k_GAMA = Gk_GAMA - Rk_GAMA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison of g-r computed a few ways\n",
    "bins = np.linspace(0, 2.0, 200)\n",
    "\n",
    "plt.figure()\n",
    "#junk=plt.hist(g_r_apparent, bins=bins, label=\"g-r\", histtype='step')\n",
    "#junk=plt.hist(sdss_g_r, bins=bins, label='From LSS Pipeline (JM?)', histtype='step', density=True)\n",
    "#junk=plt.hist(G_R, bins=bins, label=\"G-R\", histtype='step')\n",
    "junk=plt.hist(G_R_k, bins=bins, label=\"0.1^(G-R) BGS poly\", histtype='step', density=True)\n",
    "junk=plt.hist(G_R_k_GAMA, bins=bins, label=\"0.1^(G-R) GAMA poly\", histtype='step', density=True)\n",
    "#junk=plt.hist(G_R_JM1, bins=bins, label=\"0.1^(G-R) JM\", histtype='step', density=True)\n",
    "plt.xlabel(\"g-r\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend()\n",
    "plt.xlim(0.2, 1.3)\n",
    "plt.title(\"Comparison of g-r computed a few ways\")\n",
    "plt.tight_layout()\n",
    "plt.ylim(0,3.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can see global GLOBAL_RED_COLOR_CUT=0.76 here\n",
    "junk=plt.hist(G_R_k_GAMA, bins=300, alpha=0.5, label=\"0.1^(G-R) GAMA-style\")\n",
    "plt.legend()\n",
    "plt.xlim(0.5, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(is_quiescent_lost_gal_guess(g_r_apparent).sum() / len(g_r_apparent))\n",
    "assert len(G_R_k_GAMA) == len(g_r_apparent)\n",
    "print(is_quiescent_BGS_gmr(None, G_R_k_GAMA).sum() / len(G_R_k_GAMA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyutils import *\n",
    "print(BGS_LOGLGAL_BINS)\n",
    "print(BINWISE_RED_COLOR_CUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_quiescent_BGS_gmr(np.array([5.8, 9.0, 14.5]), np.array([0.5, 0.9, 0.9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get logLgal bins\n",
    "log_L_gal = abs_mag_r_to_log_solar_L(Rk) \n",
    "logLgal_bin_idx = np.digitize(log_L_gal, BGS_LOGLGAL_BINS)\n",
    "# 0 is less than the lowest, len(BGS_LOGLGAL_BINS) is greater than the highest entry in BGS_LOGLGAL_BINS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.min(log_L_gal))\n",
    "print(np.max(log_L_gal))\n",
    "print(np.min(logLgal_bin_idx))\n",
    "print(np.max(logLgal_bin_idx))\n",
    "plt.hist(log_L_gal, bins=BGS_LOGLGAL_BINS, align='mid')\n",
    "#plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a plot of G_R_k in each logLgal bin\n",
    "for i in range(0, len(BGS_LOGLGAL_BINS)+1):\n",
    "    galaxy_idx_for_this_bin = logLgal_bin_idx == i\n",
    "\n",
    "    plt.figure(dpi=80, figsize=(10, 6))\n",
    "    junk=plt.hist(G_R_k[galaxy_idx_for_this_bin], bins=np.arange(0,1.3,0.02), label=f\"0.1^(G-R) Bin {i}\", align='mid')\n",
    "    plt.legend()\n",
    "    plt.xlim(0.4, 1.2)\n",
    "    plt.xticks(np.arange(0.4, 1.2, 0.04))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mag1 = abs_mag_sdss\n",
    "mag2 = R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Absolute Magnitudes\n",
    "# Difference is how we k-correct I believe\n",
    "bins = np.linspace(-25, -10, 100)\n",
    "my_counts, my_bins, my_p = plt.hist(mag2, label=\"my abs_mag\", bins=bins, alpha=0.5)\n",
    "alex_counts, alex_bins, alex_p = plt.hist(mag1, label=\"ABSMAG_SDSS_R\", bins=bins, alpha=0.5)\n",
    "plt.xlabel(\"Absolute Mag\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Compare Absolute Mags\")\n",
    "#plt.yscale('log')\n",
    "plt.legend()\n",
    "\n",
    "print(f\"The peak is shifted from ABSMAG_SDSS_R {alex_bins[np.argmax(alex_counts)]:.1f} to my {my_bins[np.argmax(my_counts)]:.1f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=make_map(ra, dec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dn4000 Comparison (BGS, SDSS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdss = pd.read_csv(SDSS_v1_DAT_FILE, delimiter=' ', names=('RA', 'Dec', 'z', 'logLgal', 'V_max', 'quiescent', 'chi'), index_col=False)\n",
    "sdss_galprops = pd.read_csv(\"../data/sdss_galprops_v1.0.dat\", delimiter=' ', names=('Mag_g', 'Mag_r', 'sigma_v', 'Dn4000', 'concentration', 'log_M_star'))\n",
    "sdss = pd.merge(sdss, sdss_galprops, left_index=True, right_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(dn4000, bins=np.linspace(-0.5, 5.0, 100), alpha=0.6, label=\"BGS Y1\")\n",
    "plt.hist(sdss.Dn4000, bins=np.linspace(-0.5, 5.0, 100), alpha=0.8, label=\"SDSS\")\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.xlabel('Dn4000')\n",
    "plt.ylabel('Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(dn4000, bins=np.linspace(-0.5, 5.0, 100), alpha=0.6, label=\"BGS Y1\")\n",
    "plt.hist(sdss.Dn4000, bins=np.linspace(-0.5, 5.0, 100), alpha=0.8, label=\"SDSS\")\n",
    "plt.legend()\n",
    "plt.xlabel('Dn4000')\n",
    "plt.ylabel('Count')\n",
    "plt.xlim(0.9,2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdss_catalog = coord.SkyCoord(ra=sdss.RA.to_numpy()*u.degree, dec=sdss.Dec.to_numpy()*u.degree, frame='icrs')\n",
    "BGS_catalog = coord.SkyCoord(ra=ra*u.degree, dec=dec*u.degree, frame='icrs')\n",
    "\n",
    "neighbor_indexes, d2d, d3d = coord.match_coordinates_sky(BGS_catalog, sdss_catalog, storekdtree='sdss')\n",
    "ang_distances = d2d.to(u.arcsec).value\n",
    "\n",
    "match_found_filter = ang_distances < 3.0\n",
    "bgs_matches = dn4000[match_found_filter]\n",
    "sdss_indexes = neighbor_indexes[match_found_filter]\n",
    "sdss_matches = sdss.iloc[sdss_indexes].Dn4000.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{np.isclose(bgs_matches, sdss_matches, atol=0.05).sum() / len(bgs_matches)} of the matches are within 0.05 of each other.\")\n",
    "print(f\"{np.isclose(bgs_matches, sdss_matches, atol=0.1).sum() / len(bgs_matches)} of the matches are within 0.1 of each other.\")\n",
    "print(f\"{np.isclose(bgs_matches, sdss_matches, atol=0.2).sum() / len(bgs_matches)} of the matches are within 0.2 of each other.\")\n",
    "print(f\"{np.isclose(bgs_matches, sdss_matches, atol=0.3).sum() / len(bgs_matches)} of the matches are within 0.3 of each other.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=make_map(ra, dec)\n",
    "fig=make_map(sdss.RA.to_numpy(), sdss.Dec.to_numpy(), fig=fig, alpha=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(sdss_matches, bgs_matches, s=1, alpha=.2)\n",
    "plt.xlabel(\"SDSS Dn4000\")\n",
    "plt.ylabel(\"BGS Dn4000\")\n",
    "plt.xlim(1, 2.3)\n",
    "plt.ylim(1, 2.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'SDSS_Dn4000': sdss_matches, 'BGS_Dn4000': bgs_matches})\n",
    "df['diff_frac'] =  (df['BGS_Dn4000'] - df['SDSS_Dn4000']) / df['SDSS_Dn4000']\n",
    "bins = np.linspace(-1, 5, 60)\n",
    "labels = bins[0:len(bins)-1] \n",
    "df['dn4000_sdssbin'] = pd.cut(x = sdss_matches, bins = bins, labels = labels, include_lowest = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=80)\n",
    "diff_mean = df.groupby('dn4000_sdssbin').diff_frac.mean()\n",
    "diff_std= df.groupby('dn4000_sdssbin').diff_frac.std()\n",
    "\n",
    "plt.errorbar(labels, diff_mean, yerr=diff_std)\n",
    "plt.xlabel(\"SDSS Dn4000\")\n",
    "plt.ylabel(\"< (BGS-SDSS) / SDSS >\")\n",
    "plt.xlim(0.8, 2.4)\n",
    "plt.ylim(-0.75, 0.75)\n",
    "plt.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dn4000 Lgal Bin Analysis\n",
    "\n",
    "Run Color Analysis and Dn4000 Comparison first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a plot of Dn4000 in each logLgal bin\n",
    "fig,axes=plt.subplots(dpi=80, figsize=(10, 3*len(BGS_LOGLGAL_BINS)//2), ncols=2, nrows=len(BGS_LOGLGAL_BINS)//2)\n",
    "axes = np.ravel(axes)\n",
    "\n",
    "for i in range(0, len(BGS_LOGLGAL_BINS)-1):\n",
    "    galaxy_idx_for_this_bin = logLgal_bin_idx == i+1\n",
    "\n",
    "    junk=axes[i].hist(dn4000[galaxy_idx_for_this_bin], bins=np.arange(1,2.2,0.02), label=f\"Dn4000 for logLgal Bin {i+1}\", align='mid')\n",
    "    axes[i].legend()\n",
    "    axes[i].set_xlim(1, 2.2)\n",
    "    axes[i].set_xticks(np.arange(1, 2.2, 0.1))\n",
    "\n",
    "    # draw a vertical line at get_SDSS_Dcrit(logLgal)\n",
    "    axes[i].axvline(x=get_SDSS_Dcrit(BGS_LOGLGAL_BINS[i]), color='r', linestyle='-')\n",
    "\n",
    "axes = np.reshape(axes, (2, len(BGS_LOGLGAL_BINS)//2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Randoms Analysis for Footprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOMS_DENSITY = 2500 # per square degree, Ashley Ross paper on LSS pipeline or elsewhere in docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rtable = Table.read(BGS_RAND_FILE, format='fits')\n",
    "rtable.columns\n",
    "rtable.keep_columns(['LOCATION', 'FIBER', 'TARGETID', 'RA', 'DEC', 'PRIORITY', 'TILEID', 'TILELOCID', 'NTILE', 'TILES'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_dec = rtable['DEC'].astype(\"<f8\")\n",
    "r_ra = rtable['RA'].astype(\"<f8\")\n",
    "r_ntiles = rtable['NTILE'].astype(\"<i8\")\n",
    "r_tileid = rtable['TILEID'].astype(\"<i8\")\n",
    "\n",
    "# TODO TILEID is just one, TILES has them all... look at rtable['TILES'][135000] for example\n",
    "randoms_df = pd.DataFrame({'RA': r_ra, 'Dec': r_dec, 'NTILE': r_ntiles, 'TILEID': r_tileid})\n",
    "\n",
    "\n",
    "onepass_footprint = len(r_dec) / RANDOMS_DENSITY # in degrees squared\n",
    "onepass_frac_area = onepass_footprint / DEGREES_ON_SPHERE\n",
    "\n",
    "three_pass_filter = r_ntiles >= 3 # 3pass coverage\n",
    "r_dec3 = r_dec[three_pass_filter]\n",
    "r_ra3 = r_ra[three_pass_filter]\n",
    "\n",
    "threepass_footprint = len(r_dec3) / RANDOMS_DENSITY # in degrees squared\n",
    "threepass_frac_area = threepass_footprint / DEGREES_ON_SPHERE\n",
    "\n",
    "# My estimation procedure, which we don't use\n",
    "estimate = estimate_frac_area(r_ra, r_dec)\n",
    "estimate3 = estimate_frac_area(r_ra3, r_dec3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"INCORRECT RESULTS THAT USE NTILE, NOT NTILE_MINE\")\n",
    "print(f\"BGS 1pass Footprint calculated from randoms is {onepass_footprint} square degrees or frac_area={onepass_frac_area}\")\n",
    "print(f\"BGS 3pass Footprint calculated from randoms is {threepass_footprint} square degrees or frac_area={threepass_frac_area}\")\n",
    "#print(f\"BGS 1pass Footprint estimated from my algorithm: frac_area={estimate}\")\n",
    "#print(f\"BGS 3pass Footprint estimated from my algorithm: frac_area={estimate3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make map showing why we cannot use NTILE >= 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make maps\n",
    "tiles_BGS = read_tiles_Y1_file()\n",
    "#tiles_BGS = tiles_BGS.loc[tiles_BGS.FAFLAVOR == 'sv3bright']\n",
    "\n",
    "# Plot the galaxy positions (randoms) and return the set of tile_id associated with them\n",
    "plot_positions(randoms_df, randoms_df[randoms_df.NTILE >= 3], tiles_df=tiles_BGS, DEG_LONG=10, ra_min=180, dec_min=-5, split=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make map with NTILE_MINE\n",
    "\n",
    "Uses a NN lookup of the tiles for each 'galaxy'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD FROM LAST TIME WE DID THIS\n",
    "randoms_df = pickle.load(open(BIN_FOLDER + \"randoms_df.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO MAKE IT ANEW\n",
    "NTILE_MIN = 10\n",
    "ntiles_inside, nearest_tile_ids = find_tiles_for_galaxies(tiles_BGS, randoms_df, NTILE_MIN)\n",
    "randoms_df['NTILE_MINE'] = ntiles_inside\n",
    "\n",
    "print(np.sum(ntiles_inside >= 3) / len(ntiles_inside))\n",
    "print(np.sum(ntiles_inside >= 3))\n",
    "\n",
    "# Not sure if I really need the nearest tile IDs for anything\n",
    "pickle.dump(randoms_df, open(BIN_FOLDER + \"randoms_df.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the galaxy positions (randoms) and return the set of tile_id associated with them\n",
    "plot_positions(randoms_df, randoms_df[randoms_df.NTILE_MINE >= 3], tiles_df=tiles_BGS, DEG_LONG=10, split=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recalculate Footprint given this definition\n",
    "for i in range(1, 11):\n",
    "    n_pass_filter = randoms_df.NTILE_MINE >= i\n",
    "    n_pass_footprint = len(randoms_df[n_pass_filter].RA) / RANDOMS_DENSITY # in degrees squared\n",
    "    n_pass_frac_area = n_pass_footprint / DEGREES_ON_SPHERE\n",
    "    print(f\"BGS {i}pass Footprint calculated from randoms is {n_pass_footprint} square degrees or frac_area={n_pass_frac_area}\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ian-conda311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
