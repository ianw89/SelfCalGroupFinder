{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is code for studying the BGS data from the LSS Catalogs prior to running group finding. There are blocks to create the \"Merged Files\" that are inputs for the preprocessing and groupfinding routines in the GroupCatalog class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import astropy.coordinates as coord\n",
    "import astropy.units as u\n",
    "import astropy.io.fits as fits\n",
    "from astropy.table import Table,join,vstack,unique,QTable\n",
    "import sys\n",
    "from urllib.parse import urljoin\n",
    "from scipy.ndimage import gaussian_filter\n",
    "import seaborn as sns\n",
    "\n",
    "if './SelfCalGroupFinder/py/' not in sys.path:\n",
    "    sys.path.append('./SelfCalGroupFinder/py/')\n",
    "from pyutils import *\n",
    "from dataloc import *\n",
    "from photoz import *\n",
    "from bgs_helpers import *\n",
    "import groupcatalog as gc\n",
    "from nnanalysis import *\n",
    "from plotting import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEEP_PASSES = 1\n",
    "APP_MAG_CUT = 19.5 # BGS BRIGHT, though 19.54 for some cameras. I don't know if FLUX_R has been corrected for this.\n",
    "#APP_MAG_CUT = 20.0 # BGS FAINT, and in SV3 it is 20.3 instead\n",
    "Z_MIN = 0.001\n",
    "Z_MAX = 0.5\n",
    "KEEP_ONLY_OBSERVED = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "    # BASS r-mag offset with DECaLS.\n",
    "#offset = 0.04\n",
    "\"\"\"\n",
    "    if targtype == 'bright':\n",
    "        if south:\n",
    "            bgs &= rflux > 10**((22.5-19.5)/2.5)\n",
    "            bgs &= rflux <= 10**((22.5-12.0)/2.5)\n",
    "            bgs &= rfibertotflux <= 10**((22.5-15.0)/2.5)\n",
    "        else:\n",
    "            bgs &= rflux > 10**((22.5-(19.5+offset))/2.5)\n",
    "            bgs &= rflux <= 10**((22.5-12.0)/2.5)\n",
    "            bgs &= rfibertotflux <= 10**((22.5-15.0)/2.5)\n",
    "    elif targtype == 'faint':\n",
    "        if south:\n",
    "            bgs &= rflux > 10**((22.5-20.175)/2.5)\n",
    "            bgs &= rflux <= 10**((22.5-19.5)/2.5)\n",
    "            bgs &= (rfibcol)\n",
    "        else:\n",
    "            bgs &= rflux > 10**((22.5-(20.220))/2.5)\n",
    "            bgs &= rflux <= 10**((22.5-(19.5+offset))/2.5)\n",
    "            bgs &= (rfibcol)\n",
    "\n",
    "    return bgs\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Empty Photo-z Ledger file\n",
    " \n",
    "See photoz.py for the code that populates this file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INITIALIZE LIGHTWEIGHT DESI BGS PHOTO-Z TABLE\n",
    "# Don't re-run! Will overwrite the file.\n",
    "\"\"\"\n",
    "desi_table = Table.read(IAN_BGS_Y3_MERGED_FILE, format='fits')\n",
    "desi_table2 = Table.read(IAN_BGS_SV3_MERGED_NOY3_FILE, format='fits')\n",
    "\n",
    "assert len(np.unique(desi_table['TARGETID'])) == len(desi_table), \"There are duplicate TARGETIDs in the Y3 file\"\n",
    "assert len(np.unique(desi_table2['TARGETID'])) == len(desi_table2), \"There are duplicate TARGETIDs in the SV3 file\"\n",
    "\n",
    "desi_table.keep_columns(['TARGETID', 'RA', 'DEC'])\n",
    "desi_table2.keep_columns(['TARGETID', 'RA', 'DEC'])\n",
    "\n",
    "desi_targets_table = vstack([desi_table, desi_table2], join_type='inner')\n",
    "desi_targets_table = unique(desi_targets_table, 'TARGETID')\n",
    "desi_targets_table['Z_LEGACY_BEST'] = NO_PHOTO_Z\n",
    "\n",
    "# add columns for 'RELEASE', 'BRICKID', 'OBJID', 'REF_CAT', 'MATCH_DIST' with no values\n",
    "desi_targets_table.add_column(np.zeros(len(desi_targets_table), dtype=int), name='RELEASE')\n",
    "desi_targets_table.add_column(np.zeros(len(desi_targets_table), dtype=int), name='BRICKID')\n",
    "desi_targets_table.add_column(np.zeros(len(desi_targets_table), dtype=int), name='OBJID')\n",
    "desi_targets_table.add_column(np.zeros(len(desi_targets_table), dtype='S2'), name='REF_CAT')\n",
    "desi_targets_table.add_column(np.full(len(desi_targets_table), 999999, dtype=float), name='MATCH_DIST')\n",
    "\n",
    "\n",
    "desi_targets_table = desi_targets_table.to_pandas()\n",
    "desi_targets_table.set_index('TARGETID', inplace=True)\n",
    "pickle.dump(desi_targets_table, open(IAN_PHOT_Z_FILE, 'wb'))\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read BGS_IMAGES_FOLDER + \"terminal.txt\" into an array of strings, 1 per line\n",
    "f = open(BGS_IMAGES_FOLDER + \"terminal.txt\", \"r\")\n",
    "lines = f.readlines()\n",
    "f.close()\n",
    "\n",
    "# Now filter it down to ones like this:\n",
    "#Start processing brick #X\n",
    "#Matched 0 out of Z\n",
    "#start_lines = [line for line in lines if \"Start processing brick\" in line]\n",
    "matched_lines = [line for line in lines if \"Matched\" in line]\n",
    "\n",
    "#print(len(start_lines))\n",
    "print(len(matched_lines))\n",
    "\n",
    "# Extract X, Y, Z from each line \n",
    "#start_lines = [line.split()[3] for line in start_lines]    \n",
    "matched_lines = [int(line.split()[1]) for line in matched_lines]\n",
    "\n",
    "# Strip away # and convert to int\n",
    "#start_lines = [int(line[1:]) for line in start_lines]\n",
    "\n",
    "# The index is the brick number for matched_lines\n",
    "# Find the brick numbers where the value is 0 and remember those indexes\n",
    "zero_indexes = [i for i in range(len(matched_lines)) if matched_lines[i] == 0]\n",
    "\n",
    "#pickle.dump(zero_indexes, open(BRICKS_TO_SKIP_S_FILE, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle.dump([], open(BRICKS_TO_SKIP_N_FILE, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build other support files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_sv3_clustering_randoms_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_sv3_full_randoms_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a file with just TARGETID and QUIESCENT columns to send to Joe for Clustering\n",
    "tbl = Table.read(IAN_BGS_Y3_MERGED_FILE_LOA, format='fits') \n",
    "tbl.keep_columns(['TARGETID', 'QUIESCENT'])\n",
    "tbl.write(BGS_Y3_FOLDER_LOA + \"COLOR_LOOKUP_LOA.fits\", format='fits', overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine data in a Merged BGS File (SV3, Y1, Y3, whatever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick one\n",
    "#table = Table.read(IAN_BGS_SV3_MERGED_FILE, format='fits')\n",
    "#table = Table.read(IAN_BGS_SV3_MERGED_NOY3_FILE, format='fits')\n",
    "table = Table.read(IAN_BGS_Y1_MERGED_FILE, format='fits')\n",
    "#table = Table.read(IAN_BGS_Y3_MERGED_FILE_KIBO, format='fits')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cut to the galaxy data we actually need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [name for name in table.colnames if len(table[name].shape) <= 1]\n",
    "df = table[names].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO this gets easilly out of sync with the .py file that does the 'production' filtering\n",
    "\n",
    "if np.ma.is_masked(table['Z']):\n",
    "    print(\"Masked table\")\n",
    "    z_obs = table['Z'].data.data.astype(\"<f8\")\n",
    "    obj_type = table['SPECTYPE'].data.data\n",
    "    unobserved = table['Z'].mask # the masked values are what is unobserved\n",
    "    deltachi2 = table['DELTACHI2'].data.data  \n",
    "else:\n",
    "    print(\"Unmasked table\")\n",
    "    # SV3 version didn't do this\n",
    "    z_obs = table['Z']\n",
    "    obj_type = table['SPECTYPE']\n",
    "    unobserved = table['Z'].astype(\"<i8\") == 999999\n",
    "    deltachi2 = table['DELTACHI2']\n",
    "\n",
    "maskbits = table['MASKBITS'].data.data if np.ma.is_masked(table['MASKBITS']) else table['MASKBITS']\n",
    "dec = table['DEC'].astype(\"<f8\")\n",
    "ra = table['RA'].astype(\"<f8\")\n",
    "z_phot = table['Z_PHOT'].astype(\"<f8\")\n",
    "target_id = table['TARGETID']\n",
    "app_mag_r = get_app_mag(table['FLUX_R'])\n",
    "app_mag_g = get_app_mag(table['FLUX_G'])\n",
    "flux_r = table['FLUX_R'].astype(\"<f8\")\n",
    "flux_g = table['FLUX_G'].astype(\"<f8\")\n",
    "g_r_apparent = app_mag_g - app_mag_r\n",
    "abs_mag_r = table['ABS_MAG_R']\n",
    "abs_mag_g = table['ABS_MAG_G']\n",
    "#sdss_g_r = table['ABSMAG_SDSS_G'] - table['ABSMAG_SDSS_R'] \n",
    "G_R_JM1 = table['ABSMAG01_SDSS_G'] - table['ABSMAG01_SDSS_R']\n",
    "p_obs = table['PROB_OBS'] \n",
    "ntiles = table['NTILE'].astype(\"<i8\")\n",
    "#tiles = table['TILES']\n",
    "tile_id = table['TILEID'] if 'TILEID' in table.columns else None\n",
    "#numobs = table['NUMOBS']\n",
    "#tile_locid = table['TILELOCID']\n",
    "ntiles_mine = table['NTILE_MINE']\n",
    "tileids = table['NEAREST_TILEIDS'][:,0].astype(\"<i8\") # TODO there are 10 here, we want NTILES_MINE many...\n",
    "#abs_mag_sdss = table['ABSMAG_SDSS_R']\n",
    "dn4000 = table['DN4000_MODEL'].data.data\n",
    "ref_cat = table['REF_CAT']\n",
    "quiescent = table['QUIESCENT']\n",
    "\n",
    "\n",
    "before_count = len(dec)\n",
    "print(before_count, \"objects in FITS file\")\n",
    "\n",
    "# TODO BUG Can we be mistaking STARS for GALAXIES?\n",
    "# Make filter array (True/False values)\n",
    "PASSES_REQUIRED = [1,2,3,4,10]\n",
    "\n",
    "galaxy_observed_filter = obj_type == b'GALAXY'\n",
    "app_mag_filter = app_mag_r < APP_MAG_CUT\n",
    "redshift_filter = z_obs > Z_MIN\n",
    "redshift_hi_filter = z_obs < Z_MAX\n",
    "deltachi2_filter = deltachi2 > 40\n",
    "#abs_mag_sdss_filter = abs_mag_sdss < 100\n",
    "#observed_requirements = np.all([galaxy_observed_filter, app_mag_filter, redshift_filter, redshift_hi_filter, deltachi2_filter, abs_mag_sdss_filter], axis=0)\n",
    "observed_requirements = np.all([galaxy_observed_filter, app_mag_filter, redshift_filter, redshift_hi_filter, deltachi2_filter], axis=0)\n",
    "\n",
    "treat_as_unobserved = np.all([galaxy_observed_filter, app_mag_filter, np.invert(deltachi2_filter)], axis=0)\n",
    "\n",
    "unobserved = np.all([app_mag_filter, np.logical_or(unobserved, treat_as_unobserved)], axis=0)\n",
    "keep = np.all([np.logical_or(observed_requirements, unobserved)], axis=0)\n",
    "\n",
    "print(\"\\nWhole sample:\")\n",
    "print(f\"There are {len(obj_type):,} objects in the entire sample, of which {np.sum(galaxy_observed_filter):,} are observed galaxies.\") \n",
    "\n",
    "for n in PASSES_REQUIRED:\n",
    "    n_pass_filter = ntiles_mine >= n\n",
    "    n_pass_filter_old = ntiles >= n\n",
    "    unobserved_n = np.all([n_pass_filter, unobserved], axis=0)\n",
    "    observed_requirements_n = np.all([n_pass_filter, observed_requirements], axis=0)\n",
    "    keepn = np.all([np.logical_or(observed_requirements_n, unobserved_n)], axis=0)\n",
    "\n",
    "    print(f\"\\n{n}-pass analysis (NTILE_MINE):\")\n",
    "    print(f\"There are {np.sum(observed_requirements_n):,} galaxies in the <{APP_MAG_CUT} mag sample that pass our quality checks.\")\n",
    "    print(f\"There are {np.sum(unobserved_n):,} unobserved galaxies, including bad observed galaxies.\")\n",
    "    print(f\"This {n}-pass catalog would have {np.sum(keepn):,} galaxies ({np.sum(unobserved_n) / np.sum(keepn) * 100:.2f}% lost).\")\n",
    "\n",
    "    # We've demonstratred this is definetely not what we want\n",
    "    #unobserved_n_old = np.all([n_pass_filter_old, unobserved], axis=0)\n",
    "    #observed_requirements_n_old = np.all([n_pass_filter_old, observed_requirements], axis=0)\n",
    "    #keepn_old = np.all([np.logical_or(observed_requirements_n_old, unobserved_n_old)], axis=0)\n",
    "    #print(f\"\\n{n}-pass analysis (NTILE):\")\n",
    "    #print(f\"There are {np.sum(observed_requirements_n_old):,} galaxies in the bright (<{APP_MAG_CUT} mag) sample that pass our quality checks.\")\n",
    "    #print(f\"There are {np.sum(unobserved_n_old):,} unobserved galaxies, including bad observed galaxies.\")\n",
    "    #print(f\"This {n}-pass catalog would have {np.sum(keepn_old):,} galaxies ({np.sum(unobserved_n_old) / np.sum(keepn_old) * 100:.2f}% lost).\")\n",
    "\n",
    "if KEEP_ONLY_OBSERVED:\n",
    "    keep = np.all([keep, ntiles_mine >= KEEP_PASSES, ~unobserved], axis=0)\n",
    "else:\n",
    "    keep = np.all([keep, ntiles_mine >= KEEP_PASSES], axis=0)\n",
    "\n",
    "obj_type = obj_type[keep]\n",
    "dec = dec[keep]\n",
    "ra = ra[keep]\n",
    "z_phot = z_phot[keep]\n",
    "z_obs = z_obs[keep]\n",
    "target_id = target_id[keep] \n",
    "flux_r = flux_r[keep]\n",
    "app_mag_r = app_mag_r[keep]\n",
    "app_mag_g = app_mag_g[keep]\n",
    "g_r_apparent = g_r_apparent[keep]\n",
    "p_obs = p_obs[keep]\n",
    "unobserved = unobserved[keep]\n",
    "deltachi2 = deltachi2[keep]\n",
    "ntiles = ntiles[keep]\n",
    "abs_mag_r = abs_mag_r[keep]\n",
    "abs_mag_g = abs_mag_g[keep]\n",
    "#tiles = tiles[keep]\n",
    "#ztileid = ztileid[keep]\n",
    "ntiles_mine = ntiles_mine[keep]\n",
    "tileids = tileids[keep]\n",
    "tile_id = tile_id[keep] if 'TILEID' in table.columns else None\n",
    "#numobs = numobs[keep]\n",
    "#tile_locid = tile_locid[keep]\n",
    "#abs_mag_sdss = abs_mag_sdss[keep]\n",
    "#sdss_g_r = sdss_g_r[keep]\n",
    "G_R_JM1 = G_R_JM1[keep]\n",
    "dn4000 = dn4000[keep]\n",
    "ref_cat = ref_cat[keep]\n",
    "maskbits = maskbits[keep]\n",
    "indexes_not_assigned = np.flatnonzero(unobserved)\n",
    "quiescent = quiescent[keep]\n",
    "\n",
    "after_count = len(dec)\n",
    "\n",
    "print(f\"\\nAfter all filters we have {after_count:,} of the original {before_count:,} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This 10-pass catalog would have 100,171 galaxies (2.23% lost).\n",
    "\n",
    "#This 10-pass catalog would have 100,171 galaxies (2.23% lost).\n",
    "#This 10-pass catalog would have 100,171 galaxies (2.23% lost).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This reproduces the above.\n",
    "df = table[names].to_pandas()\n",
    "df = df[df['APP_MAG_R'] < APP_MAG_CUT]\n",
    "df['UNOBSERVED'] = np.isnan(df['Z'])\n",
    "\n",
    "galaxy_observed_filter = df['SPECTYPE'] == b'GALAXY'\n",
    "deltachi2_filter = df['DELTACHI2'] > 40\n",
    "observed_requirements = np.all([galaxy_observed_filter, df['Z'] > Z_MIN, df['Z'] < Z_MAX, deltachi2_filter], axis=0)\n",
    "\n",
    "treat_as_unobserved = np.all([galaxy_observed_filter, np.invert(deltachi2_filter)], axis=0)\n",
    "\n",
    "df['UNOBSERVED'] = np.logical_or(df['UNOBSERVED'], treat_as_unobserved)\n",
    "keep = np.all([np.logical_or(observed_requirements, df['UNOBSERVED'])], axis=0)\n",
    "\n",
    "if KEEP_ONLY_OBSERVED:\n",
    "    keep = np.all([keep, df['NTILE_MINE'] >= KEEP_PASSES, ~df['UNOBSERVED']], axis=0)\n",
    "else:\n",
    "    keep = np.all([keep, df['NTILE_MINE'] >= KEEP_PASSES], axis=0)\n",
    "\n",
    "df = df[keep]\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make maps\n",
    "two_pass_filter = ntiles_mine >= 2 \n",
    "three_pass_filter = ntiles_mine >= 3 \n",
    "four_pass_filter = ntiles_mine >= 4 \n",
    "\n",
    "ra2 = ra[two_pass_filter]\n",
    "dec2 = dec[two_pass_filter]\n",
    "tileids2 =  tileids[two_pass_filter]\n",
    "unobserved2 = unobserved[two_pass_filter]\n",
    "\n",
    "ra3 = ra[three_pass_filter]\n",
    "dec3 = dec[three_pass_filter]\n",
    "tileids3 =  tileids[three_pass_filter]\n",
    "unobserved3 = unobserved[three_pass_filter]\n",
    "\n",
    "ra4 = ra[four_pass_filter]\n",
    "dec4 = dec[four_pass_filter]\n",
    "tileids4 =  tileids[four_pass_filter]\n",
    "unobserved4 = unobserved[four_pass_filter]\n",
    "\n",
    "one_pass_df = pd.DataFrame({'RA': ra, 'DEC': dec, 'Z_ASSIGNED_FLAG': unobserved, 'TILEID': tileids})\n",
    "two_pass_df = pd.DataFrame({'RA': ra[two_pass_filter], 'DEC': dec[two_pass_filter], 'Z_ASSIGNED_FLAG': unobserved2, 'TILEID': tileids2})\n",
    "three_pass_df = pd.DataFrame({'RA': ra3, 'DEC': dec3, 'Z_ASSIGNED_FLAG': unobserved3, 'TILEID': tileids3})\n",
    "four_pass_df = pd.DataFrame({'RA': ra4, 'DEC': dec4, 'Z_ASSIGNED_FLAG': unobserved4, 'TILEID': tileids4})\n",
    "\n",
    "#fig=make_map(ra, dec)\n",
    "#ra_4 = ra[four_pass_filter]\n",
    "#dec_4 = dec[four_pass_filter]\n",
    "#print(f\"Number of 4-pass galaxies: {len(ra_4)}, number of 3-pass galaxies: {len(ra)}\")\n",
    "#fig=make_map(ra_4, dec_4, fig=fig, alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(ntiles_mine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiles_BGS = read_tiles_Y1_main()\n",
    "plot_positions(one_pass_df, three_pass_df, tiles_df=tiles_BGS, DEG_LONG=5, split=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See where missing photo-z's are\n",
    "#idx_no_zphot = z_phot == NO_PHOTO_Z\n",
    "#no_photoz_df = pd.DataFrame({'RA': ra[idx_no_zphot], 'DEC': dec[idx_no_zphot], 'Z_ASSIGNED_FLAG': unobserved[idx_no_zphot], 'TILEID': tileids[idx_no_zphot]})\n",
    "#plot_positions(one_pass_df, no_photoz_df, tiles_df=tiles_BGS, DEG_LONG=3, split=False)\n",
    "\n",
    "fig=make_map(ra3, dec3, dpi=300, alpha=0.02)\n",
    "#ra_4 = ra[four_pass_filter]\n",
    "#dec_4 = dec[four_pass_filter]\n",
    "#print(f\"Number of 4-pass galaxies: {len(ra_4)}, number of 3-pass galaxies: {len(ra)}\")\n",
    "#fig=make_map(ra[idx_no_zphot], dec[idx_no_zphot], fig=fig, alpha=0.1)\n",
    "#fig=make_map(ra[ntiles_mine >= 10 ], dec[ntiles_mine >= 10 ], fig=fig, alpha=0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_map(ra, dec, fig=fig, alpha=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(table['ABSMAG01_SDSS_R'], bins=100, range=(-25, -15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the random redshift hashtables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For comparison\n",
    "with open(IAN_MXXL_LOST_APP_TO_Z_FILE, 'rb') as f:\n",
    "    mxxl_app_mag_bins, mxxl_the_map = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is how we did it for MXXL\n",
    "app_mag_bins, the_map = build_app_mag_to_z_map(app_mag_r[~unobserved], z_obs[~unobserved])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New better way I think\n",
    "app_mag_bins2, the_map2 = build_app_mag_to_z_map_2(app_mag_r[~unobserved], z_obs[~unobserved])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_mag_bins2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in the_map2.keys():\n",
    "    print(f\"App mag bin {key} has {len(the_map2[key])} galaxies.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    m = app_mag_r[~unobserved][i]\n",
    "    zp = z_phot[~unobserved][i]\n",
    "    distribution = the_map2[np.digitize(m, app_mag_bins2)]\n",
    "    percentiles = np.percentile(distribution, [5, 16, 50, 84, 95])\n",
    "    mean = np.mean(distribution)\n",
    "    std = np.std(distribution)\n",
    "    # what sigma does zp lie at?\n",
    "    sigma = (zp - mean) / std\n",
    "    print(f\"App mag: {m:.2f}, z_p: {zp:.4f}, sigma: {sigma:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And now the photo-z infused way (there is a version 3 as well which fills empty/small bins with neighborin values)\n",
    "app_mag_bins3, z_phot_bins, the_map3 = build_app_mag_to_z_map_4(app_mag_r[~unobserved].copy(), z_phot[~unobserved].copy(),  z_obs[~unobserved].copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.printoptions(precision=4, linewidth=200):\n",
    "    for k in the_map3.keys():\n",
    "        print(f\"Key {k} has {len(the_map3[k])} galaxies\")\n",
    "        #print(f\"Key {k} has {len(the_map3[k])} galaxies: {np.percentile(the_map3[k], [5, 16, 50, 84, 95])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(0, 0.5, 50)\n",
    "\n",
    "#print(bins)\n",
    "plt.figure(figsize=(10, 6))\n",
    "trash=plt.hist(the_map2[33],bins=bins, color='green', density=True, histtype='step', label=app_mag_bins2[33], linestyle='dashed')\n",
    "trash=plt.hist(the_map2[284],bins=bins, color='darkred', density=True, histtype='step', label=app_mag_bins2[284], linestyle='dashed')\n",
    "\n",
    "trash=plt.hist(the_map3[39, 10], bins=bins, color=[0.0, 1.0, 0.0], density=True, histtype='step', label=f\"z={z_phot_bins[10]:.3f} r={app_mag_bins3[39]:.2f}\", linestyle='dotted')\n",
    "trash=plt.hist(the_map3[30, 10], bins=bins, color=[0.1, 0.8, 0.0], density=True, histtype='step', label=f\"z={z_phot_bins[10]:.3f} r={app_mag_bins3[30]:.2f}\", linestyle='dotted')\n",
    "trash=plt.hist(the_map3[22, 10], bins=bins, color=[0.2, 0.6, 0.0], density=True, histtype='step', label=f\"z={z_phot_bins[10]:.3f} r={app_mag_bins3[22]:.2f}\", linestyle='dotted')\n",
    "trash=plt.hist(the_map3[15, 10], bins=bins, color=[0.3, 0.4, 0.0], density=True, histtype='step', label=f\"z={z_phot_bins[10]:.3f} r={app_mag_bins3[15]:.2f}\", linestyle='dotted')\n",
    "\n",
    "trash=plt.hist(mxxl_the_map[29],bins=bins, color='green', density=True, histtype='step', label=mxxl_app_mag_bins[29])\n",
    "#trash=plt.hist(app_mag_bins_read[50],bins=bins, color='orange', density=True, histtype='step', label=app_mag_bins_read[50], linestyle='dotted')\n",
    "trash=plt.hist(mxxl_the_map[90],bins=bins, color='darkred', density=True, histtype='step', label=mxxl_app_mag_bins[90])\n",
    "plt.legend()\n",
    "\n",
    "plt.xlim(0, 0.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(BGS_Y3_LOST_APP_TO_Z_FILE, 'wb') as f:\n",
    "    pickle.dump((app_mag_bins2, the_map2), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(BGS_Y3_LOST_APP_AND_ZPHOT_TO_Z_FILE, 'wb') as f:\n",
    "    pickle.dump((app_mag_bins3, z_phot_bins, the_map3), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make SDSS BGS-Cut Variant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=make_map(ra3, dec3)\n",
    "\n",
    "sdss_df = pd.read_csv(SDSS_v2_DAT_FILE, header=None, sep=' ', names=['RA', 'DEC', 'Z', 'LOGLGAL', 'VMAX', 'color_flag', 'chi'])\n",
    "sdss_gp_df = pd.read_csv(SDSS_v2_GALPROPS_FILE, delimiter=' ', header=None, names=('MAG_G', 'MAG_R', 'SIGMA_V', 'DN4000', 'CONCENTRATION', 'LOG_M_STAR', 'Z_ASSIGNED_FLAG'))\n",
    "\n",
    "#fig=make_map(sdss_df['RA'].to_numpy(), sdss_df['DEC'].to_numpy(), alpha=0.3, fig=fig)\n",
    "make_map(sdss_df.loc[sdss_gp_df.Z_ASSIGNED_FLAG == AssignedRedshiftFlag.NEIGHBOR_ONE.value, 'RA'].to_numpy(), sdss_df.loc[sdss_gp_df.Z_ASSIGNED_FLAG == AssignedRedshiftFlag.NEIGHBOR_ONE.value, 'DEC'].to_numpy(), alpha=0.5, dotsize=0.05, fig=fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sra = sdss_df['RA'].to_numpy()    \n",
    "ra_angles = coord.Angle(sra*u.degree)\n",
    "ra_angles = ra_angles.wrap_at(180*u.degree)\n",
    "sras = ra_angles.value\n",
    "sdec = sdss_df['DEC'].to_numpy()\n",
    "print(sra.min(), sra.max())\n",
    "print(sdec.min(), sdec.max())\n",
    "\n",
    "#stripe_to_cut = np.logical_and(sras < 90, sras > -90) # removes the 3 stripes\n",
    "block_to_cut = np.logical_and(np.logical_and(sra > 130, sra < 195), np.logical_and(sdec > 15, sdec < 55))\n",
    "block2_to_cut = np.logical_and(np.logical_and(sra > 210, sra < 225), np.logical_and(sdec > 15, sdec < 30))\n",
    "rows_to_cut = np.logical_or(block2_to_cut, block_to_cut)\n",
    "\n",
    "print(f\"Cutting {np.sum(rows_to_cut)} rows from SDSS data\")\n",
    "\n",
    "sdss_cut = sdss_df.loc[~rows_to_cut]\n",
    "make_map(sdss_cut['RA'].to_numpy(), sdss_cut['DEC'].to_numpy(), alpha=0.3, fig=fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{spectroscopic_complete_percent(sdss_gp_df.Z_ASSIGNED_FLAG):.1%} of SDSS v2 galaxies have a redshift\")\n",
    "print(f\"{spectroscopic_complete_percent(sdss_gp_df[~rows_to_cut].Z_ASSIGNED_FLAG):.1%} of SDSS BGS-Cut galaxies have a redshift\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write this new catalog to a file\n",
    "sdss_cut.to_csv(SDSS_BGSCUT_DAT_FILE, sep=' ', header=False, index=False)\n",
    "sdss_gp_df.loc[~rows_to_cut].to_csv(SDSS_BGSCUT_GALPROPS_FILE, sep=' ', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test reading the results\n",
    "test1 = pd.read_csv(SDSS_BGSCUT_DAT_FILE, delimiter=' ', header=None, names=['RA', 'DEC', 'Z', 'LOGLGAL', 'VMAX', 'color_flag', 'chi'])\n",
    "test2 = pd.read_csv(SDSS_BGSCUT_GALPROPS_FILE, delimiter=' ', header=None, names=('MAG_G', 'MAG_R', 'SIGMA_V', 'DN4000', 'CONCENTRATION', 'LOG_M_STAR', 'Z_ASSIGNED_FLAG'))\n",
    "print(len(test1))\n",
    "print(len(test2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Known area of SDSS is .179, assuming constant density of galaxies we can estimate the area of the cut catalog\n",
    "print(399434 / len(sra) * .179) \n",
    "\n",
    "# Compare with my crappy other estimation code\n",
    "print(f\"Orig area estimate: {estimate_frac_area(sra, sdec)}\")\n",
    "print(f\"Cut area estimate: {estimate_frac_area(test1.RA.to_numpy(), test1['DEC'].to_numpy())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quality Cuts Research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Siena Galaxy Atlas Analysis (SGA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(ref_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understand how many galaxies are affected by Siena Galaxy Atlas (SGA) Masks\n",
    "has_a_maskbit = maskbits != 0\n",
    "idx_with_masks = np.flatnonzero(maskbits)\n",
    "print(f\"{np.sum(has_a_maskbit):,} galaxies ({np.sum(has_a_maskbit) / len(maskbits) * 100:.2f}%) have a maskbit set.\")\n",
    "\n",
    "unobserved_with_maskbits = np.logical_and(has_a_maskbit, unobserved)\n",
    "print(f\"{np.sum(unobserved_with_maskbits):,} galaxies ({np.sum(unobserved_with_maskbits) / len(maskbits) * 100:.2f}%) have a maskbit set and are unobserved.\")\n",
    "\n",
    "# See https://www.legacysurvey.org/dr9/bitmasks/\n",
    "# https://github.com/legacysurvey/legacypipe/blob/master/py/legacypipe/bits.py\n",
    "BITMASK_SGA = 0x1000 \n",
    "sga_collision = (maskbits & BITMASK_SGA) != 0\n",
    "print(f\"{np.sum(sga_collision):,} galaxies ({np.sum(sga_collision) / len(maskbits) * 100:.2f}%) have a SGA collision.\")\n",
    "\n",
    "#sga_central = np.logical_or(ref_cat == b'L3', ref_cat == b'G2', ref_cat == b'GE')\n",
    "sga_central = ref_cat == b'L3'\n",
    "print(f\"{np.sum(sga_central):,} galaxies ({np.sum(sga_central) / len(maskbits) * 100:.2f}%) are SGA centrals.\")\n",
    "\n",
    "to_remove = sga_collision & ~sga_central\n",
    "print(f\"{np.sum(to_remove):,} galaxies ({np.sum(to_remove) / len(maskbits) * 100:.2f}%) have a SGA collision and are not SGA centrals.\")\n",
    "\n",
    "# Examine color of these compared to the rest\n",
    "g_r_sga = g_r_apparent[to_remove]\n",
    "bins = np.linspace(-2, 2, 100)\n",
    "plt.hist(g_r_sga, bins=bins, alpha=0.5, label='SGA Collision', density=True)\n",
    "g_r_rest = g_r_apparent[np.logical_and(~unobserved, z_obs < 0.05)]\n",
    "plt.hist(g_r_rest, bins=bins, alpha=0.5, label='Rest', density=True)\n",
    "plt.legend()\n",
    "\n",
    "to_remove_blue = to_remove & (g_r_apparent < 0.8)\n",
    "print(f\"{np.sum(to_remove_blue):,} galaxies ({np.sum(to_remove_blue) / len(maskbits) * 100:.2f}%) have a SGA collision, are not SGA centrals, and are blue enough to remove.\")\n",
    "\n",
    "df_print = pd.DataFrame({'RA': ra[to_remove_blue], 'DEC': dec[to_remove_blue]})\n",
    "df_print.to_csv(OUTPUT_FOLDER + f'sga_collisions.csv', index=False)\n",
    "\n",
    "df_print = pd.DataFrame({'RA': ra[to_remove & ~to_remove_blue], 'DEC': dec[to_remove & ~to_remove_blue]})\n",
    "df_print.to_csv(OUTPUT_FOLDER + f'sga_collisions_not_blue_enough.csv', index=False)\n",
    "\n",
    "# Inspecting these, I see that most are galaxies, not HII regions. Not sure if we want to remove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fracflux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(MASKBITS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interesting = df.loc[df['MASKBITS'] & MASKBITS['NPRIMARY'] != 0]\n",
    "print(f\"Number of interesting objects: {len(interesting)}\")\n",
    "interesting.reset_index().loc[0:10, ['RA', 'DEC', 'MASKBITS']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(np.isnan(df['FRACFLUX_R']).sum() + np.isnan(df['FRACFLUX_G']).sum() + np.isnan(df['FRACFLUX_Z']).sum())\n",
    "\n",
    "#orig_table = Table.read(BGS_ANY_FULL_FILE, format='fits')\n",
    "FF_CUT = 0.35\n",
    "# LOW Z folks used 0.35 for this. I think we can get away with cutting less than that.\n",
    "ff_g = df['FRACFLUX_G'] < FF_CUT\n",
    "ff_r = df['FRACFLUX_R'] < FF_CUT\n",
    "ff_z = df['FRACFLUX_Z'] < FF_CUT\n",
    "\n",
    "print(f\"There are {len(df):,} galaxies in the sample.\")\n",
    "print(f\"There are {np.sum(~ff_g):,} galaxies with FRACFLUX_G > {FF_CUT}.\")\n",
    "print(f\"There are {np.sum(~ff_r):,} galaxies with FRACFLUX_R > {FF_CUT}.\")\n",
    "print(f\"There are {np.sum(~ff_z):,} galaxies with FRACFLUX_Z > {FF_CUT}.\")\n",
    "ff_mask = np.sum([ff_g, ff_r, ff_z], axis=0) >= 2\n",
    "\n",
    "# I manually inspected the first 50 of these and I'm only unhappy about cutting 7 of them.\n",
    "# More than 7 did look like galaxies but were quite complicated overlappin situations.\n",
    "removed = df[~ff_mask]\n",
    "print(f\"Removing {len(removed):,} galaxies ({len(removed) / len(df) * 100:.2f}%) with 2 or more FRACFLUX values > {FF_CUT}.\")\n",
    "\n",
    "# How many were spectroscopic ones?\n",
    "spectro_removed = removed[~np.isnan(removed['Z'])]\n",
    "print(f\"Of the removed galaxies, {len(spectro_removed):,} ({len(spectro_removed) / len(removed) * 100:.2f}%) had spectra and survived other cuts.\")\n",
    "\n",
    "print(np.isnan(df['Z']).sum() / len(df))\n",
    "print(np.isnan(removed['Z']).sum() / len(removed))\n",
    "\n",
    "removed.reset_index().loc[0:10, ['FRACFLUX_G','FRACFLUX_Z','FRACFLUX_R']]\n",
    "removed.reset_index().loc[0:100,['RA', 'DEC']].to_csv(\"removed3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Photo-Z vs Spec-Z Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"There are {(z_phot != NO_PHOTO_Z).sum():,} ({(z_phot != NO_PHOTO_Z).sum() / len(z_phot):.1%}) targets with phot-z\")\n",
    "\n",
    "good_idx = np.flatnonzero((z_phot != NO_PHOTO_Z) & ~unobserved) #& ~(np.isclose(z_phot, z_obs, atol=0.00025, rtol=0.000001)))\n",
    "print(f\"Amongst observed galaxies there are {len(good_idx):,} ({len(good_idx) / np.sum(~unobserved) * 100:.2f}%) galaxies with photo-z.\")\n",
    "\n",
    "unobserved_with_photz = np.flatnonzero((z_phot != NO_PHOTO_Z) & unobserved)\n",
    "print(f\"Amongst unobserved galaxies there are {len(unobserved_with_photz):,} ({len(unobserved_with_photz) / np.sum(unobserved) * 100:.2f}%) with photo-z.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_z = z_phot[good_idx] - z_obs[good_idx]\n",
    "plt.hist(delta_z, bins=500, range=(-0.1, 0.1))\n",
    "#plt.yscale(\"log\")\n",
    "plt.title(\"Photo-z Quality\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xlabel(\"z_phot - z_spec\")\n",
    "plt.ylim(0, 20000)\n",
    "\n",
    "# add bars for my z_thresh\n",
    "plt.axvline(-SIM_Z_THRESH, color='red')\n",
    "plt.axvline(SIM_Z_THRESH, color='red')\n",
    "\n",
    "percentiles = np.percentile(delta_z, [16, 50, 84])\n",
    "print(f\"Median delta z: {percentiles[1]:.4f}, 16th percentile: {percentiles[0]:.4f}, 84th percentile: {percentiles[2]:.4f}\")\n",
    "percentiles = np.percentile(delta_z, [2.5, 97.5])\n",
    "print(f\"2.5th percentile: {percentiles[0]:.4f}, 97.5th percentile: {percentiles[1]:.4f}\")\n",
    "# add bars for the percentiles\n",
    "#plt.axvline(percentiles[0], color='green')\n",
    "#plt.axvline(percentiles[2], color='green')\n",
    "\n",
    "# What % fall within 0.005 of the true redshift?\n",
    "within_5_milli = np.abs(delta_z) < SIM_Z_THRESH\n",
    "print(f\"{np.sum(within_5_milli) / len(delta_z) * 100:.2f}% of galaxies have a photometric redshift within {SIM_Z_THRESH} of the spectroscopic redshift.\")\n",
    "\n",
    "# Now look only at quiescent galaxies less than 10^9 solar luminosities\n",
    "# TODO \n",
    "#luminosity = abs_mag_r_to_log_solar_L(app_mag_to_abs_mag_k(app_mag_r, z_obs, g_r_apparent))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define Gaussian and Lorentzian functions\n",
    "def gaussian(x, amp, mean, stddev):\n",
    "    return amp * np.exp(-((x - mean) ** 2) / (2 * stddev ** 2))\n",
    "\n",
    "def lorentzian(x, amp, mean, gamma, exp):\n",
    "    return amp * gamma**2 / (np.power(np.abs(x - mean), exp) + gamma**2)\n",
    "\n",
    "# Prepare data\n",
    "abs_delta_z = delta_z\n",
    "hist, bin_edges = np.histogram(abs_delta_z, bins=1000, range=(-0.2, 0.2), density=True)\n",
    "bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "\n",
    "# Fit Gaussian\n",
    "popt_gauss, _ = curve_fit(gaussian, bin_centers, hist, p0=[1, 0, 0.01], maxfev=100000)\n",
    "# Poor fit\n",
    "\n",
    "# Fit Lorentzian\n",
    "popt_lorentz, _ = curve_fit(lorentzian, bin_centers, hist, p0=[24, 0.0001, 0.0153, 2.0], maxfev=100000)\n",
    "#Lorentzian fit parameters: amp=22.902018525656057, mean=8.151405674056218e-05, gamma=0.009227129158691942, exp=2.2857148452956757\n",
    "\n",
    "\n",
    "# Plot histogram and fits\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(abs_delta_z, bins=1000, range=(-0.2, 0.2), density=True, alpha=0.6, label='Data')\n",
    "plt.plot(bin_centers, gaussian(bin_centers, *popt_gauss), label='Gaussian fit', color='red')\n",
    "plt.plot(bin_centers, lorentzian(bin_centers, *popt_lorentz), label='~Lorentzian fit', color='green')\n",
    "plt.xlabel('z_phot - z_spec')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.title('Fitting delta_z with Gaussian and ~Lorentzian')\n",
    "plt.show()\n",
    "\n",
    "# Print fit parameters\n",
    "print(f\"Gaussian fit parameters: amp={popt_gauss[0]}, mean={popt_gauss[1]}, stddev={popt_gauss[2]}\")\n",
    "print(f\"Lorentzian fit parameters: amp={popt_lorentz[0]}, mean={popt_lorentz[1]}, gamma={popt_lorentz[2]}, exp={popt_lorentz[3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SV3 Analysis\n",
    "\n",
    "SV3 is composed of 20 regions where 10 or 11 exposures eacj were taken, almost completely on top of each other.  Our SV3 analysis takes the inner part of these patches (NTILE_MINE >= 10) of these regions as the data set.  \n",
    "\n",
    "Then, we can eliminate 1 tile from each of these regions to make test sets in order to view our systematics as a function of NTILE_MINE. The order they are eliminated in matters; we need to go backwards in time.}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a DataFrame filtered down to the galaxies we want to keep\n",
    "sv3_merged_table = Table.read(IAN_BGS_SV3_MERGED_FILE, format='fits')\n",
    "sv3_merged_table.remove_column('NEAREST_TILEIDS')\n",
    "sv3_df = sv3_merged_table.to_pandas()\n",
    "print(len(sv3_df))\n",
    "sv3_df['APP_MAG_R'] = get_app_mag(sv3_df['FLUX_R'])\n",
    "unobserved = sv3_merged_table['Z'].astype(\"<i8\") == 999999\n",
    "galaxy_observed_filter = sv3_df['SPECTYPE'] == b'GALAXY'\n",
    "redshift_filter = sv3_df['Z'] > Z_MIN\n",
    "redshift_hi_filter = sv3_df['Z'] < Z_MAX\n",
    "deltachi2_filter = sv3_df['DELTACHI2'] > 40\n",
    "app_mag_filter = sv3_df['APP_MAG_R'] < 20.3\n",
    "observed_requirements = np.all([galaxy_observed_filter, app_mag_filter, redshift_filter, redshift_hi_filter, deltachi2_filter], axis=0)\n",
    "treat_as_unobserved = np.all([galaxy_observed_filter, app_mag_filter, np.invert(deltachi2_filter)], axis=0)\n",
    "\n",
    "unobserved = np.all([app_mag_filter, np.logical_or(unobserved, treat_as_unobserved)], axis=0)\n",
    "sv3_df['OBSERVED'] = np.invert(unobserved)\n",
    "keep = np.all([np.logical_or(observed_requirements, unobserved)], axis=0)\n",
    "keep = np.all([keep, sv3_df['NTILE_MINE'] >= 10], axis=0)\n",
    "\n",
    "sv3_df = sv3_df.loc[keep] \n",
    "sv3_df.reset_index(drop=True, inplace=True)\n",
    "print(len(sv3_df))\n",
    "\n",
    "# Initialize new columns for observed as function of N pass\n",
    "for i in range(0, 12):\n",
    "    sv3_df[f'OBSERVED_{i}'] = sv3_df['OBSERVED']\n",
    "\n",
    "for FAINT in [False, True]:\n",
    "\n",
    "    if not FAINT:\n",
    "        mag_filter = sv3_df['APP_MAG_R'] < 19.5\n",
    "    else:\n",
    "        mag_filter = sv3_df['APP_MAG_R'] > 19.5\n",
    "        \n",
    "    print(f\"{len(sv3_df[mag_filter]) / 138.192} galaxies per sq degree\")\n",
    "\n",
    "    for patch_number in range(len(sv3_regions_sorted)):\n",
    "        tilelist = sv3_regions_sorted[patch_number]\n",
    "        #print(f'Patch {patch_number} - TILE IDs: {tilelist}')\n",
    "        \n",
    "        row_selector = np.logical_and(sv3_df['TILEID'].isin(tilelist), mag_filter)\n",
    "\n",
    "        #one_patch_df = sv3_df[sv3_df['TILEID'].isin(tilelist)]\n",
    "        #print(f\"{len(one_patch_df)} galaxies, {np.sum(one_patch_df['OBSERVED']) / len(one_patch_df) :.1%} of the targets are observed\")\n",
    "        #one_patch_df[f'OBSERVED_{len(tilelist)}'] = one_patch_df['OBSERVED']\n",
    "        \n",
    "        #print (\"Remove tiles in reverse TILEID order:\")\n",
    "        for i in np.flip(np.arange(0, len(tilelist))):\n",
    "            tileid = tilelist[i]\n",
    "            observed_by_this_tile = sv3_df.loc[row_selector, 'TILEID'] == tileid\n",
    "            #print(f'{np.sum(observed_by_this_tile)} galaxies were observed by tile {tileid} ({i+1}/{len(tilelist)})')\n",
    "            prev = sv3_df.loc[row_selector, f'OBSERVED_{i+1}']\n",
    "            sv3_df.loc[row_selector, f'OBSERVED_{i}'] = np.where(observed_by_this_tile, False, prev)\n",
    "            \n",
    "        #for i in np.flip(np.arange(0, len(tilelist)+1)):\n",
    "        #    if FAINT:\n",
    "        #        totals_observed_faint[i] += np.sum(sv3_df.loc[row_selector, f'OBSERVED_{i}'])\n",
    "        #        totals_all_faint[i] += len(sv3_df.loc[row_selector])\n",
    "        #    else:\n",
    "        #        totals_observed_bright[i] += np.sum(sv3_df.loc[row_selector, f'OBSERVED_{i}'])\n",
    "        #        totals_all_bright[i] += len(sv3_df.loc[row_selector])\n",
    "\n",
    "            #print(f\"{np.sum(one_patch_df[f'OBSERVED_{i}']) / len(one_patch_df) :.1%} of the targets are observed with {i} passes\")\n",
    "                \n",
    "    #for i in range(1, 12):\n",
    "    #    if FAINT:\n",
    "    #        print(f\"{totals_observed_faint[i]:,} ({totals_observed_faint[i] / totals_all_faint[i]:.1%}) faint galaxies are observed with {i} passes\")\n",
    "    #    else: \n",
    "    #        print(f\"{totals_observed_bright[i]:,} ({totals_observed_bright[i] / totals_all_bright[i]:.1%}) bright galaxies are observed with {i} passes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observed_faint = np.zeros(12, dtype=int)\n",
    "observed_bright = np.zeros(12, dtype=int)\n",
    "\n",
    "total_faint = np.sum(sv3_df['APP_MAG_R'] > 19.5)\n",
    "total_bright = np.sum(sv3_df['APP_MAG_R'] < 19.5)\n",
    "\n",
    "for i in range(0, 12):\n",
    "    observed_faint[i] = np.sum(sv3_df.loc[sv3_df['APP_MAG_R'] > 19.5, f'OBSERVED_{i}'])\n",
    "    observed_bright[i] = np.sum(sv3_df.loc[sv3_df['APP_MAG_R'] < 19.5, f'OBSERVED_{i}'])\n",
    "\n",
    "\n",
    "plt.plot(observed_bright / total_bright, color='b', label=\"BGS BRIGHT ME\")\n",
    "plt.plot(observed_faint / total_faint, color='orange', label=\"BGS FAINT ME\")\n",
    "plt.plot([1,2,3,4], [.29, .52, 0.68, .81], '--', color='b', label=\"BGS BRIGHT PAPER\")\n",
    "plt.plot([1,2,3,4], [.15, .32, 0.47, .62], '--', color='orange', label=\"BGS FAINT PAPER\")\n",
    "plt.xlabel(\"Number of passes\")\n",
    "plt.ylabel(\"Fraction of targets observed\")\n",
    "plt.title(\"SV3 BGS Completeness\")\n",
    "plt.xticks(np.arange(0, 12))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sv3_df.loc[sv3_df['APP_MAG_R'] < 19.5, 'OBSERVED'].count() - sv3_df.loc[sv3_df['APP_MAG_R'] < 19.5, 'OBSERVED'].sum())\n",
    "print(sv3_df.loc[sv3_df['APP_MAG_R'] < 19.5, 'OBSERVED'].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above numbers do not seem to track with the Y1 data. In Y1 no region has more than 4 passes so how do I have a fiber incompleteness better than the above number?\n",
    "\n",
    "Also Figure 17 of https://iopscience.iop.org/article/10.3847/1538-3881/accff8/pdf disagrees with my above analysis. So what is above is wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nearest Neighbor Analysis (for SV3 Merged File)\n",
    "\n",
    "Have a nearest neighbor catalog that is all the actually observed galaxies in SV3 with my Y3 supplement. \n",
    "\n",
    "Can run analysis on entire sample or a subset of 'pretend-to-be unobserved' galaxies (~obs_7p)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create bin files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percentiles for 10 equal bins\n",
    "percentiles = np.linspace(0, 100, 16)\n",
    "bin_edges = np.percentile(z_obs, percentiles)\n",
    "\n",
    "with np.printoptions(precision=3, linewidth=200):\n",
    "    print(f\"Even number z bin: {np.array(bin_edges[1:])}\")\n",
    "    print(f\"The Z Bins we use: {Z_BINS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAKE SURE that above KEEP_PASSES is set to 1 and you filter out unobserved galaxies as we need a source of 'truth'\n",
    "assert KEEP_PASSES == 1 and np.sum(unobserved) == 0\n",
    "assert Z_MIN == 0.001 and Z_MAX == 0.5\n",
    "\n",
    "gmr = app_mag_g - app_mag_r\n",
    "Rk = app_mag_to_abs_mag_k(app_mag_r, z_obs, gmr, band='r')\n",
    "Gk = app_mag_to_abs_mag_k(app_mag_g, z_obs, gmr, band='g')\n",
    "quiescent = is_quiescent_BGS_gmr(None, Gk-Rk)\n",
    "\n",
    "# We will only consider the observed galaxies in SV3 as we need a source of truth for the analysis.\n",
    "# This is ~98% so this analysis should be representative.\n",
    "# As usual for SV3, only want galaxies that were fully covered by the rosetting pattern. (in_10p_zone)\n",
    "# We then pretend to have not observed ~20% of galaxies, as is the case in the main survey. (obs_7p)\n",
    "# TODO BUG Ashley says my method is wrong for doing this.\n",
    "in_10p_zone = ntiles_mine >= 10\n",
    "obs_7p = ~gc.drop_SV3_passes(3, tile_id, unobserved)\n",
    "\n",
    "print(np.sum(quiescent))\n",
    "print(np.sum(in_10p_zone))\n",
    "print(np.sum(obs_7p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_id.mask.sum() # BUG prevents using LOST_GALAXIES_ONLY=True. Is it because of Y3 supplement has no TILE_ID?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "### Create NEIGHBOR_ANALYSIS_SV3_BINS_FILE\n",
    "###\n",
    "p_obs = np.where(np.isnan(p_obs), 0.689, p_obs) # Fill in missing p_obs with 0.689, the Y3 mean\n",
    "newobj = NNAnalyzer_cic.from_data(dec, ra, z_obs, app_mag_r, Rk, g_r_apparent, quiescent, obs_7p, p_obs)\n",
    "#newobj.set_row_locator( np.logical_and(obs_10p, app_mag_r < 19.5) ) # 10p inner regions and BRIGHT only\n",
    "newobj.set_row_locator(in_10p_zone)\n",
    "newobj.find_nn_properties(LOST_GALAXIES_ONLY=False) \n",
    "newobj.make_bins()\n",
    "newobj.save(NEIGHBOR_ANALYSIS_SV3_BINS_FILE)\n",
    "\n",
    "print(np.sum(newobj.all_ang_bincounts))\n",
    "print(np.sum(newobj.all_sim_z_bincounts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian smoothing version\n",
    "loaded_nna = NNAnalyzer_cic.from_results_file(NEIGHBOR_ANALYSIS_SV3_BINS_FILE)\n",
    "\n",
    "# Apply Gaussian filter\n",
    "loaded_nna.apply_gaussian_smoothing(0.75)\n",
    "\n",
    "# Save extra smoothed one\n",
    "loaded_nna.save(NEIGHBOR_ANALYSIS_SV3_BINS_SMOOTHED_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at bin results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_nna = NNAnalyzer_cic.from_results_file(NEIGHBOR_ANALYSIS_SV3_BINS_FILE)\n",
    "smoothed_nna = NNAnalyzer_cic.from_results_file(NEIGHBOR_ANALYSIS_SV3_BINS_SMOOTHED_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_nna.plot_angdist_appmag_per_zbin_cc(z_bin_numbers_to_plot=[2,4,6,8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothed_nna.plot_angdist_appmag_per_zbin_cc(z_bin_numbers_to_plot=[2,4,6,8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(smoothed_nna.all_ang_bincounts))\n",
    "print(np.prod(np.shape(smoothed_nna.all_ang_bincounts)))\n",
    "print(np.median(smoothed_nna.all_ang_bincounts))\n",
    "print(np.mean(smoothed_nna.all_ang_bincounts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These make P_obs seem useles...\n",
    "smoothed_nna.plot_angdist_pobs_per_zbin_cc(t_c=[1], nn_c=[0], z_bin_numbers_to_plot=[1,3,5,7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothed_nna.plot_angdist_pobs_per_zbin_cc(t_c=[1], nn_c=[1], z_bin_numbers_to_plot=[1,4,7,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO de-dupe with pre_process_BGS(...)\n",
    "\n",
    "# Examine photo-z NN relation\n",
    "NUM_NEIGHBORS = 30\n",
    "unobs_7p = ~obs_7p\n",
    "bright = app_mag_r[in_10p_zone] < 19.5\n",
    "use = bright & unobs_7p\n",
    "neighbor_indexes = np.zeros(shape=(NUM_NEIGHBORS, use.sum()), dtype=np.int32) # indexes point to CATALOG locations\n",
    "ang_distances = np.zeros(shape=(NUM_NEIGHBORS, use.sum()))\n",
    "\n",
    "catalog = coord.SkyCoord(ra=ra[in_10p_zone][obs_7p]*u.degree, dec=dec[in_10p_zone][obs_7p]*u.degree, frame='icrs')\n",
    "\n",
    "print(f\"Finding nearest {NUM_NEIGHBORS} neighbors... \", end='\\r')   \n",
    "for n in range(0, NUM_NEIGHBORS):\n",
    "    to_match = coord.SkyCoord(ra=ra[in_10p_zone][use]*u.degree, dec=dec[in_10p_zone][use]*u.degree, frame='icrs')\n",
    "    idx, d2d, d3d = coord.match_coordinates_sky(to_match, catalog, nthneighbor=n+1, storekdtree='sv3')\n",
    "    neighbor_indexes[n] = idx\n",
    "    ang_distances[n] = d2d.to(u.arcsec).value\n",
    "print(f\"Finding nearest {NUM_NEIGHBORS} neighbors... done!\")   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ztruth = z_obs[in_10p_zone][use]\n",
    "# fill zphot_fake with ztruth + a random gaussian draw around 0.0 with sigma of 0.01\n",
    "zphot = z_phot[in_10p_zone][use]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_pass_df.Z_ASSIGNED_FLAG = ~obs_7p\n",
    "plot_positions(one_pass_df, tiles_df=None, DEG_LONG=2.5, split=True, ra_min=one_pass_df.RA[0], dec_min=one_pass_df['DEC'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test of how gaussian filter works\n",
    "\n",
    "# Create a sample 2D array with zeros and a few ones\n",
    "array = np.zeros((30, 30))\n",
    "array[0, 0] = 1\n",
    "array[1, 0] = 1\n",
    "array[5, 5] = 1\n",
    "array[8, 5] = 1\n",
    "array[20, 20] = 3\n",
    "\n",
    "# Apply Gaussian filter\n",
    "sigma = 0.75  # Standard deviation for Gaussian kernel\n",
    "smoothed_array = gaussian_filter(array, sigma=sigma, axes=[0,1])\n",
    "\n",
    "assert np.isclose(np.sum(array), np.sum(smoothed_array))\n",
    "\n",
    "# Plot the original and smoothed arrays\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "ax[0].imshow(array, cmap='gray')\n",
    "ax[0].set_title('Original Array')\n",
    "ax[1].imshow(smoothed_array, cmap='gray')\n",
    "ax[1].set_title('Smoothed Array')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial look at Photo-z only matching to neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramspace = np.arange(0.001, 0.05, 0.001)\n",
    "                       \n",
    "cumulative_percent_correct_by_n_zp_firstonly = np.zeros((NUM_NEIGHBORS, len(paramspace)) , dtype=float)\n",
    "\n",
    "i = 0\n",
    "for PHOTOZ_MATCHING_THRESHOLD in paramspace:\n",
    "    \n",
    "    this_neighbor_correct = np.zeros(shape=(NUM_NEIGHBORS, len(ztruth)), dtype=bool)\n",
    "    z_phot_neighbor_match = np.zeros(shape=(NUM_NEIGHBORS, len(ztruth)), dtype=bool)\n",
    "    cumulative_percent_z_phot_neighbor_match = []\n",
    "    z_phot_first_neighbor_match_idx = np.ones(len(ztruth), dtype=int) * 999 # sentinal value for no match\n",
    "    z_phot_match_correct = np.zeros(shape=(NUM_NEIGHBORS, len(ztruth)), dtype=bool)\n",
    "    delta_z = np.zeros(shape=(NUM_NEIGHBORS, len(ztruth)), dtype=float)\n",
    "\n",
    "    percent_correct_by_n = []\n",
    "    percent_correct_by_n_zp = []\n",
    "    cumulative_percent_correct_by_n = []\n",
    "    cumulative_percent_correct_by_n_zp = []\n",
    "    cumulative_percent_correct_by_n_zp_closestonly = []\n",
    "    first_matched_but_incorrect = []\n",
    "\n",
    "    for n in range(0, NUM_NEIGHBORS):\n",
    "\n",
    "        z_neighbor = z_obs[in_10p_zone][obs_7p][neighbor_indexes[n]]\n",
    "        this_neighbor_correct[n] = close_enough(ztruth, z_neighbor)\n",
    "        percent_correct_by_n.append(this_neighbor_correct[n].sum() / len(ztruth))\n",
    "        any_neighbor_correct = this_neighbor_correct[0:n+1].max(axis=0) # max will be True if any neighbor is True\n",
    "        cumulative_percent_correct_by_n.append(any_neighbor_correct.sum() / len(ztruth))\n",
    "\n",
    "        z_phot_neighbor_match[n] = close_enough(zphot, z_neighbor, threshold=PHOTOZ_MATCHING_THRESHOLD)\n",
    "        any_z_phot_match = z_phot_neighbor_match[0:n+1].max(axis=0) # will be True if z_phot matches a neighbor\n",
    "        cumulative_percent_z_phot_neighbor_match.append(any_z_phot_match.sum() / len(ztruth))\n",
    "        z_phot_match_correct[n] = z_phot_neighbor_match[n] & this_neighbor_correct[n]\n",
    "        any_neighbor_zp_correct = z_phot_match_correct[0:n+1].max(axis=0) # will be True if z_phot matches a neighbor\n",
    "        percent_correct_by_n_zp.append(z_phot_match_correct[n].sum() / len(ztruth))\n",
    "        cumulative_percent_correct_by_n_zp.append(any_neighbor_zp_correct.sum() / len(ztruth))\n",
    "\n",
    "\n",
    "        # Now only consider the closest neighbor, by photo-z matching\n",
    "        delta_z[n] = zphot - z_neighbor\n",
    "        best_match_idx = np.argmin(delta_z[0:n+1], axis=0, keepdims=True)\n",
    "        closest_delta_z_correct = np.take_along_axis(z_phot_match_correct, best_match_idx, axis=0)[0] # will be True bset zphot match is correct\n",
    "        cumulative_percent_correct_by_n_zp_closestonly.append(closest_delta_z_correct.sum() / len(ztruth))\n",
    "\n",
    "        # Now only consider the first neighbor in order of angular distance\n",
    "        z_phot_first_neighbor_match_idx = np.minimum(z_phot_first_neighbor_match_idx, np.where(z_phot_neighbor_match[n],n,999))\n",
    "        has_match = z_phot_first_neighbor_match_idx != 999\n",
    "\n",
    "        # if z_phot_first_neighbor_match_idx is 999 for  row, first_correct will be False\n",
    "        # if z_phot_first_neighbor_match_idx is an index for row, first_correct will be z_phot_match_correct value for that row at that index\n",
    "        first_correct = np.repeat(False, len(ztruth))\n",
    "        first_correct[has_match] = z_phot_match_correct[z_phot_first_neighbor_match_idx[has_match], np.arange(len(ztruth))[has_match]]\n",
    "        cumulative_percent_correct_by_n_zp_firstonly[n,i] = (first_correct.sum() / len(ztruth))\n",
    "\n",
    "        first_matched_but_incorrect.append(cumulative_percent_z_phot_neighbor_match[n] - cumulative_percent_correct_by_n_zp_firstonly[n,i])\n",
    "\n",
    "    if i == 26:\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(range(1, NUM_NEIGHBORS+1), percent_correct_by_n, label=\"This Neighbor is correct z\")\n",
    "        plt.plot(range(1, NUM_NEIGHBORS+1), percent_correct_by_n_zp, label=\"This Neighbor photo-z matched and correct (thresh={PHOTOZ_MATCHING_THRESHOLD:.3})\")\n",
    "        plt.plot(range(1, NUM_NEIGHBORS+1), cumulative_percent_correct_by_n, label=\"Any Neighbor has correct z\")\n",
    "        plt.plot(range(1, NUM_NEIGHBORS+1), cumulative_percent_correct_by_n_zp, label=\"Any neighbor photo-z matched and correct (thresh={PHOTOZ_MATCHING_THRESHOLD:.3})\")\n",
    "        plt.plot(range(1, NUM_NEIGHBORS+1), cumulative_percent_correct_by_n_zp_closestonly, label=\"Minimum delta-z matched neighbor correct (thresh={PHOTOZ_MATCHING_THRESHOLD:.3})\")\n",
    "        plt.plot(range(1, NUM_NEIGHBORS+1), cumulative_percent_correct_by_n_zp_firstonly[:,i], label=f\"First photo-z matched neighbor correct (thresh={PHOTOZ_MATCHING_THRESHOLD:.3})\")\n",
    "        plt.plot(range(1, NUM_NEIGHBORS+1), cumulative_percent_z_phot_neighbor_match, label=\"Any Neighbor photo-z matched (thresh={PHOTOZ_MATCHING_THRESHOLD:.3})\")\n",
    "        plt.ylabel(\"Fraction\")\n",
    "        plt.xlabel(\"Nth Nearest Neighbor\")\n",
    "        plt.xticks(np.arange(1, NUM_NEIGHBORS+1))\n",
    "        plt.ylim(0, 0.8)\n",
    "        #plt.axhline(y=0.0125, color='r', linestyle='--', label=\"Random Chance\")\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.figure(figsize=(10, 5))\n",
    "        # stacked bar chart of \n",
    "        plt.bar(range(1, NUM_NEIGHBORS+1), cumulative_percent_correct_by_n_zp_firstonly[:,i], label=f\"First photo-z matched neighbor correct (thresh={PHOTOZ_MATCHING_THRESHOLD:.3})\")\n",
    "        plt.bar(range(1, NUM_NEIGHBORS+1), first_matched_but_incorrect, label=f\"First photo-z matched neighbor incorrect (thresh={PHOTOZ_MATCHING_THRESHOLD:.3})\", bottom = cumulative_percent_correct_by_n_zp_firstonly[:,i])\n",
    "\n",
    "        plt.ylabel(\"Fraction\")\n",
    "        plt.xlabel(\"Nth Nearest Neighbor\")\n",
    "        plt.xticks(np.arange(1, NUM_NEIGHBORS+1))\n",
    "        plt.ylim(0, 1.0)\n",
    "        plt.legend()\n",
    "    \n",
    "    i = i + 1\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the maximum point\n",
    "best = np.max(cumulative_percent_correct_by_n_zp_firstonly)\n",
    "best_idx = np.unravel_index(np.argmax(cumulative_percent_correct_by_n_zp_firstonly), cumulative_percent_correct_by_n_zp_firstonly.shape)\n",
    "print(cumulative_percent_correct_by_n_zp_firstonly.shape)\n",
    "print(best_idx)\n",
    "print(f\"Best photo-z match threshold: {paramspace[best_idx[1]]:.3f}, Neighbors={best_idx[0]+1}, {best:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, NUM_NEIGHBORS+1), cumulative_percent_correct_by_n_zp_firstonly[:,4],        label=f\"thresh={paramspace[4]:.3})\", color=[0.1, 0.8, 0])\n",
    "plt.plot(range(1, NUM_NEIGHBORS+1), cumulative_percent_correct_by_n_zp_firstonly[:,9],        label=f\"thresh={paramspace[9]:.3})\", color=[0.2, 0.7, 0])\n",
    "plt.plot(range(1, NUM_NEIGHBORS+1), cumulative_percent_correct_by_n_zp_firstonly[:,19],       label=f\"thresh={paramspace[19]:.3})\", color=[0.3, 0.6, 0])\n",
    "plt.plot(range(1, NUM_NEIGHBORS+1), cumulative_percent_correct_by_n_zp_firstonly[:,26], '--', label=f\"thresh={paramspace[26]:.3})\", color=[0.4, 0.5, 0])\n",
    "plt.plot(range(1, NUM_NEIGHBORS+1), cumulative_percent_correct_by_n_zp_firstonly[:,39],       label=f\"thresh={paramspace[39]:.3})\", color=[0.5, 0.4, 0])\n",
    "plt.plot(range(1, NUM_NEIGHBORS+1), cumulative_percent_correct_by_n_zp_firstonly[:,49],       label=f\"thresh={paramspace[49]:.3})\", color=[0.6, 0.3, 0])\n",
    "plt.plot(range(1, NUM_NEIGHBORS+1), cumulative_percent_correct_by_n_zp_firstonly[:,59],       label=f\"thresh={paramspace[59]:.3})\", color=[0.7, 0.2, 0])\n",
    "plt.plot(range(1, NUM_NEIGHBORS+1), cumulative_percent_correct_by_n_zp_firstonly[:,99],       label=f\"thresh={paramspace[99]:.3})\", color=[0.8, 0.1, 0])\n",
    "plt.ylabel(\"Fraction Correct\")\n",
    "plt.xlabel(\"Nth Nearest Neighbor\")\n",
    "plt.xticks(np.arange(1, NUM_NEIGHBORS+1))\n",
    "plt.ylim(0, 0.45)\n",
    "#plt.axhline(y=0.0125, color='r', linestyle='--', label=\"Random Chance\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a figure showing the best photo-z match for each galaxy\n",
    "plt.figure(figsize=(10, 5))\n",
    "for i in range(0, len(paramspace)):\n",
    "    color = [i/(len(paramspace)+1),0.5,0]\n",
    "    plt.plot(range(1, NUM_NEIGHBORS+1), cumulative_percent_correct_by_n_zp_firstonly[:,i], label=f\"Photo-z Match Threshold {paramspace[i]:.3f}\", color=color)\n",
    "plt.ylabel(\"Fraction Correct\")\n",
    "plt.xlabel(\"Nth Nearest Neighbor\")\n",
    "plt.xticks(np.arange(1, NUM_NEIGHBORS+1))\n",
    "plt.ylim(0, 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quiescent vs Star-Forming Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logLgal_bin_idx = np.digitize(df['LOG_L_GAL'], BGS_LOGLGAL_BINS)\n",
    "num_bins = len(BGS_LOGLGAL_BINS) - 1\n",
    "# 0 is less than the lowest, len(BGS_LOGLGAL_BINS) is greater than the highest entry in BGS_LOGLGAL_BINS\n",
    "\n",
    "print(np.array(BGS_LOGLGAL_BINS))\n",
    "\n",
    "# Calculate the mean value of LOGLGAL in each bin\n",
    "mean_logLgal_per_bin = []\n",
    "for i in range(1, len(BGS_LOGLGAL_BINS)):\n",
    "    bin_values = df['LOG_L_GAL'][logLgal_bin_idx == i]\n",
    "    mean_logLgal_per_bin.append(np.mean(bin_values))\n",
    "\n",
    "# Print the mean values\n",
    "for i, mean_value in enumerate(mean_logLgal_per_bin, 1):\n",
    "    print(f\"Mean LOGLGAL in bin {i}: {mean_value}\")\n",
    "\n",
    "\n",
    "\n",
    "df['SSFR'] = df['SFR'] / np.power(10, df['LOGMSTAR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old G-R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = app_mag_to_abs_mag(app_mag_g, z_obs)\n",
    "R = app_mag_to_abs_mag(app_mag_r, z_obs)\n",
    "\n",
    "G_R = G - R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It doesn't matter if you use g_r_apparent or G_R as the difference between the is the same!\n",
    "\n",
    "Gk = k_correct_bgs(G, z_obs, g_r_apparent, band='g')\n",
    "Rk = k_correct_bgs(R, z_obs, g_r_apparent, band='r')\n",
    "G_R_k_BGS1 = Gk - Rk\n",
    "\n",
    "Gk_GAMA = k_correct_gama(G, z_obs, g_r_apparent, band='g')\n",
    "Rk_GAMA = k_correct_gama(R, z_obs, g_r_apparent, band='r')\n",
    "G_R_k_GAMA = Gk_GAMA - Rk_GAMA\n",
    "\n",
    "Gk_BGS2 = k_correct_bgs_v2(G, z_obs, g_r_apparent, band='g')\n",
    "Rk_BGS2 = k_correct_bgs_v2(R, z_obs, g_r_apparent, band='r')\n",
    "G_R_k_BGS2 = Gk_BGS2 - Rk_BGS2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison of g-r computed a few ways\n",
    "bins = np.linspace(0, 2.0, 200)\n",
    "\n",
    "plt.figure()\n",
    "#junk=plt.hist(g_r_apparent, bins=bins, label=\"g-r\", histtype='step', density=True)\n",
    "#junk=plt.hist(sdss_g_r, bins=bins, label='From LSS Pipeline (JM?)', histtype='step', density=True)\n",
    "junk=plt.hist(G_R_JM1, bins=bins, label=\"0.1^(G-R) JM\", histtype='step', density=True)\n",
    "#junk=plt.hist(G_R, bins=bins, label=\"G-R\", histtype='step', density=True)\n",
    "junk=plt.hist(G_R_k_BGS1, bins=bins, label=\"0.1^(G-R) BGS poly v1\", histtype='step', density=True)\n",
    "junk=plt.hist(G_R_k_GAMA, bins=bins, label=\"0.1^(G-R) GAMA poly\", histtype='step', density=True)\n",
    "junk=plt.hist(G_R_k_BGS2, bins=bins, label=\"0.1^(G-R) BGS poly v2\", histtype='step', density=True)\n",
    "plt.xlabel(\"g-r\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend()\n",
    "plt.xlim(0.2, 1.3)\n",
    "plt.title(\"Comparison of g-r computed a few ways\")\n",
    "plt.tight_layout()\n",
    "plt.ylim(0,3.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can see global GLOBAL_RED_COLOR_CUT=0.76 here\n",
    "junk=plt.hist(G_R_k_GAMA, bins=300, alpha=0.5, label=\"0.1^(G-R) GAMA-style\")\n",
    "junk=plt.hist(G_R_JM1, bins=300, alpha=0.5, label=\"0.1^(G-R) JM\")\n",
    "plt.legend()\n",
    "plt.xlim(0.5, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(is_quiescent_lost_gal_guess(g_r_apparent).sum() / len(g_r_apparent))\n",
    "assert len(G_R_k_GAMA) == len(g_r_apparent)\n",
    "print(is_quiescent_BGS_gmr(None, G_R_k_GAMA).sum() / len(G_R_k_GAMA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyutils import *\n",
    "print(BGS_LOGLGAL_BINS)\n",
    "print(BINWISE_RED_COLOR_CUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_quiescent_BGS_gmr(np.array([5.8, 9.0, 14.5]), np.array([0.5, 0.9, 0.9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get logLgal bins\n",
    "log_L_gal = abs_mag_r_to_log_solar_L(Rk) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define 8 bins that have equal number of galaxies\n",
    "log_L_gal_bins = np.percentile(log_L_gal, np.linspace(0, 100, 9))\n",
    "print(log_L_gal_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.min(log_L_gal))\n",
    "print(np.max(log_L_gal))\n",
    "print(np.min(logLgal_bin_idx))\n",
    "print(np.max(logLgal_bin_idx))\n",
    "plt.hist(log_L_gal, bins=BGS_LOGLGAL_BINS, align='mid')\n",
    "#plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a plot of G_R_k in each logLgal bin\n",
    "for i in range(0, len(BGS_LOGLGAL_BINS)+1):\n",
    "    galaxy_idx_for_this_bin = logLgal_bin_idx == i\n",
    "\n",
    "    plt.figure(dpi=80, figsize=(5, 3))\n",
    "    junk=plt.hist(G_R_k_GAMA[galaxy_idx_for_this_bin], bins=np.arange(0,1.3,0.02), label=f\"0.1^(G-R) Bin {i}\", align='mid')\n",
    "    plt.legend()\n",
    "    plt.xlim(0.4, 1.2)\n",
    "    plt.xticks(np.arange(0.4, 1.2, 0.04))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dn4000 Comparison (BGS, SDSS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdss = pd.read_csv(SDSS_v1_DAT_FILE, delimiter=' ', names=('RA', 'DEC', 'Z', 'LOGLGAL', 'VMAX', 'QUIESCENT', 'chi'), index_col=False)\n",
    "sdss_galprops = pd.read_csv(SDSS_v1_1_GALPROPS_FILE, delimiter=' ', names=('MAG_G', 'MAG_R', 'SIGMA_V', 'DN4000', 'CONCENTRATION', 'LOG_M_STAR', 'Z_ASSIGNED_FLAG'))\n",
    "sdss = pd.merge(sdss, sdss_galprops, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(dn4000, bins=np.linspace(-0.5, 5.0, 100), label=\"BGS\", density=True, histtype='step')    \n",
    "plt.hist(sdss.DN4000, bins=np.linspace(-0.5, 5.0, 100), label=\"SDSS\", density=True, histtype='step')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.xlabel('DN4000')\n",
    "plt.xlim(0.9,2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdss_catalog = coord.SkyCoord(ra=sdss.RA.to_numpy()*u.degree, dec=sdss['DEC'].to_numpy()*u.degree, frame='icrs')\n",
    "BGS_catalog = coord.SkyCoord(ra=ra*u.degree, dec=dec*u.degree, frame='icrs')\n",
    "\n",
    "neighbor_indexes, d2d, d3d = coord.match_coordinates_sky(BGS_catalog, sdss_catalog, storekdtree='sdss')\n",
    "ang_distances = d2d.to(u.arcsec).value\n",
    "\n",
    "match_found_filter = np.logical_and(ang_distances < 1.0, ~np.isnan(dn4000))\n",
    "bgs_matches = dn4000[match_found_filter]\n",
    "sdss_indexes = neighbor_indexes[match_found_filter]\n",
    "sdss_matches = sdss.iloc[sdss_indexes].DN4000.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{len(bgs_matches)} matches found (with Dn4000 available)\")\n",
    "print(f\"{np.isclose(bgs_matches, sdss_matches, atol=0.05).sum() / len(bgs_matches)} of the matches are within 0.05 of each other.\")\n",
    "print(f\"{np.isclose(bgs_matches, sdss_matches, atol=0.1).sum() / len(bgs_matches)} of the matches are within 0.1 of each other.\")\n",
    "print(f\"{np.isclose(bgs_matches, sdss_matches, atol=0.2).sum() / len(bgs_matches)} of the matches are within 0.2 of each other.\")\n",
    "print(f\"{np.isclose(bgs_matches, sdss_matches, atol=0.3).sum() / len(bgs_matches)} of the matches are within 0.3 of each other.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "difference = np.array(bgs_matches) - np.array(sdss_matches)\n",
    "\n",
    "# bin by sdss_matches\n",
    "bins = np.linspace(0.8, 2.5, 30)\n",
    "digitized = np.digitize(sdss_matches, bins)\n",
    "bin_means = [difference[digitized == i].mean() for i in range(1, len(bins))]\n",
    "#bin_stds = np.array([difference[digitized == i].std() for i in range(1, len(bins))])\n",
    "bin_counts = [np.sum(digitized == i) for i in range(1, len(bins))]\n",
    "\n",
    "# Calculate 1 sigma error bars\n",
    "bin_intervals1 = [np.percentile(difference[digitized == i], [16, 84]) for i in range(1, len(bins))]\n",
    "bin_lows1 = np.array([interval[0] for interval in bin_intervals1])\n",
    "bin_highs1 = np.array([interval[1] for interval in bin_intervals1])\n",
    "\n",
    "# Calculate 2 sigma error bars\n",
    "bin_intervals = [np.percentile(difference[digitized == i], [2.5, 97.5]) for i in range(1, len(bins))]\n",
    "bin_lows = np.array([interval[0] for interval in bin_intervals])\n",
    "bin_highs = np.array([interval[1] for interval in bin_intervals])\n",
    "\n",
    "# Convert yerr to a format that plt.errorbar can handle\n",
    "yerr_1sigma = np.array([bin_means - bin_lows1, bin_highs1 - bin_means])\n",
    "yerr_2sigma = np.array([bin_means - bin_lows, bin_highs - bin_means])\n",
    "\n",
    "plt.errorbar(bins[1:], bin_means, yerr=yerr_1sigma, fmt='o', label='Mean Difference (1 sigma)', color='k')\n",
    "plt.errorbar(bins[1:], bin_means, yerr=yerr_2sigma, fmt='o', label='Mean Difference (2 sigma)', alpha=0.5)\n",
    "\n",
    "plt.xlabel('SDSS Dn4000')\n",
    "plt.ylabel('BGS - SDSS Dn4000')\n",
    "plt.legend()\n",
    "plt.title(\"Shared Targets: Dn4000 Comparison\")\n",
    "#draw horizontal line at 0.0\n",
    "plt.axhline(0.0, color='r', linestyle='--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare quescient evaluation to SDSS for matched ones\n",
    "#general_match = ang_distances < 1.0\n",
    "bgs_quiescent_matched = quiescent[match_found_filter]\n",
    "print(len(bgs_quiescent_matched))\n",
    "\n",
    "bgs_matches = dn4000[match_found_filter]\n",
    "sdss_indexes_q = neighbor_indexes[match_found_filter]\n",
    "sdss_quiescent_matched = sdss.iloc[sdss_indexes_q]['QUIESCENT'].to_numpy()\n",
    "\n",
    "# Percent that agree on quiescent classification\n",
    "print(f\"{np.sum(bgs_quiescent_matched == sdss_quiescent_matched) / len(bgs_quiescent_matched):.1%} of the matched galaxies agree on quiescent classification using production methods (what was saved in the merged file).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dn4000 \n",
    "\n",
    "Run Color Analysis and Dn4000 Comparison first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DN4000_MODEL take a look\n",
    "plt.figure()\n",
    "junk=plt.hist(df['DN4000_MODEL'], bins=np.linspace(1, 2.5, 200), histtype='step')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('DN4000_MODEL')\n",
    "plt.ylabel('Count')\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a plot of Dn4000 in each logLgal bin\n",
    "# Show SDSS and my new Dn4000 model quiescent breakpoints\n",
    "fig,axes=plt.subplots(dpi=80, figsize=(18, 8), ncols=4, nrows=2)\n",
    "axes = np.ravel(axes)\n",
    "\n",
    "for i in range(0, len(BGS_LOGLGAL_BINS)-1):\n",
    "    galaxy_idx_for_this_bin = logLgal_bin_idx == i+1\n",
    "\n",
    "    junk=axes[i].hist(dn4000[galaxy_idx_for_this_bin], bins=np.arange(1,2.2,0.02), label=f\"Dn4000 for L Bin {i+1}\", align='mid')\n",
    "    axes[i].legend()\n",
    "    axes[i].set_xlim(1.1, 2.1)\n",
    "    axes[i].set_xticks(np.arange(1.2, 2.1, 0.1))\n",
    "    axes[i].set_xlabel(\"DN4000\")\n",
    "    axes[i].set_ylabel(\"Count\")\n",
    "\n",
    "    # draw a vertical line at get_SDSS_Dcrit(logLgal)\n",
    "    midpoint = (BGS_LOGLGAL_BINS[i] + BGS_LOGLGAL_BINS[i+1]) / 2\n",
    "    axes[i].axvline(x=get_SDSS_Dcrit(midpoint), color='r', linestyle='-', alpha=1.0, linewidth=3)\n",
    "    axes[i].axvline(x=get_Dn4000_crit(midpoint), color='k', linestyle='--', alpha=1.0, linewidth=3)\n",
    "\n",
    "\n",
    "axes = np.reshape(axes, (2, len(BGS_LOGLGAL_BINS)//2))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isnan(dn4000).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the GMM and plot the results\n",
    "good = ~np.isnan(dn4000)\n",
    "intersections = fit_gmm_and_plot(dn4000[good], logLgal_bin_idx[good], num_bins, 1.0, 2.2, 'Dn4000', means_init=np.array([[1.25], [1.9]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(intersections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import erf\n",
    "from scipy.optimize import curve_fit\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#values = [1.41675126, 1.46407341, 1.51939392, 1.58815607, 1.63857298, 1.67328342, 1.70584468, 1.74753757]\n",
    "values = intersections\n",
    "\n",
    "#values = [1.40480885, 1.44768939, 1.4939971, 1.548841358, 1.599508340, 1.626652483, 1.654479619, 1.698544214]\n",
    "\n",
    "# Define the fitting function\n",
    "def fitting_function(logLgal, A, B, C, D):\n",
    "    return A + B*(1 + erf((logLgal - C) / D))\n",
    "\n",
    "priors = [1.42, .175, 9.9, 0.8]\n",
    "\n",
    "# Fit the function to the midpoints\n",
    "popt, pcov = curve_fit(fitting_function, mean_logLgal_per_bin, values, p0=priors, maxfev=10000)\n",
    "\n",
    "print(f\"Fitted parameters: A={popt[0]:.3f}, B={popt[1]:.3f}, C={popt[2]:.3f}, D={popt[3]:.3f}\")\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(dpi=80, figsize=(8, 6))\n",
    "plt.plot(mean_logLgal_per_bin, values, 'o', label='Midpoints')\n",
    "x = np.linspace(8.0, 11.3, 1000)\n",
    "y = fitting_function(x, *popt)\n",
    "plt.plot(x, get_SDSS_Dcrit(x), label='SDSS Dcrit')\n",
    "plt.plot(x, get_Dn4000_crit(x), label='Saved BGS Dcrit')\n",
    "plt.plot(x, y, '--', label='Newly Fitted Function')\n",
    "plt.xlabel('logLgal')\n",
    "plt.ylabel('Dn4000 Midpoint')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HAlpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HALPHA_EW take a look\n",
    "plt.figure()\n",
    "junk=plt.hist(df['HALPHA_EW'], bins=np.logspace(-3, 3.0, 200), histtype='step')\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('HALPHA_EW')\n",
    "plt.ylabel('Count')\n",
    "plt.plot()\n",
    "\n",
    "plt.figure()\n",
    "junk=plt.hist(df['HALPHA_EW'], bins=np.linspace(0, 30, 200), histtype='step')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('HALPHA_EW')\n",
    "plt.ylabel('Count')\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_halpha = np.log10(df['HALPHA_EW'])\n",
    "no_halpha =(c_halpha < -2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the GMM and plot the results\n",
    "c_halpha = np.log10(df['HALPHA_EW']).to_numpy()\n",
    "good = np.logical_and(~no_halpha, ~np.isnan(c_halpha))\n",
    "intersections = fit_gmm_and_plot(c_halpha[good], np.digitize(df['LOG_L_GAL'][good], BGS_LOGLGAL_BINS), num_bins, -1, 2.3, \"Halpha\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import erf\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "values = [0.96861388, 0.88567805, 0.85693346, 0.78651901, 0.74789913, 0.67413471, 0.47214859, 0.33088361]\n",
    "values = intersections\n",
    "\n",
    "# Define the fitting function\n",
    "def fitting_function(logLgal, A, B, C, D):\n",
    "    return A + B*(1 + erf((logLgal - C) / D))\n",
    "\n",
    "priors = [0.9, -0.175, 9.9, 0.8]\n",
    "\n",
    "# Fit the function to the midpoints\n",
    "popt, pcov = curve_fit(fitting_function, mean_logLgal_per_bin, values, p0=priors, maxfev=10000)\n",
    "\n",
    "print(f\"Fitted parameters: A={popt[0]:.3f}, B={popt[1]:.3f}, C={popt[2]:.3f}, D={popt[3]:.3f}\")\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(dpi=80, figsize=(8, 6))\n",
    "plt.plot(mean_logLgal_per_bin, values, 'o', label='Midpoints')\n",
    "x = np.linspace(8.0, 11.3, 1000)\n",
    "y = fitting_function(x, *popt)\n",
    "z = get_halpha_crit(x)\n",
    "plt.plot(x, z, label='Saved Fitted Function')\n",
    "plt.plot(x, y, '--', label='New Fitted Function')\n",
    "plt.xlabel('log(L)')\n",
    "plt.ylabel('Log(Halpha) Threshold')\n",
    "plt.title(\"Halpha Threshold\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SSFR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SSFR take a look\n",
    "plt.figure()\n",
    "junk=plt.hist(df['SSFR'], bins=np.logspace(-14, -8.5, 200), histtype='step')\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('SSFR')\n",
    "plt.ylabel('Count')\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_ssfr = np.log10(df['SSFR']).to_numpy()\n",
    "no_ssfr = c_ssfr <-14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the GMM and plot the results\n",
    "good = np.logical_and(~no_ssfr, ~np.isnan(c_ssfr))\n",
    "intersectinos = fit_gmm_and_plot(c_ssfr[good], np.digitize(df['LOG_L_GAL'][good], BGS_LOGLGAL_BINS), num_bins, -14, -8.5, \"SSFR\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this case the GMM doesn't make as much sense Arjun says. Just use -11 as cut.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## g-r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GMR take a look\n",
    "plt.figure()\n",
    "junk=plt.hist(df['G_R_BEST'], bins=np.linspace(0, 1.4, 200), histtype='step')\n",
    "plt.xlabel('G_R_BEST')\n",
    "plt.ylabel('Count')\n",
    "plt.plot()\n",
    "\n",
    "c_gmr = df['G_R_BEST'].to_numpy()\n",
    "good = ~np.isnan(c_gmr)\n",
    "good = np.logical_and(good, c_gmr < 1.4)\n",
    "# Fit the GMM and plot the results\n",
    "intersections = fit_gmm_and_plot(c_gmr[good], np.digitize(df['LOG_L_GAL'][good], BGS_LOGLGAL_BINS), num_bins, 0, 1.25, \"G-R\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del intersections[1]\n",
    "intersections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import erf\n",
    "from scipy.optimize import curve_fit\n",
    "import numpy as np\n",
    "\n",
    "intersections[0] = 0.76\n",
    "intersections[1] = 0.76\n",
    "\n",
    "# Note: bin 1 was a crappy GMM, intersection was 0.601, I am manually overriding it\n",
    "values = [0.66, 0.69779395, 0.75626146, 0.80508822, 0.82563084, 0.84159505, 0.85176696, 0.85783165]\n",
    "values = [0.73991699, 0.75192245, 0.78478675, 0.83537103, 0.84573671, 0.85990522, 0.86774624, 0.86471373 ]\n",
    "values = intersections\n",
    "#values.insert(0, 0.7) \n",
    "\n",
    "# Define the fitting function\n",
    "def fitting_function(logLgal, A, B, C, D):\n",
    "    return A + B*(1 + erf((logLgal - C) / D))\n",
    "\n",
    "priors = [0.42, .175, 9.9, 0.8]\n",
    "\n",
    "# Fit the function to the midpoints\n",
    "popt, pcov = curve_fit(fitting_function, mean_logLgal_per_bin, values, p0=priors, maxfev=10000)\n",
    "\n",
    "print(f\"Fitted parameters: A={popt[0]:.3f}, B={popt[1]:.3f}, C={popt[2]:.3f}, D={popt[3]:.3f}\")\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(dpi=80, figsize=(8, 6))\n",
    "plt.plot(mean_logLgal_per_bin, values, 'o', label='Midpoints')\n",
    "x = np.linspace(8.0, 11.3, 1000)\n",
    "y = fitting_function(x, *popt)\n",
    "plt.plot(x, y, label='Fitted Function')\n",
    "plt.plot(x, get_gmr_crit(x), label='Saved Fitted Function')\n",
    "plt.xlabel('logLgal')\n",
    "plt.ylabel('Bin Midpoint Value')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KMeans Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification = is_quiescent_BGS_kmeans(df['LOG_L_GAL'].to_numpy(), df['DN4000_MODEL'].to_numpy(), df['HALPHA_EW'].to_numpy(), df['SSFR'].to_numpy(), df['G_R_BEST'].to_numpy())\n",
    "df['KMEANS_LABEL'] = classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# There are 16k  / 117k galaxies with ~0 Halpha EW that need special handling\n",
    "\n",
    "df['KMEANS_LABEL'] = 2 # value for the ones that are not classified because they are so dead (will call them quiescent)\n",
    "no_halpha = df['HALPHA_EW'] < 0.001\n",
    "no_ssfr = np.log10(df['SSFR']) < -14\n",
    "\n",
    "dead = np.logical_or(no_halpha, no_ssfr)\n",
    "#df.loc[dead, 'KMEANS_LABEL'] = 0 # QUIESCENT\n",
    "\n",
    "c_halpha = np.log10(df['HALPHA_EW'][~dead])\n",
    "# Already has a low cut from above\n",
    "c_halpha = np.where(c_halpha > 3, 3, c_halpha)\n",
    "#c_halpha = c_halpha - get_halpha_crit(df['LOG_L_GAL'][~dead])\n",
    "c_halpha = c_halpha - np.log10(2.0) # 2 Angstrom cut\n",
    "\n",
    "c_dn4000 = df['DN4000_MODEL'][~dead]\n",
    "c_dn4000 = np.where(c_dn4000 < 1.0, 1.0, c_dn4000)\n",
    "c_dn4000 = np.where(c_dn4000 > 2.5, 2.5, c_dn4000)\n",
    "c_dn4000 = c_dn4000 - get_Dn4000_crit(df['LOG_L_GAL'][~dead])\n",
    "\n",
    "c_ssfr = np.log10(df['SSFR'][~dead])\n",
    "# Already has a low cut from above\n",
    "c_ssfr = np.where(c_ssfr > -9, -9, c_ssfr)\n",
    "c_ssfr = c_ssfr + 11 # -11 is the cut\n",
    "\n",
    "c_gmr = df['G_R_BEST'][~dead]\n",
    "c_gmr = np.where(c_gmr < 0.0, 0.0, c_gmr)\n",
    "c_gmr = np.where(c_gmr > 1.3, 1.3, c_gmr)\n",
    "c_gmr = c_gmr - get_gmr_crit(df['LOG_L_GAL'][~dead])\n",
    "\n",
    "# TODO normalize them in some way, perhaps multiply by the sqrt(inverse variance) of the measured values\n",
    "# These are ad-hoc choices that put them in the same range\n",
    "x = c_dn4000.to_numpy() * 1.0\n",
    "y = c_halpha * 0.25\n",
    "z = c_ssfr * 0.25\n",
    "zz = c_gmr.to_numpy() * 0.6\n",
    "data = list(zip(x, y, z, zz))\n",
    "\n",
    "# Print off 95% ranges of values, and the difference between the 2.5 and 97.5 percentiles\n",
    "print(f\"DN4000: {np.percentile(x, 2.5):.3f} to {np.percentile(x, 97.5):.3f} ({np.percentile(x, 97.5) - np.percentile(x, 2.5):.3f})\")\n",
    "print(f\"Halpha: {np.percentile(y, 2.5):.3f} to {np.percentile(y, 97.5):.3f} ({np.percentile(y, 97.5) - np.percentile(y, 2.5):.3f})\")\n",
    "print(f\"SSFR: {np.percentile(z, 2.5):.3f} to {np.percentile(z, 97.5):.3f} ({np.percentile(z, 97.5) - np.percentile(z, 2.5):.3f})\")\n",
    "print(f\"G-R: {np.percentile(zz, 2.5):.3f} to {np.percentile(zz, 97.5):.3f} ({np.percentile(zz, 97.5) - np.percentile(zz, 2.5):.3f})\")\n",
    "# We want these to all be similar to weight equally in the classification\n",
    "\n",
    "kmeans = KMeans(n_clusters=2)\n",
    "kmeans.fit(data)\n",
    "\n",
    "classification = kmeans.labels_\n",
    "# Each run will give 0 or 1 a different meaning. Let's standardize it so 1 meads red and 0 is blue\n",
    "if kmeans.cluster_centers_[0][0] > kmeans.cluster_centers_[1][0]:\n",
    "    classification = 1 - classification\n",
    "    \n",
    "df.loc[~dead, 'KMEANS_LABEL'] = classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATE PLOTS OF KMEANS CLASSIFICATION IN THE MODIFIED FITTING SPACE\n",
    "\n",
    "classification = classification.astype(bool)\n",
    "# Turn df['KMEANS_LABEL'] into a color\n",
    "\n",
    "def plot_density(x, y, q, xlabel, ylabel, scatter=False):\n",
    "    \"\"\"\n",
    "    Create a density plot for the given x and y properties, classified by the provided labels.\n",
    "\n",
    "    Parameters:\n",
    "    x (array-like): Data for the x-axis.\n",
    "    y (array-like): Data for the y-axis.\n",
    "    cls (array-like): Boolean array for classification (True/False).\n",
    "    xlabel (str): Label for the x-axis.\n",
    "    ylabel (str): Label for the y-axis.\n",
    "    alpha (float): Transparency level for the points.\n",
    "    \"\"\"\n",
    "    if x is pd.Series:\n",
    "        x = x.to_numpy()\n",
    "    if y is pd.Series:\n",
    "        y = y.to_numpy()\n",
    "    if q is pd.Series:\n",
    "        q = q.to_numpy()\n",
    "\n",
    "    # First randomy downsample to 1500 points of each class\n",
    "    if len(x) > 3000:\n",
    "        x_q = x[q]\n",
    "        y_q = y[q]\n",
    "        x_sf = x[~q]\n",
    "        y_sf = y[~q]\n",
    "\n",
    "        if len(x_q) > 1500:\n",
    "            idx = np.random.choice(len(x_q), 1500, replace=False)\n",
    "            x_q = x_q[idx]\n",
    "            y_q = y_q[idx]\n",
    "\n",
    "        if len(x_sf) > 1500:\n",
    "            idx = np.random.choice(len(x_sf), 1500, replace=False)\n",
    "            x_sf = x_sf[idx]\n",
    "            y_sf = y_sf[idx]\n",
    "\n",
    "        x = np.concatenate((x_q, x_sf))\n",
    "        y = np.concatenate((y_q, y_sf))\n",
    "        q = np.concatenate((np.repeat(True, len(x_q)), np.repeat(False, len(x_sf))))\n",
    "\n",
    "    plt.figure()\n",
    "    sns.kdeplot(\n",
    "        x=x, y=y, hue=q, fill=True, levels=10, palette=[\"blue\", \"red\"], alpha=0.7, bw_adjust=1.0,\n",
    "    )\n",
    "    plt.axhline(0.0, color='k', linestyle='--')\n",
    "    plt.axvline(0.0, color='k', linestyle='--')\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.show()\n",
    "\n",
    "    if scatter:\n",
    "        plt.figure()\n",
    "        plt.scatter(x_q, y_q, c='red', alpha=0.15, label='Q')\n",
    "        plt.scatter(x_sf, y_sf, c='blue', alpha=0.15, label='SF')\n",
    "        plt.xlabel(xlabel)\n",
    "        plt.ylabel(ylabel)\n",
    "        plt.axhline(0.0, color='k', linestyle='--')\n",
    "        plt.axvline(0.0, color='k', linestyle='--')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "# Plot each combination of x, y, z, zz\n",
    "dim = df['LOG_L_GAL'][~dead] < 9.0\n",
    "#arrays = [x[dim], y[dim], z[dim], zz[dim]]\n",
    "arrays = [x, y, z, zz]\n",
    "names = ['~Dn4000', '~log(Halpha)', '~log(SSFR)', '~G-R']\n",
    "for i in range(len(arrays)):\n",
    "    for j in range(i+1, len(arrays)):\n",
    "        plot_density(arrays[i], arrays[j], classification, names[i], names[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHOW RESULTS PLOTS OF KMEANS CLASSIFICATION IN THE SPACE OF THE ORIGINAL VARIABLES\n",
    "\n",
    "def quiescent_classification_plots(df):\n",
    "    alpha = 0.7\n",
    "\n",
    "    # Downsample the data to make it easier to plot\n",
    "    df_q = df.loc[df['KMEANS_LABEL'] != 0]\n",
    "    df_sf = df.loc[df['KMEANS_LABEL'] == 0]\n",
    "    idx_q = np.random.choice(len(df_q), np.min([1500, len(df_q)]), replace=False)\n",
    "    idx_sf = np.random.choice(len(df_sf), np.min([1500, len(df_sf)]), replace=False)\n",
    "    idx = np.concatenate((idx_q, idx_sf))\n",
    "    sample = df.iloc[idx]\n",
    "    colors = np.where(sample['KMEANS_LABEL'] == 0, 0, 1) # darkred => red\n",
    "    #colors = np.where(sample['KMEANS_LABEL'] == 2, 'darkred', colors)\n",
    "\n",
    "    names = ['DN4000', 'HALPHA_EW', 'SSFR', 'G_R_BEST']\n",
    "    for i in range(len(names)):\n",
    "        for j in range(i+1, len(names)):\n",
    "            plt.figure()\n",
    "            sns.kdeplot(\n",
    "                x=sample[names[i]], \n",
    "                y=sample[names[j]], \n",
    "                hue=colors, \n",
    "                fill=True, \n",
    "                levels=10, \n",
    "                palette=[\"blue\", \"red\"], \n",
    "                alpha=alpha,\n",
    "            )\n",
    "            if names[i] == 'HALPHA_EW':\n",
    "                #plt.xscale('log')\n",
    "                #plt.xlim(0.001, 100)\n",
    "                plt.xlim(0, 60)\n",
    "            if names[j] == 'HALPHA_EW':\n",
    "                #plt.yscale('log')\n",
    "                #plt.ylim(0.001, 100)\n",
    "                plt.ylim(0, 60)\n",
    "            if names[i] == 'SSFR':\n",
    "                plt.xscale('log')\n",
    "                plt.xlim(1e-14, 1e-9)\n",
    "            if names[j] == 'SSFR':\n",
    "                plt.yscale('log')\n",
    "                plt.ylim(1e-14, 1e-9)\n",
    "            if names[i] == 'G_R_BEST':\n",
    "                plt.xlim(0, 1.5)\n",
    "            if names[j] == 'G_R_BEST':\n",
    "                plt.ylim(0, 1.5)\n",
    "            if names[i] == 'DN4000':\n",
    "                plt.xlim(1.0, 2.5)\n",
    "            if names[j] == 'DN4000':\n",
    "                plt.ylim(1.0, 2.5)\n",
    "            plt.plot()\n",
    "\n",
    "quiescent_classification_plots(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at just low L galaxies now\n",
    "dim = df.loc[df['LOG_L_GAL'] < 9.0]\n",
    "\n",
    "quiescent_classification_plots(dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare to SDSS\n",
    "sdss = pd.read_csv(SDSS_v1_DAT_FILE, delimiter=' ', names=('RA', 'DEC', 'Z', 'LOGLGAL', 'VMAX', 'QUIESCENT', 'chi'), index_col=False)\n",
    "\n",
    "# TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Randoms Analysis for Footprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_sv3_clustering_randoms_files()\n",
    "build_sv3_full_randoms_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_y3_likesv3_clustering_randoms_files()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_y1_randoms_files()\n",
    "build_y3_randoms_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv3_randoms = pickle.load(open(MY_RANDOMS_SV3_MINI, 'rb'))\n",
    "tiles_BGS = read_tiles_Y3_sv3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the randoms positions\n",
    "fig=make_map(sv3_randoms['RA'].to_numpy(), sv3_randoms['DEC'].to_numpy())\n",
    "#make_map(ra, dec, fig=fig, alpha=0.05)\n",
    "\n",
    "\n",
    "# Some zoom-ins\n",
    "\n",
    "# These are the two regions we cut due to poor Y3 overlap\n",
    "#plot_positions(randoms_df0, randoms_df0[randoms_df0.NTILE_MINE >= 10], tiles_df=tiles_BGS, DEG_LONG=7, ra_min=192, dec_min=23, split=False)\n",
    "\n",
    "plot_positions(sv3_randoms, sv3_randoms[sv3_randoms.NTILE_MINE >= 10], DEG_LONG=5, ra_min=213, dec_min=50, split=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CORRECT FOOTPRINTS\n",
    "# These results copied into the groupcatalog.py file foorprints\n",
    "\n",
    "def frac_sky_from_randoms(randoms: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Provide a single randoms file at 2500 targets / square degrees for this, not a larger combined file.\n",
    "    \"\"\"\n",
    "    RANDOMS_DENSITY = 2500 # per square degree, Ashley Ross paper on LSS pipeline or elsewhere in docs\n",
    "\n",
    "    for i in range(1, 11):\n",
    "        n_pass_filter = randoms.NTILE_MINE >= i\n",
    "        n_pass_footprint = len(randoms[n_pass_filter].RA) / RANDOMS_DENSITY # in degrees squared\n",
    "        n_pass_frac_area = n_pass_footprint / DEGREES_ON_SPHERE\n",
    "        print(f\"BGS {i}pass Footprint calculated from randoms is {n_pass_footprint} square degrees or frac_area={n_pass_frac_area}\")\n",
    "\n",
    "print(\"-=- SV3 18 regions -=-\")\n",
    "frac_sky_from_randoms(pickle.load(open(MY_RANDOMS_SV3_MINI, 'rb')))\n",
    "\n",
    "print(\"-=- SV3 20 regions -=-\")\n",
    "frac_sky_from_randoms(pickle.load(open(MY_RANDOMS_SV3_MINI_20, 'rb')))\n",
    "\n",
    "print(\"-=- Y1 -=-\")\n",
    "frac_sky_from_randoms(pickle.load(open(MY_RANDOMS_Y1_MINI, 'rb')))\n",
    "\n",
    "print(\"-=- Y3 -=-\")\n",
    "frac_sky_from_randoms(pickle.load(open(MY_RANDOMS_Y3_MINI, 'rb')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
