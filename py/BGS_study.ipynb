{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import astropy.coordinates as coord\n",
    "import astropy.units as u\n",
    "import astropy.io.fits as fits\n",
    "from astropy.table import Table,join,vstack,unique,QTable\n",
    "from astropy.table import Table,join,vstack,unique,QTable\n",
    "import sys\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "if './SelfCalGroupFinder/py/' not in sys.path:\n",
    "    sys.path.append('./SelfCalGroupFinder/py/')\n",
    "from pyutils import *\n",
    "from dataloc import *\n",
    "from photoz import *\n",
    "import groupcatalog as gc\n",
    "from nnanalysis import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEEP_PASSES = 1\n",
    "#APP_MAG_CUT = 19.5 # BGS BRIGHT, though 19.54 for some cameras. I don't know if FLUX_R has been corrected for this.\n",
    "APP_MAG_CUT = 20.175 # BGS FAINT, and in SV3 it is 20.3 instead\n",
    "Z_MIN = 0.001\n",
    "Z_MAX = 0.8\n",
    "\n",
    "# This corresponds to 8.33 square degrees and empircally makes sense by looking at the randoms\n",
    "TILE_RADIUS = 5862.0 * u.arcsec # arcsec\n",
    "\n",
    "def find_tiles_for_galaxies(tiles_df, gals_df, num_tiles_to_find):\n",
    "    num_galaxies = len(gals_df.RA)\n",
    "    num_tiles = len(tiles_df.RA)\n",
    "\n",
    "    tiles_coord = coord.SkyCoord(ra=tiles_df.RA.to_numpy()*u.degree, dec=tiles_df.Dec.to_numpy()*u.degree, frame='icrs')\n",
    "    gals_coord = coord.SkyCoord(ra=gals_df.RA.to_numpy()*u.degree, dec=gals_df.Dec.to_numpy()*u.degree, frame='icrs')\n",
    "\n",
    "    # Structure for resultant data\n",
    "    nearest_tile_ids = np.zeros((num_galaxies, num_tiles_to_find), dtype=int)\n",
    "    ntiles_inside = np.zeros((num_galaxies), dtype=int)\n",
    "\n",
    "    for n in range(num_tiles_to_find):\n",
    "        idx, d2d, d3d = coord.match_coordinates_sky(gals_coord, tiles_coord, nthneighbor=n+1, storekdtree='kdtree_tiles')\n",
    "        nearest_tile_ids[:,n] = tiles_df.iloc[idx].TILEID\n",
    "        ntiles_inside += (d2d < TILE_RADIUS).astype(int)\n",
    "\n",
    "    \n",
    "    return ntiles_inside, nearest_tile_ids\n",
    "\n",
    "def add_mag_columns(table):\n",
    "    app_mag_r = get_app_mag(table['FLUX_R'])\n",
    "    app_mag_g = get_app_mag(table['FLUX_G'])\n",
    "    g_r = app_mag_g - app_mag_r\n",
    "\n",
    "    if np.ma.is_masked(table['Z']):\n",
    "        z_obs = table['Z'].data.data\n",
    "    else:\n",
    "        z_obs = table['Z']\n",
    "    \n",
    "    abs_mag_R = app_mag_to_abs_mag(app_mag_r, z_obs)\n",
    "    abs_mag_R_k = k_correct(abs_mag_R, z_obs, g_r, band='r')\n",
    "    abs_mag_G = app_mag_to_abs_mag(app_mag_g, z_obs)\n",
    "    abs_mag_G_k = k_correct(abs_mag_G, z_obs, g_r, band='g')\n",
    "    log_L_gal = abs_mag_r_to_log_solar_L(abs_mag_R_k) \n",
    "    G_R_k = abs_mag_G_k - abs_mag_R_k\n",
    "    quiescent = is_quiescent_BGS_gmr(log_L_gal, G_R_k)\n",
    "\n",
    "    table.add_column(app_mag_r, name='APP_MAG_R')\n",
    "    table.add_column(app_mag_g, name='APP_MAG_G')\n",
    "    table.add_column(abs_mag_R, name='ABS_MAG_R')\n",
    "    table.add_column(abs_mag_R_k, name='ABS_MAG_R_K')\n",
    "    table.add_column(abs_mag_G, name='ABS_MAG_G')\n",
    "    table.add_column(abs_mag_G_k, name='ABS_MAG_G_K')\n",
    "    table.add_column(log_L_gal, name='LOG_L_GAL')\n",
    "    table.add_column(quiescent, name='QUIESCENT')\n",
    "\n",
    "\n",
    "def add_photz_columns(table_file, phot_z_file):\n",
    "    \"\"\"\n",
    "    Reads an astropy table and adds columns from the legacy survey file we built (photo-z, etc.).\n",
    "    \"\"\"\n",
    "\n",
    "    table = Table.read(table_file, format='fits') # astropy\n",
    "    if 'Z_PHOT' in table.columns:\n",
    "        print(\"Z_PHOT already in table, replacing it.\")\n",
    "        table.remove_columns(['Z_PHOT', 'RELEASE', 'BRICKID', 'OBJID', 'REF_CAT'])\n",
    "\n",
    "    phot_z_table = pickle.load(open(phot_z_file, 'rb'))\n",
    "    phot_z_table['TARGETID'] = phot_z_table.index # in the DataFrame TARGETID is the index, not a column, so copy it over so the conversion keeps it\n",
    "    # Merge in the photo-z and whatever else info we took from Legacy Surveys sweeps\n",
    "\n",
    "    percent_complete = (phot_z_table['Z_LEGACY_BEST'] != -99.0).sum() / len(phot_z_table)\n",
    "    print(f\"Phot-z file has phot-z for {percent_complete:.2%} of targets.\")\n",
    "\n",
    "    final_table = join(table, QTable.from_pandas(phot_z_table), join_type='left', keys=\"TARGETID\")\n",
    "\n",
    "    print(len(table))\n",
    "    print(len(phot_z_table))\n",
    "    print(len(final_table))\n",
    "\n",
    "    final_table.rename_column('Z_LEGACY_BEST', 'Z_PHOT')\n",
    "    final_table.rename_column('RA_1', 'RA')\n",
    "    final_table.rename_column('DEC_1', 'DEC')\n",
    "    final_table.remove_columns(['RA_2', 'DEC_2'])\n",
    "    print(final_table.columns)\n",
    "\n",
    "    # TODO I should switch to having the merged file be pickle.dump of a DataFrame. \n",
    "    # Only thing is NEAREST_TILEIDS cannot be a lit\n",
    "    final_table.write(table_file, format='fits', overwrite=True)\n",
    "\n",
    "\n",
    "\n",
    "def table_to_df(table: Table):\n",
    "    \"\"\"\n",
    "    This does not work for all purposes yet.\n",
    "    \"\"\"\n",
    "    # TODO why not use to_pandas()?\n",
    "    #df = table.to_pandas()\n",
    "    \n",
    "    obj_type = table['SPECTYPE'].data.data\n",
    "    dec = table['DEC'].astype(\"<f8\") # Big endian vs little endian regression in pandas. Convert more of these fields like this\n",
    "    ra = table['RA'].astype(\"<f8\") # as needed if using pandas with this data\n",
    "    z_obs = table['Z'].data.data\n",
    "    target_id = table['TARGETID']\n",
    "    #flux_r = table['FLUX_R']\n",
    "    #flux_g = table['FLUX_G']\n",
    "    app_mag_r = get_app_mag(table['FLUX_R'])\n",
    "    app_mag_g = get_app_mag(table['FLUX_G'])\n",
    "    g_r_apparent = app_mag_g - app_mag_r\n",
    "    #sdss_g_r = table['ABSMAG_SDSS_G'] - table['ABSMAG_SDSS_R'] \n",
    "    #G_R_JM1 = table['ABSMAG01_SDSS_G'] - table['ABSMAG01_SDSS_R']\n",
    "    p_obs = table['PROB_OBS'] \n",
    "    unobserved = table['Z'].mask\n",
    "    deltachi2 = table['DELTACHI2'].data.data\n",
    "    ntiles = table['NTILE']\n",
    "    #abs_mag_sdss = table['ABSMAG_SDSS_R']\n",
    "    dn4000 = table['DN4000'].data.data\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'SPECTYPE': obj_type,\n",
    "        'Dec': dec,\n",
    "        'RA': ra,\n",
    "        'z': z_obs,\n",
    "        'TARGETID': target_id,\n",
    "        #'FLUX_R': flux_r,\n",
    "        #'FLUX_G': flux_g,\n",
    "        'APP_MAG_R': app_mag_r,\n",
    "        'APP_MAG_G': app_mag_g,\n",
    "        'G_R_APPARENT': g_r_apparent,\n",
    "        #'SDSS_G_R': sdss_g_r,\n",
    "        #'G_R_JM1': G_R_JM1,\n",
    "        'PROB_OBS': p_obs,\n",
    "        'UNOBSERVED': unobserved,\n",
    "        'DELTACHI2': deltachi2,\n",
    "        'NTILE': ntiles,\n",
    "        #'ABS_MAG_SDSS': abs_mag_sdss,\n",
    "        'DN4000': dn4000\n",
    "        })\n",
    "\n",
    "    return df\n",
    "\n",
    "def read_tiles_Y1_main():\n",
    "    tiles_table = Table.read(BGS_TILES_FILE, format='csv')\n",
    "    tiles_table.keep_columns(['TILEID', 'FAFLAVOR', 'TILERA', 'TILEDEC'])\n",
    "    tiles_df = pd.DataFrame({'RA': tiles_table['TILERA'].astype(\"<f8\"), 'Dec': tiles_table['TILEDEC'].astype(\"<f8\"), 'FAFLAVOR': tiles_table['FAFLAVOR'], 'TILEID': tiles_table['TILEID']})\n",
    "    tiles_df = tiles_df[tiles_df.FAFLAVOR == 'mainbright']\n",
    "    tiles_df.reset_index(drop=True, inplace=True)\n",
    "    return tiles_df\n",
    "\n",
    "def read_tiles_Y3_file():\n",
    "    tiles_table = Table.read(BGS_Y3_TILES_FILE, format='csv')\n",
    "    tiles_table.keep_columns(['TILEID', 'FAFLAVOR', 'TILERA', 'TILEDEC'])\n",
    "    tiles_df = pd.DataFrame({'RA': tiles_table['TILERA'].astype(\"<f8\"), 'Dec': tiles_table['TILEDEC'].astype(\"<f8\"), 'FAFLAVOR': tiles_table['FAFLAVOR'], 'TILEID': tiles_table['TILEID']})\n",
    "    return tiles_df\n",
    "\n",
    "def read_tiles_Y3_main():\n",
    "    tiles_table = Table.read(BGS_Y3_TILES_FILE, format='csv')\n",
    "    tiles_table.keep_columns(['TILEID', 'FAFLAVOR', 'TILERA', 'TILEDEC'])\n",
    "    tiles_df = pd.DataFrame({'RA': tiles_table['TILERA'].astype(\"<f8\"), 'Dec': tiles_table['TILEDEC'].astype(\"<f8\"), 'FAFLAVOR': tiles_table['FAFLAVOR'], 'TILEID': tiles_table['TILEID']})\n",
    "    tiles_df = tiles_df[tiles_df.FAFLAVOR == 'mainbright']\n",
    "    tiles_df.reset_index(drop=True, inplace=True)\n",
    "    return tiles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_columns_in_phot_z_file(f):\n",
    "    phot_z_table = pickle.load(open(f, 'rb'))\n",
    "    phot_z_table['REF_CAT_NEW'] = phot_z_table['REF_CAT'].astype('S2')\n",
    "    phot_z_table.loc[(phot_z_table.REF_CAT_NEW == b'  '), 'REF_CAT_NEW'] = b''\n",
    "    phot_z_table.loc[(phot_z_table.REF_CAT_NEW == b'na'), 'REF_CAT_NEW'] = b''\n",
    "    phot_z_table.drop('REF_CAT', inplace=True, axis=1)\n",
    "    phot_z_table.rename(columns={'REF_CAT_NEW':'REF_CAT'}, inplace=True)\n",
    "    pickle.dump(phot_z_table, open(f, 'wb'))\n",
    "\n",
    "# Run this after building photo-z file\n",
    "#fix_columns_in_phot_z_file(IAN_PHOT_Z_FILE_NOSPEC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Merged Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build SV3 Merged File\n",
    "This requires Y3 Merged file to be built first, because we add in Y3 galaxies to supplement the NN catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv3_table = Table.read(BGS_SV3_ANY_FULL_FILE, format='fits')\n",
    "tiles = read_tiles_Y3_file()\n",
    "SV3_tiles = tiles.loc[tiles.FAFLAVOR == 'sv3bright']\n",
    "print(sv3_table.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to needed columns only and save\n",
    "# TILELOCID\n",
    "# TODO Investigate ZTILEID and NUMOBS\n",
    "sv3_table.keep_columns(['TARGETID', 'SPECTYPE', 'DEC', 'RA', 'Z_not4clus', 'ZTILEID', 'NUMOBS', 'FLUX_R', 'FLUX_G', 'PROB_OBS', 'ZWARN', 'DELTACHI2', 'NTILE', 'TILES', 'TILEID', 'MASKBITS'])\n",
    "sv3_table.rename_column('Z_not4clus', 'Z')\n",
    "\n",
    "sv3_df = sv3_table.to_pandas()\n",
    "sv3_df.rename(columns={'DEC': 'Dec'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntiles_inside, nearest_tile_ids = find_tiles_for_galaxies(SV3_tiles, sv3_df, 10) # \n",
    "sv3_table.add_column(ntiles_inside, name=\"NTILE_MINE\")\n",
    "sv3_table.add_column(nearest_tile_ids, name=\"NEAREST_TILEIDS\")\n",
    "\n",
    "add_mag_columns(sv3_table)\n",
    "\n",
    "sv3_table.write(IAN_BGS_SV3_MERGED_NOY3_FILE, format='fits', overwrite='True')\n",
    "del(sv3_table)\n",
    "del(sv3_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_photz_columns(IAN_BGS_SV3_MERGED_NOY3_FILE, IAN_PHOT_Z_FILE_NOSPEC) # For SV3 Analysis we want to analyze pure photo-z's, so use the version without external spec-z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now add Y3 galaxies to supplement the NN catalog, especialy valuable at the edges of the regions\n",
    "# They won't go into the main catalog because their NTILE_MINE is < 10\n",
    "sv3_table: Table = Table.read(IAN_BGS_SV3_MERGED_NOY3_FILE, format='fits')\n",
    "y3_table: Table = Table.read(IAN_BGS_Y3_MERGED_FILE, format='fits')\n",
    "\n",
    "# Let's cut Y3 data down to targets somewhat close to SV3 regions\n",
    "# No point in keeping rest and slowing down code\n",
    "gals_coord = coord.SkyCoord(ra=y3_table['RA']*u.degree, dec=y3_table['DEC']*u.degree, frame='icrs')\n",
    "close_array = gc.get_objects_near_sv3_regions(gals_coord, 2.5) # 2.5 deg radius is generously around the center of each SV3 region\n",
    "\n",
    "y3_table = y3_table[close_array]\n",
    "\n",
    "print(f\"{len(y3_table)} galaxies will be added for SV3 NN catalog\")\n",
    "assert (y3_table['NTILE_MINE'] > 9).sum() == 0, \"Y3 shouldn't add any galaxies that will go into the catalog\"\n",
    "\n",
    "# if the y3 table doesn't have prob_obs as is the case in JURA, add a prob_obs with 0.5 for everything\n",
    "if 'PROB_OBS' not in y3_table.columns:\n",
    "    print(\"Adding PROB_OBS column with 0.5 for all Y3 galaxies\")\n",
    "    y3_table.add_column(np.full(len(y3_table), 0.5), name=\"PROB_OBS\")\n",
    "\n",
    "# Remove rows from y3_table that have TARGETID already in sv3_table\n",
    "y3_in_sv3 = np.isin(y3_table['TARGETID'], sv3_table['TARGETID'])\n",
    "print(f\"Removing {y3_in_sv3.sum()} (of {len(y3_table)}) Y3 galaxies that are already in SV3 catalog\")\n",
    "y3_table.remove_rows(y3_in_sv3)\n",
    "\n",
    "#colname = 'NEAREST_TILEIDS'\n",
    "#print(sv3_table[colname].shape)\n",
    "#print(y3_table[colname].shape)\n",
    "\n",
    "sv3_table.remove_column('TILES')\n",
    "y3_table.remove_column('TILES')\n",
    "\n",
    "combined = vstack([sv3_table, y3_table], join_type='outer')\n",
    "\n",
    "# Ensure resultant data is what we want\n",
    "print(sv3_table.columns)\n",
    "print(y3_table.columns)\n",
    "print(combined.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.write(IAN_BGS_SV3_MERGED_FILE, format='fits', overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Empty Photo-z Ledger file\n",
    " \n",
    "See photoz.py for the code that populates this file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INITIALIZE LIGHTWEIGHT DESI BGS PHOTO-Z TABLE\n",
    "# Don't re-run! Will overwrite the file.\n",
    "\"\"\"\n",
    "desi_table = Table.read(IAN_BGS_Y3_MERGED_FILE, format='fits')\n",
    "desi_table2 = Table.read(IAN_BGS_SV3_MERGED_NOY3_FILE, format='fits')\n",
    "\n",
    "assert len(np.unique(desi_table['TARGETID'])) == len(desi_table), \"There are duplicate TARGETIDs in the Y3 file\"\n",
    "assert len(np.unique(desi_table2['TARGETID'])) == len(desi_table2), \"There are duplicate TARGETIDs in the SV3 file\"\n",
    "\n",
    "desi_table.keep_columns(['TARGETID', 'RA', 'DEC'])\n",
    "desi_table2.keep_columns(['TARGETID', 'RA', 'DEC'])\n",
    "\n",
    "desi_targets_table = vstack([desi_table, desi_table2], join_type='inner')\n",
    "desi_targets_table = unique(desi_targets_table, 'TARGETID')\n",
    "desi_targets_table['Z_LEGACY_BEST'] = -99.0\n",
    "\n",
    "# add columns for 'RELEASE', 'BRICKID', 'OBJID', 'REF_CAT', 'MATCH_DIST' with no values\n",
    "desi_targets_table.add_column(np.zeros(len(desi_targets_table), dtype=int), name='RELEASE')\n",
    "desi_targets_table.add_column(np.zeros(len(desi_targets_table), dtype=int), name='BRICKID')\n",
    "desi_targets_table.add_column(np.zeros(len(desi_targets_table), dtype=int), name='OBJID')\n",
    "desi_targets_table.add_column(np.zeros(len(desi_targets_table), dtype='S2'), name='REF_CAT')\n",
    "desi_targets_table.add_column(np.full(len(desi_targets_table), 999999, dtype=float), name='MATCH_DIST')\n",
    "\n",
    "\n",
    "desi_targets_table = desi_targets_table.to_pandas()\n",
    "desi_targets_table.set_index('TARGETID', inplace=True)\n",
    "pickle.dump(desi_targets_table, open(IAN_PHOT_Z_FILE, 'wb'))\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read BGS_IMAGES_FOLDER + \"terminal.txt\" into an array of strings, 1 per line\n",
    "f = open(BGS_IMAGES_FOLDER + \"terminal.txt\", \"r\")\n",
    "lines = f.readlines()\n",
    "f.close()\n",
    "\n",
    "# Now filter it down to ones like this:\n",
    "#Start processing brick #X\n",
    "#Matched 0 out of Z\n",
    "#start_lines = [line for line in lines if \"Start processing brick\" in line]\n",
    "matched_lines = [line for line in lines if \"Matched\" in line]\n",
    "\n",
    "#print(len(start_lines))\n",
    "print(len(matched_lines))\n",
    "\n",
    "# Extract X, Y, Z from each line \n",
    "#start_lines = [line.split()[3] for line in start_lines]    \n",
    "matched_lines = [int(line.split()[1]) for line in matched_lines]\n",
    "\n",
    "# Strip away # and convert to int\n",
    "#start_lines = [int(line[1:]) for line in start_lines]\n",
    "\n",
    "# The index is the brick number for matched_lines\n",
    "# Find the brick numbers where the value is 0 and remember those indexes\n",
    "zero_indexes = [i for i in range(len(matched_lines)) if matched_lines[i] == 0]\n",
    "\n",
    "#pickle.dump(zero_indexes, open(BRICKS_TO_SKIP_S_FILE, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle.dump([], open(BRICKS_TO_SKIP_N_FILE, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Y1 Merged File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdul = fits.open(BGS_FASTSPEC_FILE, memmap=True)\n",
    "#print(hdul[1].columns)\n",
    "data = hdul[1].data\n",
    "fastspecfit_id = data['TARGETID']\n",
    "DN4000 = data['DN4000'] # TODO there is also DN4000_OBS and DN4000_MODEL (and inverse variance)\n",
    "FSF_G = data['ABSMAG01_SDSS_G']\n",
    "FSF_R = data['ABSMAG01_SDSS_R']\n",
    "hdul.close()\n",
    "\n",
    "print(len(fastspecfit_id))\n",
    "print(len(DN4000))\n",
    "\n",
    "fastspecfit_table = Table([fastspecfit_id, DN4000, FSF_G, FSF_R], names=('TARGETID', 'DN4000', 'ABSMAG01_SDSS_G', 'ABSMAG01_SDSS_R'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_table = Table.read(BGS_ANY_FULL_FILE, format='fits')\n",
    "print(main_table.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALREADY DONE FOR US; only needed to do this in Iron v1.2 due to a bug.\n",
    "# Prob obs file\n",
    "#p_table = Table.read(BGS_PROB_OBS_FILE, format='fits')\n",
    "#print(len(p_table))\n",
    "\n",
    "# Join them all on TARGETID\n",
    "#joined_table = join(main_table, p_table, keys=\"TARGETID\")\n",
    "#print(len(joined_table))\n",
    "\n",
    "to_join = main_table\n",
    "#to_join = p_table\n",
    "\n",
    "# The lost galaxies will not have fastspecfit rows I think\n",
    "final_table = join(to_join, fastspecfit_table, join_type='left', keys=\"TARGETID\")\n",
    "print(len(final_table))\n",
    "\n",
    "# Sanity check that everything went as intended\n",
    "assert len(final_table) == len(main_table)\n",
    "\n",
    "# Filter to needed columns only and save\n",
    "final_table.keep_columns(['TARGETID', 'SPECTYPE', 'DEC', 'RA', 'Z_not4clus', 'FLUX_R', 'FLUX_G', 'BITWEIGHTS', 'PROB_OBS', 'ZWARN', 'DELTACHI2', 'NTILE', 'TILES', 'DN4000', 'ABSMAG01_SDSS_G', 'ABSMAG01_SDSS_R', 'MASKBITS'])\n",
    "final_table.rename_column('Z_not4clus', 'Z')\n",
    "\n",
    "add_mag_columns(final_table)\n",
    "\n",
    "\n",
    "final_table.write(IAN_BGS_MERGED_FILE, format='fits', overwrite='True')\n",
    "\n",
    "del(main_table)\n",
    "#del(p_table)\n",
    "del(fastspecfit_table)\n",
    "del(final_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_NTILE_MINE_to_table(table_file):\n",
    "    tiles_df = read_tiles_Y1_main()\n",
    "    table = Table.read(table_file, format='fits')\n",
    "    galaxies_df = table_to_df(table)\n",
    "    \n",
    "    ntiles_inside, nearest_tile_ids = find_tiles_for_galaxies(tiles_df, galaxies_df, 15)\n",
    "    if 'NTILE_MINE' in table.columns:\n",
    "        table.remove_columns(['NTILE_MINE', 'NEAREST_TILEIDS'])\n",
    "    table.add_column(ntiles_inside, name=\"NTILE_MINE\")\n",
    "    table.add_column(nearest_tile_ids, name=\"NEAREST_TILEIDS\")\n",
    "\n",
    "    table.write(table_file, format='fits', overwrite='True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_NTILE_MINE_to_table(IAN_BGS_MERGED_FILE)\n",
    "#add_NTILE_MINE_to_table(IAN_BGS_MERGED_FILE_OLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_photz_columns(IAN_BGS_MERGED_FILE, IAN_PHOT_Z_FILE_WSPEC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Y3 Merged File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't have all files needed to use the above pipline yet, so just working with what we have\n",
    "\n",
    "# Main file\n",
    "main_table = Table.read(BGS_Y3_ANY_FULL_FILE, format='fits')\n",
    "print(len(main_table))\n",
    "\n",
    "# Filter to needed columns only and save\n",
    "# TODO need PROB_OBS\n",
    "main_table.keep_columns(['TARGETID', 'SPECTYPE', 'DEC', 'RA', 'Z_not4clus', 'FLUX_R', 'FLUX_G', 'ZWARN', 'DELTACHI2', 'NTILE', 'TILES', 'MASKBITS'])\n",
    "main_table.rename_column('Z_not4clus', 'Z')\n",
    "\n",
    "galaxies_df = pd.DataFrame({\n",
    "    'Dec': main_table['DEC'],\n",
    "    'RA': main_table['RA'],\n",
    "    })\n",
    "\n",
    "tiles_BGS = read_tiles_Y3_main()\n",
    "\n",
    "ntiles_inside, nearest_tile_ids = find_tiles_for_galaxies(tiles_BGS, galaxies_df, 10)\n",
    "\n",
    "main_table.add_column(ntiles_inside, name=\"NTILE_MINE\")\n",
    "main_table.add_column(nearest_tile_ids, name=\"NEAREST_TILEIDS\")\n",
    "\n",
    "add_mag_columns(main_table)\n",
    "\n",
    "main_table.write(IAN_BGS_Y3_MERGED_FILE, format='fits', overwrite='True')\n",
    "\n",
    "del(main_table)\n",
    "del(tiles_BGS)\n",
    "del(galaxies_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_photz_columns(IAN_BGS_Y3_MERGED_FILE, IAN_PHOT_Z_FILE_WSPEC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine data in a Merged BGS File (SV3, Y1, Y3, whatever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick one\n",
    "#table = Table.read(IAN_BGS_MERGED_FILE, format='fits')\n",
    "table = Table.read(IAN_BGS_SV3_MERGED_FILE, format='fits')\n",
    "#table = Table.read(IAN_BGS_SV3_MERGED_NOY3_FILE, format='fits')\n",
    "#table = Table.read(IAN_BGS_Y3_MERGED_FILE, format='fits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See two equivalent ways of determining which rows are for unobserved galaxies\n",
    "one=table['ZWARN'] == 999999\n",
    "two=table['Z'].mask\n",
    "three=table['Z'] == 999999.0\n",
    "assert(np.all(one == two))\n",
    "assert(np.all(one == three))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cut to the galaxy data we actually need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO this gets easilly out of sync with the .py file that does the 'production' filtering\n",
    "\n",
    "if np.ma.is_masked(table['Z']):\n",
    "    print(\"Masked table\")\n",
    "    z_obs = table['Z'].data.data.astype(\"<f8\")\n",
    "    obj_type = table['SPECTYPE'].data.data\n",
    "    unobserved = table['Z'].mask # the masked values are what is unobserved\n",
    "    deltachi2 = table['DELTACHI2'].data.data  \n",
    "    maskbits = table['MASKBITS'].data.data\n",
    "else:\n",
    "    print(\"Unmasked table\")\n",
    "    # SV3 version didn't do this\n",
    "    z_obs = table['Z']\n",
    "    obj_type = table['SPECTYPE']\n",
    "    unobserved = table['Z'].astype(\"<i8\") == 999999\n",
    "    deltachi2 = table['DELTACHI2']\n",
    "    maskbits = table['MASKBITS']\n",
    "    \n",
    "dec = table['DEC'].astype(\"<f8\")\n",
    "ra = table['RA'].astype(\"<f8\")\n",
    "z_phot = table['Z_PHOT'].astype(\"<f8\")\n",
    "target_id = table['TARGETID']\n",
    "app_mag_r = get_app_mag(table['FLUX_R'])\n",
    "app_mag_g = get_app_mag(table['FLUX_G'])\n",
    "flux_r = table['FLUX_R'].astype(\"<f8\")\n",
    "flux_g = table['FLUX_G'].astype(\"<f8\")\n",
    "g_r_apparent = app_mag_g - app_mag_r\n",
    "#sdss_g_r = table['ABSMAG_SDSS_G'] - table['ABSMAG_SDSS_R'] \n",
    "#G_R_JM1 = table['ABSMAG01_SDSS_G'] - table['ABSMAG01_SDSS_R']\n",
    "p_obs = table['PROB_OBS'] \n",
    "ntiles = table['NTILE'].astype(\"<i8\")\n",
    "#tiles = table['TILES']\n",
    "#ztileid = table['ZTILEID']\n",
    "tile_id = table['TILEID']\n",
    "#numobs = table['NUMOBS']\n",
    "#tile_locid = table['TILELOCID']\n",
    "ntiles_mine = table['NTILE_MINE']\n",
    "tileids = table['NEAREST_TILEIDS'][:,0].astype(\"<i8\") # TODO there are 10 here, we want NTILES_MINE many...\n",
    "#abs_mag_sdss = table['ABSMAG_SDSS_R']\n",
    "#dn4000 = table['DN4000'].data.data\n",
    "ref_cat = table['REF_CAT']\n",
    "\n",
    "\n",
    "before_count = len(dec)\n",
    "print(before_count, \"objects in FITS file\")\n",
    "\n",
    "# TODO BUG Can we be mistaking STARS for GALAXIES?\n",
    "# Make filter array (True/False values)\n",
    "PASSES_REQUIRED = [1,2,3,4,10]\n",
    "\n",
    "galaxy_observed_filter = obj_type == b'GALAXY'\n",
    "app_mag_filter = app_mag_r < APP_MAG_CUT\n",
    "redshift_filter = z_obs > Z_MIN\n",
    "redshift_hi_filter = z_obs < Z_MAX\n",
    "deltachi2_filter = deltachi2 > 40\n",
    "#abs_mag_sdss_filter = abs_mag_sdss < 100\n",
    "#observed_requirements = np.all([galaxy_observed_filter, app_mag_filter, redshift_filter, redshift_hi_filter, deltachi2_filter, abs_mag_sdss_filter], axis=0)\n",
    "observed_requirements = np.all([galaxy_observed_filter, app_mag_filter, redshift_filter, redshift_hi_filter, deltachi2_filter], axis=0)\n",
    "\n",
    "treat_as_unobserved = np.all([galaxy_observed_filter, app_mag_filter, np.invert(deltachi2_filter)], axis=0)\n",
    "\n",
    "unobserved = np.all([app_mag_filter, np.logical_or(unobserved, treat_as_unobserved)], axis=0)\n",
    "keep = np.all([np.logical_or(observed_requirements, unobserved)], axis=0)\n",
    "\n",
    "print(\"\\nWhole sample:\")\n",
    "print(f\"There are {len(obj_type):,} objects in the entire sample, of which {np.sum(galaxy_observed_filter):,} are observed galaxies.\") \n",
    "\n",
    "for n in PASSES_REQUIRED:\n",
    "    n_pass_filter = ntiles_mine >= n\n",
    "    n_pass_filter_old = ntiles >= n\n",
    "    unobserved_n = np.all([n_pass_filter, unobserved], axis=0)\n",
    "    observed_requirements_n = np.all([n_pass_filter, observed_requirements], axis=0)\n",
    "    keepn = np.all([np.logical_or(observed_requirements_n, unobserved_n)], axis=0)\n",
    "\n",
    "    print(f\"\\n{n}-pass analysis (NTILE_MINE):\")\n",
    "    print(f\"There are {np.sum(observed_requirements_n):,} galaxies in the <{APP_MAG_CUT} mag sample that pass our quality checks.\")\n",
    "    print(f\"There are {np.sum(unobserved_n):,} unobserved galaxies, including bad observed galaxies.\")\n",
    "    print(f\"This {n}-pass catalog would have {np.sum(keepn):,} galaxies ({np.sum(unobserved_n) / np.sum(keepn) * 100:.2f}% lost).\")\n",
    "\n",
    "    # We've demonstratred this is definetely not what we want\n",
    "    #unobserved_n_old = np.all([n_pass_filter_old, unobserved], axis=0)\n",
    "    #observed_requirements_n_old = np.all([n_pass_filter_old, observed_requirements], axis=0)\n",
    "    #keepn_old = np.all([np.logical_or(observed_requirements_n_old, unobserved_n_old)], axis=0)\n",
    "    #print(f\"\\n{n}-pass analysis (NTILE):\")\n",
    "    #print(f\"There are {np.sum(observed_requirements_n_old):,} galaxies in the bright (<{APP_MAG_CUT} mag) sample that pass our quality checks.\")\n",
    "    #print(f\"There are {np.sum(unobserved_n_old):,} unobserved galaxies, including bad observed galaxies.\")\n",
    "    #print(f\"This {n}-pass catalog would have {np.sum(keepn_old):,} galaxies ({np.sum(unobserved_n_old) / np.sum(keepn_old) * 100:.2f}% lost).\")\n",
    "\n",
    "# FOR PARTS BELOW SET WHAT YOU WANT TO KEEP!\n",
    "keep = np.all([keep, ntiles_mine >= KEEP_PASSES, ~unobserved], axis=0)\n",
    "#keep = np.all([keep, ntiles_mine >= KEEP_PASSES], axis=0)\n",
    "\n",
    "obj_type = obj_type[keep]\n",
    "dec = dec[keep]\n",
    "ra = ra[keep]\n",
    "z_phot = z_phot[keep]\n",
    "z_obs = z_obs[keep]\n",
    "target_id = target_id[keep] \n",
    "flux_r = flux_r[keep]\n",
    "app_mag_r = app_mag_r[keep]\n",
    "app_mag_g = app_mag_g[keep]\n",
    "g_r_apparent = g_r_apparent[keep]\n",
    "p_obs = p_obs[keep]\n",
    "unobserved = unobserved[keep]\n",
    "deltachi2 = deltachi2[keep]\n",
    "ntiles = ntiles[keep]\n",
    "#tiles = tiles[keep]\n",
    "#ztileid = ztileid[keep]\n",
    "ntiles_mine = ntiles_mine[keep]\n",
    "tileids = tileids[keep]\n",
    "tile_id = tile_id[keep]\n",
    "#numobs = numobs[keep]\n",
    "#tile_locid = tile_locid[keep]\n",
    "#abs_mag_sdss = abs_mag_sdss[keep]\n",
    "#sdss_g_r = sdss_g_r[keep]\n",
    "#G_R_JM1 = G_R_JM1[keep]\n",
    "#dn4000 = dn4000[keep]\n",
    "ref_cat = ref_cat[keep]\n",
    "maskbits = maskbits[keep]\n",
    "indexes_not_assigned = np.argwhere(unobserved)\n",
    "\n",
    "after_count = len(dec)\n",
    "\n",
    "print(f\"\\nAfter all filters we have {after_count:,} of the original {before_count:,} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make maps\n",
    "two_pass_filter = ntiles_mine >= 2 \n",
    "three_pass_filter = ntiles_mine >= 3 \n",
    "four_pass_filter = ntiles_mine >= 4 \n",
    "\n",
    "ra2 = ra[two_pass_filter]\n",
    "dec2 = dec[two_pass_filter]\n",
    "tileids2 =  tileids[two_pass_filter]\n",
    "unobserved2 = unobserved[two_pass_filter]\n",
    "\n",
    "ra3 = ra[three_pass_filter]\n",
    "dec3 = dec[three_pass_filter]\n",
    "tileids3 =  tileids[three_pass_filter]\n",
    "unobserved3 = unobserved[three_pass_filter]\n",
    "\n",
    "ra4 = ra[four_pass_filter]\n",
    "dec4 = dec[four_pass_filter]\n",
    "tileids4 =  tileids[four_pass_filter]\n",
    "unobserved4 = unobserved[four_pass_filter]\n",
    "\n",
    "one_pass_df = pd.DataFrame({'RA': ra, 'Dec': dec, 'z_assigned_flag': unobserved, 'TILEID': tileids})\n",
    "two_pass_df = pd.DataFrame({'RA': ra2, 'Dec': dec2, 'z_assigned_flag': unobserved2, 'TILEID': tileids2})\n",
    "three_pass_df = pd.DataFrame({'RA': ra3, 'Dec': dec3, 'z_assigned_flag': unobserved3, 'TILEID': tileids3})\n",
    "four_pass_df = pd.DataFrame({'RA': ra4, 'Dec': dec4, 'z_assigned_flag': unobserved4, 'TILEID': tileids4})\n",
    "\n",
    "#fig=make_map(ra, dec)\n",
    "#ra_4 = ra[four_pass_filter]\n",
    "#dec_4 = dec[four_pass_filter]\n",
    "#print(f\"Number of 4-pass galaxies: {len(ra_4)}, number of 3-pass galaxies: {len(ra)}\")\n",
    "#fig=make_map(ra_4, dec_4, fig=fig, alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_positions(one_pass_df, three_pass_df, tiles_df=tiles_BGS, DEG_LONG=2, split=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See where missing photo-z's are\n",
    "idx_no_zphot = z_phot == -99.0\n",
    "no_photoz_df = pd.DataFrame({'RA': ra[idx_no_zphot], 'Dec': dec[idx_no_zphot], 'z_assigned_flag': unobserved[idx_no_zphot], 'TILEID': tileids[idx_no_zphot]})\n",
    "#plot_positions(one_pass_df, no_photoz_df, tiles_df=tiles_BGS, DEG_LONG=3, split=False)\n",
    "\n",
    "fig=make_map(ra, dec)\n",
    "#ra_4 = ra[four_pass_filter]\n",
    "#dec_4 = dec[four_pass_filter]\n",
    "#print(f\"Number of 4-pass galaxies: {len(ra_4)}, number of 3-pass galaxies: {len(ra)}\")\n",
    "#fig=make_map(ra[idx_no_zphot], dec[idx_no_zphot], fig=fig, alpha=0.1)\n",
    "fig=make_map(ra[ntiles_mine >= 10 ], dec[ntiles_mine >= 10 ], fig=fig, alpha=0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Siena Galaxy Atlas Analysis (SGA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(ref_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understand how many galaxies are affected by Siena Galaxy Atlas (SGA) Masks\n",
    "has_a_maskbit = maskbits != 0\n",
    "idx_with_masks = np.flatnonzero(maskbits)\n",
    "print(f\"{np.sum(has_a_maskbit):,} galaxies ({np.sum(has_a_maskbit) / len(maskbits) * 100:.2f}%) have a maskbit set.\")\n",
    "\n",
    "unobserved_with_maskbits = np.logical_and(has_a_maskbit, unobserved)\n",
    "print(f\"{np.sum(unobserved_with_maskbits):,} galaxies ({np.sum(unobserved_with_maskbits) / len(maskbits) * 100:.2f}%) have a maskbit set and are unobserved.\")\n",
    "\n",
    "# See https://www.legacysurvey.org/dr9/bitmasks/\n",
    "# https://github.com/legacysurvey/legacypipe/blob/master/py/legacypipe/bits.py\n",
    "BITMASK_SGA = 0x1000 \n",
    "sga_collision = (maskbits & BITMASK_SGA) != 0\n",
    "print(f\"{np.sum(sga_collision):,} galaxies ({np.sum(sga_collision) / len(maskbits) * 100:.2f}%) have a SGA collision.\")\n",
    "\n",
    "sga_collision_unobs = np.logical_and(sga_collision, unobserved)\n",
    "print(f\"{np.sum(sga_collision_unobs):,} galaxies ({np.sum(sga_collision_unobs) / len(maskbits) * 100:.2f}%) have a SGA collision and are unobserved.\")\n",
    "\n",
    "#sga_central = np.logical_or(ref_cat == b'L3', ref_cat == b'G2', ref_cat == b'GE')\n",
    "sga_central = ref_cat == b'L3'\n",
    "print(f\"{np.sum(sga_central):,} galaxies ({np.sum(sga_central) / len(maskbits) * 100:.2f}%) are SGA centrals.\")\n",
    "\n",
    "to_remove = sga_collision & ~sga_central\n",
    "print(f\"{np.sum(to_remove):,} galaxies ({np.sum(to_remove) / len(maskbits) * 100:.2f}%) have a SGA collision and are not SGA centrals.\")\n",
    "\n",
    "sga_ra = ra[to_remove]\n",
    "sga_dec = dec[to_remove]\n",
    "df = pd.DataFrame({'RA': sga_ra, 'Dec': sga_dec})\n",
    "df.to_csv(OUTPUT_FOLDER + f'sga_collisions.csv', index=False)\n",
    "\n",
    "# Inspecting these, I see that most are galaxies, not HII regions. Not sure if we want to remove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Photo-Z vs Spec-Z Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"There are {(z_phot != -99.0).sum():,} ({(z_phot != -99.0).sum() / len(z_phot):.1%}) targets with phot-z\")\n",
    "\n",
    "good_idx = np.flatnonzero((z_phot != -99.0) & ~unobserved)\n",
    "print(f\"Amongst observed galaxies there are {len(good_idx):,} ({len(good_idx) / np.sum(~unobserved) * 100:.2f}%) galaxies with photo-z.\")\n",
    "\n",
    "unobserved_with_photz = np.flatnonzero((z_phot != -99.0) & unobserved)\n",
    "print(f\"Amongst unobserved galaxies there are {len(unobserved_with_photz):,} ({len(unobserved_with_photz) / np.sum(unobserved) * 100:.2f}%) with photo-z.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_z = z_phot[good_idx] - z_obs[good_idx]\n",
    "plt.hist(delta_z, bins=500, range=(-0.1, 0.1))\n",
    "plt.yscale(\"log\")\n",
    "plt.title(\"Photo-z Quality\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xlabel(\"z_phot - z_spec\")\n",
    "\n",
    "# add bars for my z_thresh\n",
    "plt.axvline(-SIM_Z_THRESH, color='red')\n",
    "plt.axvline(SIM_Z_THRESH, color='red')\n",
    "\n",
    "percentiles = np.percentile(delta_z, [16, 50, 84])\n",
    "print(f\"Median delta z: {percentiles[1]:.4f}, 16th percentile: {percentiles[0]:.4f}, 84th percentile: {percentiles[2]:.4f}\")\n",
    "# add bars for the percentiles\n",
    "#plt.axvline(percentiles[0], color='green')\n",
    "#plt.axvline(percentiles[2], color='green')\n",
    "\n",
    "\n",
    "\n",
    "# What % fall within 0.005 of the true redshift?\n",
    "within_5_milli = np.abs(delta_z) < SIM_Z_THRESH\n",
    "print(f\"{np.sum(within_5_milli) / len(delta_z) * 100:.2f}% of galaxies have a photometric redshift within {SIM_Z_THRESH} of the spectroscopic redshift.\")\n",
    "\n",
    "# Now look only at quiescent galaxies less than 10^9 solar luminosities\n",
    "# TODO \n",
    "#luminosity = abs_mag_r_to_log_solar_L(app_mag_to_abs_mag_k(app_mag_r, z_obs, g_r_apparent))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SV3 Analysis\n",
    "\n",
    "SV3 is composed of 20 regions where 10 or 11 exposures eacj were taken, almost completely on top of each other.  Our SV3 analysis takes the inner part of these patches (NTILE_MINE >= 10) of these regions as the data set.  \n",
    "\n",
    "Then, we can eliminate 1 tile from each of these regions to make test sets in order to view our systematics as a function of NTILE_MINE. The order they are eliminated in matters; we need to go backwards in time.}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a DataFrame filtered down to the galaxies we want to keep\n",
    "sv3_merged_table = Table.read(IAN_BGS_SV3_MERGED_FILE, format='fits')\n",
    "sv3_merged_table.remove_column('NEAREST_TILEIDS')\n",
    "sv3_df = sv3_merged_table.to_pandas()\n",
    "print(len(sv3_df))\n",
    "sv3_df['app_mag'] = get_app_mag(sv3_df['FLUX_R'])\n",
    "unobserved = sv3_merged_table['Z'].astype(\"<i8\") == 999999\n",
    "galaxy_observed_filter = sv3_df['SPECTYPE'] == b'GALAXY'\n",
    "redshift_filter = sv3_df['Z'] > Z_MIN\n",
    "redshift_hi_filter = sv3_df['Z'] < Z_MAX\n",
    "deltachi2_filter = sv3_df['DELTACHI2'] > 40\n",
    "app_mag_filter = sv3_df['app_mag'] < 20.3\n",
    "observed_requirements = np.all([galaxy_observed_filter, app_mag_filter, redshift_filter, redshift_hi_filter, deltachi2_filter], axis=0)\n",
    "treat_as_unobserved = np.all([galaxy_observed_filter, app_mag_filter, np.invert(deltachi2_filter)], axis=0)\n",
    "\n",
    "unobserved = np.all([app_mag_filter, np.logical_or(unobserved, treat_as_unobserved)], axis=0)\n",
    "sv3_df['OBSERVED'] = np.invert(unobserved)\n",
    "keep = np.all([np.logical_or(observed_requirements, unobserved)], axis=0)\n",
    "keep = np.all([keep, sv3_df['NTILE_MINE'] >= 10], axis=0)\n",
    "\n",
    "sv3_df = sv3_df.loc[keep] \n",
    "sv3_df.reset_index(drop=True, inplace=True)\n",
    "print(len(sv3_df))\n",
    "\n",
    "# Initialize new columns for observed as function of N pass\n",
    "for i in range(0, 12):\n",
    "    sv3_df[f'OBSERVED_{i}'] = sv3_df['OBSERVED']\n",
    "\n",
    "for FAINT in [False, True]:\n",
    "\n",
    "    if not FAINT:\n",
    "        mag_filter = sv3_df['app_mag'] < 19.5\n",
    "    else:\n",
    "        mag_filter = sv3_df['app_mag'] > 19.5\n",
    "        \n",
    "    print(f\"{len(sv3_df[mag_filter]) / 138.192} galaxies per sq degree\")\n",
    "\n",
    "    for patch_number in range(len(gc.sv3_regions_sorted)):\n",
    "        tilelist = gc.sv3_regions_sorted[patch_number]\n",
    "        #print(f'Patch {patch_number} - TILE IDs: {tilelist}')\n",
    "        \n",
    "        row_selector = np.logical_and(sv3_df['TILEID'].isin(tilelist), mag_filter)\n",
    "\n",
    "        #one_patch_df = sv3_df[sv3_df['TILEID'].isin(tilelist)]\n",
    "        #print(f\"{len(one_patch_df)} galaxies, {np.sum(one_patch_df['OBSERVED']) / len(one_patch_df) :.1%} of the targets are observed\")\n",
    "        #one_patch_df[f'OBSERVED_{len(tilelist)}'] = one_patch_df['OBSERVED']\n",
    "        \n",
    "        #print (\"Remove tiles in reverse TILEID order:\")\n",
    "        for i in np.flip(np.arange(0, len(tilelist))):\n",
    "            tileid = tilelist[i]\n",
    "            observed_by_this_tile = sv3_df.loc[row_selector, 'TILEID'] == tileid\n",
    "            #print(f'{np.sum(observed_by_this_tile)} galaxies were observed by tile {tileid} ({i+1}/{len(tilelist)})')\n",
    "            prev = sv3_df.loc[row_selector, f'OBSERVED_{i+1}']\n",
    "            sv3_df.loc[row_selector, f'OBSERVED_{i}'] = np.where(observed_by_this_tile, False, prev)\n",
    "            \n",
    "        #for i in np.flip(np.arange(0, len(tilelist)+1)):\n",
    "        #    if FAINT:\n",
    "        #        totals_observed_faint[i] += np.sum(sv3_df.loc[row_selector, f'OBSERVED_{i}'])\n",
    "        #        totals_all_faint[i] += len(sv3_df.loc[row_selector])\n",
    "        #    else:\n",
    "        #        totals_observed_bright[i] += np.sum(sv3_df.loc[row_selector, f'OBSERVED_{i}'])\n",
    "        #        totals_all_bright[i] += len(sv3_df.loc[row_selector])\n",
    "\n",
    "            #print(f\"{np.sum(one_patch_df[f'OBSERVED_{i}']) / len(one_patch_df) :.1%} of the targets are observed with {i} passes\")\n",
    "                \n",
    "    #for i in range(1, 12):\n",
    "    #    if FAINT:\n",
    "    #        print(f\"{totals_observed_faint[i]:,} ({totals_observed_faint[i] / totals_all_faint[i]:.1%}) faint galaxies are observed with {i} passes\")\n",
    "    #    else: \n",
    "    #        print(f\"{totals_observed_bright[i]:,} ({totals_observed_bright[i] / totals_all_bright[i]:.1%}) bright galaxies are observed with {i} passes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observed_faint = np.zeros(12, dtype=int)\n",
    "observed_bright = np.zeros(12, dtype=int)\n",
    "\n",
    "total_faint = np.sum(sv3_df['app_mag'] > 19.5)\n",
    "total_bright = np.sum(sv3_df['app_mag'] < 19.5)\n",
    "\n",
    "for i in range(0, 12):\n",
    "    observed_faint[i] = np.sum(sv3_df.loc[sv3_df['app_mag'] > 19.5, f'OBSERVED_{i}'])\n",
    "    observed_bright[i] = np.sum(sv3_df.loc[sv3_df['app_mag'] < 19.5, f'OBSERVED_{i}'])\n",
    "\n",
    "\n",
    "plt.plot(observed_bright / total_bright, color='b', label=\"BGS BRIGHT ME\")\n",
    "plt.plot(observed_faint / total_faint, color='orange', label=\"BGS FAINT ME\")\n",
    "plt.plot([1,2,3,4], [.29, .52, 0.68, .81], '--', color='b', label=\"BGS BRIGHT PAPER\")\n",
    "plt.plot([1,2,3,4], [.15, .32, 0.47, .62], '--', color='orange', label=\"BGS FAINT PAPER\")\n",
    "plt.xlabel(\"Number of passes\")\n",
    "plt.ylabel(\"Fraction of targets observed\")\n",
    "plt.title(\"SV3 BGS Completeness\")\n",
    "plt.xticks(np.arange(0, 12))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sv3_df.loc[sv3_df['app_mag'] < 19.5, 'OBSERVED'].count() - sv3_df.loc[sv3_df['app_mag'] < 19.5, 'OBSERVED'].sum())\n",
    "print(sv3_df.loc[sv3_df['app_mag'] < 19.5, 'OBSERVED'].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above numbers do not seem to track with the Y1 data. In Y1 no region has more than 4 passes so how do I have a fiber incompleteness better than the above number?\n",
    "\n",
    "Also Figure 17 of https://iopscience.iop.org/article/10.3847/1538-3881/accff8/pdf disagrees with my above analysis. So what is above is wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nearest Neighbor Analysis (for SV3 Merged File)\n",
    "\n",
    "Have a nearest neighbor catalog that is all the actually observed galaxies in SV3 with my Y3 supplement. \n",
    "\n",
    "Can run analysis on entire sample or a subset of 'pretend-to-be unobserved' galaxies (~obs_7p)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAKE SURE that above KEEP_PASSES is set to 1 and you filter out unobserved galaxies as we need a source of 'truth'\n",
    "assert KEEP_PASSES == 1 and np.sum(unobserved) == 0\n",
    "\n",
    "gmr = app_mag_g - app_mag_r\n",
    "Rk = app_mag_to_abs_mag_k(app_mag_r, z_obs, gmr, band='r')\n",
    "Gk = app_mag_to_abs_mag_k(app_mag_g, z_obs, gmr, band='g')\n",
    "quiescent = is_quiescent_BGS_gmr(None, Gk-Rk)\n",
    "\n",
    "# We will only consider the observed galaxies in SV3 as we need a source of truth for the analysis.\n",
    "# This is over 98% so this analysis should be representative.\n",
    "# We then pretend to have not observed about 20% of galaxies, as is the case in the main survey.\n",
    "# TODO BUG Ashley says my method is wrong for doing this.\n",
    "in_10p_zone = ntiles_mine >= 10\n",
    "obs_7p = ~gc.drop_SV3_passes(3, tile_id, unobserved)\n",
    "\n",
    "print(np.sum(quiescent))\n",
    "print(np.sum(in_10p_zone))\n",
    "print(np.sum(obs_7p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_id.mask.sum() # BUG prevents using LOST_GALAXIES_ONLY=True. Is it because of Y3 supplement has no TILE_ID?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = NNAnalyzer(dec, ra, z_obs, app_mag_r, Rk, None, g_r_apparent, quiescent, obs_7p, p_obs)\n",
    "#obj = NNAnalyzer(dec[obs_10p], ra[obs_10p], z_obs[obs_10p], app_mag_r[obs_10p], Rk[obs_10p], None, g_r_apparent[obs_10p], quiescent[obs_10p], obs_7p, p_obs[obs_10p])\n",
    "#obj.set_row_locator( np.logical_and(obs_10p, app_mag_r < 19.5) ) # 10p inner regions and BRIGHT only\n",
    "obj.set_row_locator(in_10p_zone) # 10p inner regions and BRIGHT only\n",
    "obj.find_nn_properties(LOST_GALAXIES_ONLY=False) # True (look only at lost ones) would be better,\n",
    "# but the galaxy count is so small that binning makes it hard to interpret. \n",
    "# Plus, our method of making the lost galaxies is sus.\n",
    "# TODO make True after using CIC and see if its OK\n",
    "obj.make_bins()\n",
    "\n",
    "print(np.sum(obj.all_ang_bincounts))\n",
    "print(np.sum(obj.all_sim_z_bincounts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newobj = NNAnalyzer_cic.from_data(dec, ra, z_obs, app_mag_r, Rk, g_r_apparent, quiescent, obs_7p, p_obs)\n",
    "#newobj.set_row_locator( np.logical_and(obs_10p, app_mag_r < 19.5) ) # 10p inner regions and BRIGHT only\n",
    "newobj.set_row_locator(in_10p_zone)\n",
    "newobj.find_nn_properties(LOST_GALAXIES_ONLY=False) \n",
    "newobj.make_bins()\n",
    "newobj.save(OUTPUT_FOLDER + 'BGS_cic_binned_data.pkl')\n",
    "\n",
    "print(np.sum(obj.all_ang_bincounts))\n",
    "print(np.sum(obj.all_sim_z_bincounts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_nna = NNAnalyzer_cic.from_results_file(OUTPUT_FOLDER + 'BGS_cic_binned_data.pkl')\n",
    "assert np.all(loaded_nna.all_ang_bincounts == newobj.all_ang_bincounts)\n",
    "assert np.all(loaded_nna.all_sim_z_bincounts == newobj.all_sim_z_bincounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = obj.all_ang_bincounts - newobj.all_ang_bincounts\n",
    "assert np.isclose(np.sum(delta), 0), f\"Sum of differences is {np.sum(delta)}\"\n",
    "\n",
    "delta_z = obj.all_sim_z_bincounts - newobj.all_sim_z_bincounts\n",
    "assert np.isclose(np.sum(delta_z), 0), f\"Sum of differences is {np.sum(delta_z)}\"\n",
    "# This difference could be from the clipping of the data in the CIC method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p_obs is messed up for SV3 so these don't look great\n",
    "obj.plot_angdist_pobs_per_zbin_cc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These look OK \n",
    "obj.plot_angdist_appmag_per_zbin_cc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These look OK \n",
    "newobj.plot_angdist_appmag_per_zbin_cc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = obj.frac_aa - newobj.frac_aa\n",
    "j=plt.hist(d.flatten(), bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO de-dupe with pre_process_BGS(...)\n",
    "\n",
    "# Examine photo-z NN relation\n",
    "NUM_NEIGHBORS = 30\n",
    "unobs_7p = ~obs_7p\n",
    "bright = app_mag_r[in_10p_zone] < 19.5\n",
    "use = bright & unobs_7p\n",
    "neighbor_indexes = np.zeros(shape=(NUM_NEIGHBORS, use.sum()), dtype=np.int32) # indexes point to CATALOG locations\n",
    "ang_distances = np.zeros(shape=(NUM_NEIGHBORS, use.sum()))\n",
    "\n",
    "catalog = coord.SkyCoord(ra=ra[in_10p_zone][obs_7p]*u.degree, dec=dec[in_10p_zone][obs_7p]*u.degree, frame='icrs')\n",
    "\n",
    "print(f\"Finding nearest {NUM_NEIGHBORS} neighbors... \", end='\\r')   \n",
    "for n in range(0, NUM_NEIGHBORS):\n",
    "    to_match = coord.SkyCoord(ra=ra[in_10p_zone][use]*u.degree, dec=dec[in_10p_zone][use]*u.degree, frame='icrs')\n",
    "    idx, d2d, d3d = coord.match_coordinates_sky(to_match, catalog, nthneighbor=n+1, storekdtree='sv3')\n",
    "    neighbor_indexes[n] = idx\n",
    "    ang_distances[n] = d2d.to(u.arcsec).value\n",
    "print(f\"Finding nearest {NUM_NEIGHBORS} neighbors... done!\")   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ztruth = z_obs[in_10p_zone][use]\n",
    "# fill zphot_fake with ztruth + a random gaussian draw around 0.0 with sigma of 0.01\n",
    "zphot = z_phot[in_10p_zone][use]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_pass_df.z_assigned_flag = ~obs_7p\n",
    "plot_positions(one_pass_df, tiles_df=None, DEG_LONG=2.5, split=True, ra_min=one_pass_df.RA[0], dec_min=one_pass_df.Dec[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramspace = np.arange(0.001, 0.05, 0.001)\n",
    "                       \n",
    "cumulative_percent_correct_by_n_zp_firstonly = np.zeros((NUM_NEIGHBORS, len(paramspace)) , dtype=float)\n",
    "\n",
    "i = 0\n",
    "for PHOTOZ_MATCHING_THRESHOLD in paramspace:\n",
    "    \n",
    "    this_neighbor_correct = np.zeros(shape=(NUM_NEIGHBORS, len(ztruth)), dtype=bool)\n",
    "    z_phot_neighbor_match = np.zeros(shape=(NUM_NEIGHBORS, len(ztruth)), dtype=bool)\n",
    "    cumulative_percent_z_phot_neighbor_match = []\n",
    "    z_phot_first_neighbor_match_idx = np.ones(len(ztruth), dtype=int) * 999 # sentinal value for no match\n",
    "    z_phot_match_correct = np.zeros(shape=(NUM_NEIGHBORS, len(ztruth)), dtype=bool)\n",
    "    delta_z = np.zeros(shape=(NUM_NEIGHBORS, len(ztruth)), dtype=float)\n",
    "\n",
    "    percent_correct_by_n = []\n",
    "    percent_correct_by_n_zp = []\n",
    "    cumulative_percent_correct_by_n = []\n",
    "    cumulative_percent_correct_by_n_zp = []\n",
    "    cumulative_percent_correct_by_n_zp_closestonly = []\n",
    "    first_matched_but_incorrect = []\n",
    "\n",
    "    for n in range(0, NUM_NEIGHBORS):\n",
    "\n",
    "        z_neighbor = z_obs[in_10p_zone][obs_7p][neighbor_indexes[n]]\n",
    "        this_neighbor_correct[n] = close_enough(ztruth, z_neighbor)\n",
    "        percent_correct_by_n.append(this_neighbor_correct[n].sum() / len(ztruth))\n",
    "        any_neighbor_correct = this_neighbor_correct[0:n+1].max(axis=0) # max will be True if any neighbor is True\n",
    "        cumulative_percent_correct_by_n.append(any_neighbor_correct.sum() / len(ztruth))\n",
    "\n",
    "        z_phot_neighbor_match[n] = close_enough(zphot, z_neighbor, threshold=PHOTOZ_MATCHING_THRESHOLD)\n",
    "        any_z_phot_match = z_phot_neighbor_match[0:n+1].max(axis=0) # will be True if z_phot matches a neighbor\n",
    "        cumulative_percent_z_phot_neighbor_match.append(any_z_phot_match.sum() / len(ztruth))\n",
    "        z_phot_match_correct[n] = z_phot_neighbor_match[n] & this_neighbor_correct[n]\n",
    "        any_neighbor_zp_correct = z_phot_match_correct[0:n+1].max(axis=0) # will be True if z_phot matches a neighbor\n",
    "        percent_correct_by_n_zp.append(z_phot_match_correct[n].sum() / len(ztruth))\n",
    "        cumulative_percent_correct_by_n_zp.append(any_neighbor_zp_correct.sum() / len(ztruth))\n",
    "\n",
    "\n",
    "        # Now only consider the closest neighbor, by photo-z matching\n",
    "        delta_z[n] = zphot - z_neighbor\n",
    "        best_match_idx = np.argmin(delta_z[0:n+1], axis=0, keepdims=True)\n",
    "        closest_delta_z_correct = np.take_along_axis(z_phot_match_correct, best_match_idx, axis=0)[0] # will be True bset zphot match is correct\n",
    "        cumulative_percent_correct_by_n_zp_closestonly.append(closest_delta_z_correct.sum() / len(ztruth))\n",
    "\n",
    "        # Now only consider the first neighbor in order of angular distance\n",
    "        z_phot_first_neighbor_match_idx = np.minimum(z_phot_first_neighbor_match_idx, np.where(z_phot_neighbor_match[n],n,999))\n",
    "        has_match = z_phot_first_neighbor_match_idx != 999\n",
    "\n",
    "        # if z_phot_first_neighbor_match_idx is 999 for  row, first_correct will be False\n",
    "        # if z_phot_first_neighbor_match_idx is an index for row, first_correct will be z_phot_match_correct value for that row at that index\n",
    "        first_correct = np.repeat(False, len(ztruth))\n",
    "        first_correct[has_match] = z_phot_match_correct[z_phot_first_neighbor_match_idx[has_match], np.arange(len(ztruth))[has_match]]\n",
    "        cumulative_percent_correct_by_n_zp_firstonly[n,i] = (first_correct.sum() / len(ztruth))\n",
    "\n",
    "        first_matched_but_incorrect.append(cumulative_percent_z_phot_neighbor_match[n] - cumulative_percent_correct_by_n_zp_firstonly[n,i])\n",
    "\n",
    "    if i == 26:\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(range(1, NUM_NEIGHBORS+1), percent_correct_by_n, label=\"This Neighbor is correct z\")\n",
    "        plt.plot(range(1, NUM_NEIGHBORS+1), percent_correct_by_n_zp, label=\"This Neighbor photo-z matched and correct (thresh={PHOTOZ_MATCHING_THRESHOLD:.3})\")\n",
    "        plt.plot(range(1, NUM_NEIGHBORS+1), cumulative_percent_correct_by_n, label=\"Any Neighbor has correct z\")\n",
    "        plt.plot(range(1, NUM_NEIGHBORS+1), cumulative_percent_correct_by_n_zp, label=\"Any neighbor photo-z matched and correct (thresh={PHOTOZ_MATCHING_THRESHOLD:.3})\")\n",
    "        plt.plot(range(1, NUM_NEIGHBORS+1), cumulative_percent_correct_by_n_zp_closestonly, label=\"Minimum delta-z matched neighbor correct (thresh={PHOTOZ_MATCHING_THRESHOLD:.3})\")\n",
    "        plt.plot(range(1, NUM_NEIGHBORS+1), cumulative_percent_correct_by_n_zp_firstonly[:,i], label=f\"First photo-z matched neighbor correct (thresh={PHOTOZ_MATCHING_THRESHOLD:.3})\")\n",
    "        plt.plot(range(1, NUM_NEIGHBORS+1), cumulative_percent_z_phot_neighbor_match, label=\"Any Neighbor photo-z matched (thresh={PHOTOZ_MATCHING_THRESHOLD:.3})\")\n",
    "        plt.ylabel(\"Fraction\")\n",
    "        plt.xlabel(\"Nth Nearest Neighbor\")\n",
    "        plt.xticks(np.arange(1, NUM_NEIGHBORS+1))\n",
    "        plt.ylim(0, 0.8)\n",
    "        #plt.axhline(y=0.0125, color='r', linestyle='--', label=\"Random Chance\")\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.figure(figsize=(10, 5))\n",
    "        # stacked bar chart of \n",
    "        plt.bar(range(1, NUM_NEIGHBORS+1), cumulative_percent_correct_by_n_zp_firstonly[:,i], label=f\"First photo-z matched neighbor correct (thresh={PHOTOZ_MATCHING_THRESHOLD:.3})\")\n",
    "        plt.bar(range(1, NUM_NEIGHBORS+1), first_matched_but_incorrect, label=f\"First photo-z matched neighbor incorrect (thresh={PHOTOZ_MATCHING_THRESHOLD:.3})\", bottom = cumulative_percent_correct_by_n_zp_firstonly[:,i])\n",
    "\n",
    "        plt.ylabel(\"Fraction\")\n",
    "        plt.xlabel(\"Nth Nearest Neighbor\")\n",
    "        plt.xticks(np.arange(1, NUM_NEIGHBORS+1))\n",
    "        plt.ylim(0, 1.0)\n",
    "        plt.legend()\n",
    "    \n",
    "    i = i + 1\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the maximum point\n",
    "best = np.max(cumulative_percent_correct_by_n_zp_firstonly)\n",
    "best_idx = np.unravel_index(np.argmax(cumulative_percent_correct_by_n_zp_firstonly), cumulative_percent_correct_by_n_zp_firstonly.shape)\n",
    "print(cumulative_percent_correct_by_n_zp_firstonly.shape)\n",
    "print(best_idx)\n",
    "print(f\"Best photo-z match threshold: {paramspace[best_idx[1]]:.3f}, Neighbors={best_idx[0]+1}, {best:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, NUM_NEIGHBORS+1), cumulative_percent_correct_by_n_zp_firstonly[:,4],        label=f\"thresh={paramspace[4]:.3})\", color=[0.1, 0.8, 0])\n",
    "plt.plot(range(1, NUM_NEIGHBORS+1), cumulative_percent_correct_by_n_zp_firstonly[:,9],        label=f\"thresh={paramspace[9]:.3})\", color=[0.2, 0.7, 0])\n",
    "plt.plot(range(1, NUM_NEIGHBORS+1), cumulative_percent_correct_by_n_zp_firstonly[:,19],       label=f\"thresh={paramspace[19]:.3})\", color=[0.3, 0.6, 0])\n",
    "plt.plot(range(1, NUM_NEIGHBORS+1), cumulative_percent_correct_by_n_zp_firstonly[:,26], '--', label=f\"thresh={paramspace[26]:.3})\", color=[0.4, 0.5, 0])\n",
    "plt.plot(range(1, NUM_NEIGHBORS+1), cumulative_percent_correct_by_n_zp_firstonly[:,39],       label=f\"thresh={paramspace[39]:.3})\", color=[0.5, 0.4, 0])\n",
    "plt.plot(range(1, NUM_NEIGHBORS+1), cumulative_percent_correct_by_n_zp_firstonly[:,49],       label=f\"thresh={paramspace[49]:.3})\", color=[0.6, 0.3, 0])\n",
    "plt.plot(range(1, NUM_NEIGHBORS+1), cumulative_percent_correct_by_n_zp_firstonly[:,59],       label=f\"thresh={paramspace[59]:.3})\", color=[0.7, 0.2, 0])\n",
    "plt.plot(range(1, NUM_NEIGHBORS+1), cumulative_percent_correct_by_n_zp_firstonly[:,99],       label=f\"thresh={paramspace[99]:.3})\", color=[0.8, 0.1, 0])\n",
    "plt.ylabel(\"Fraction Correct\")\n",
    "plt.xlabel(\"Nth Nearest Neighbor\")\n",
    "plt.xticks(np.arange(1, NUM_NEIGHBORS+1))\n",
    "plt.ylim(0, 0.45)\n",
    "#plt.axhline(y=0.0125, color='r', linestyle='--', label=\"Random Chance\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a figure showing the best photo-z match for each galaxy\n",
    "plt.figure(figsize=(10, 5))\n",
    "for i in range(0, len(paramspace)):\n",
    "    color = [i/(len(paramspace)+1),0.5,0]\n",
    "    plt.plot(range(1, NUM_NEIGHBORS+1), cumulative_percent_correct_by_n_zp_firstonly[:,i], label=f\"Photo-z Match Threshold {paramspace[i]:.3f}\", color=color)\n",
    "plt.ylabel(\"Fraction Correct\")\n",
    "plt.xlabel(\"Nth Nearest Neighbor\")\n",
    "plt.xticks(np.arange(1, NUM_NEIGHBORS+1))\n",
    "plt.ylim(0, 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Color Analysis\n",
    "\n",
    "Lesson from this analysis: the BGS data, workign with my 0.1^G-R with GAMA k-corrections, does not distribute a per logLgal bin G-R; the global 0.76 split seems to work for all bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = app_mag_to_abs_mag(app_mag_g, z_obs)\n",
    "R = app_mag_to_abs_mag(app_mag_r, z_obs)\n",
    "\n",
    "G_R = G - R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It doesn't matter if you use g_r_apparent or G_R as the difference between the is the same!\n",
    "\n",
    "Gk = k_correct_bgs(G, z_obs, g_r_apparent, band='g')\n",
    "Rk = k_correct_bgs(R, z_obs, g_r_apparent, band='r')\n",
    "G_R_k_BGS1 = Gk - Rk\n",
    "\n",
    "Gk_GAMA = k_correct_gama(G, z_obs, g_r_apparent, band='g')\n",
    "Rk_GAMA = k_correct_gama(R, z_obs, g_r_apparent, band='r')\n",
    "G_R_k_GAMA = Gk_GAMA - Rk_GAMA\n",
    "\n",
    "Gk_BGS2 = k_correct_bgs_v2(G, z_obs, g_r_apparent, band='g')\n",
    "Rk_BGS2 = k_correct_bgs_v2(R, z_obs, g_r_apparent, band='r')\n",
    "G_R_k_BGS2 = Gk_BGS2 - Rk_BGS2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison of g-r computed a few ways\n",
    "bins = np.linspace(0, 2.0, 200)\n",
    "\n",
    "plt.figure()\n",
    "#junk=plt.hist(g_r_apparent, bins=bins, label=\"g-r\", histtype='step', density=True)\n",
    "#junk=plt.hist(sdss_g_r, bins=bins, label='From LSS Pipeline (JM?)', histtype='step', density=True)\n",
    "junk=plt.hist(G_R_JM1, bins=bins, label=\"0.1^(G-R) JM\", histtype='step', density=True)\n",
    "#junk=plt.hist(G_R, bins=bins, label=\"G-R\", histtype='step', density=True)\n",
    "junk=plt.hist(G_R_k_BGS1, bins=bins, label=\"0.1^(G-R) BGS poly v1\", histtype='step', density=True)\n",
    "junk=plt.hist(G_R_k_GAMA, bins=bins, label=\"0.1^(G-R) GAMA poly\", histtype='step', density=True)\n",
    "junk=plt.hist(G_R_k_BGS2, bins=bins, label=\"0.1^(G-R) BGS poly v2\", histtype='step', density=True)\n",
    "plt.xlabel(\"g-r\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend()\n",
    "plt.xlim(0.2, 1.3)\n",
    "plt.title(\"Comparison of g-r computed a few ways\")\n",
    "plt.tight_layout()\n",
    "plt.ylim(0,3.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can see global GLOBAL_RED_COLOR_CUT=0.76 here\n",
    "junk=plt.hist(G_R_k_GAMA, bins=300, alpha=0.5, label=\"0.1^(G-R) GAMA-style\")\n",
    "plt.legend()\n",
    "plt.xlim(0.5, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(is_quiescent_lost_gal_guess(g_r_apparent).sum() / len(g_r_apparent))\n",
    "assert len(G_R_k_GAMA) == len(g_r_apparent)\n",
    "print(is_quiescent_BGS_gmr(None, G_R_k_GAMA).sum() / len(G_R_k_GAMA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyutils import *\n",
    "print(BGS_LOGLGAL_BINS)\n",
    "print(BINWISE_RED_COLOR_CUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_quiescent_BGS_gmr(np.array([5.8, 9.0, 14.5]), np.array([0.5, 0.9, 0.9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get logLgal bins\n",
    "log_L_gal = abs_mag_r_to_log_solar_L(Rk) \n",
    "logLgal_bin_idx = np.digitize(log_L_gal, BGS_LOGLGAL_BINS)\n",
    "# 0 is less than the lowest, len(BGS_LOGLGAL_BINS) is greater than the highest entry in BGS_LOGLGAL_BINS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.min(log_L_gal))\n",
    "print(np.max(log_L_gal))\n",
    "print(np.min(logLgal_bin_idx))\n",
    "print(np.max(logLgal_bin_idx))\n",
    "plt.hist(log_L_gal, bins=BGS_LOGLGAL_BINS, align='mid')\n",
    "#plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a plot of G_R_k in each logLgal bin\n",
    "for i in range(0, len(BGS_LOGLGAL_BINS)+1):\n",
    "    galaxy_idx_for_this_bin = logLgal_bin_idx == i\n",
    "\n",
    "    plt.figure(dpi=80, figsize=(10, 6))\n",
    "    junk=plt.hist(G_R_k_BGS1[galaxy_idx_for_this_bin], bins=np.arange(0,1.3,0.02), label=f\"0.1^(G-R) Bin {i}\", align='mid')\n",
    "    plt.legend()\n",
    "    plt.xlim(0.4, 1.2)\n",
    "    plt.xticks(np.arange(0.4, 1.2, 0.04))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mag1 = abs_mag_sdss\n",
    "mag2 = R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Absolute Magnitudes\n",
    "# Difference is how we k-correct I believe\n",
    "bins = np.linspace(-25, -10, 100)\n",
    "my_counts, my_bins, my_p = plt.hist(mag2, label=\"my abs_mag\", bins=bins, alpha=0.5)\n",
    "alex_counts, alex_bins, alex_p = plt.hist(mag1, label=\"ABSMAG_SDSS_R\", bins=bins, alpha=0.5)\n",
    "plt.xlabel(\"Absolute Mag\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Compare Absolute Mags\")\n",
    "#plt.yscale('log')\n",
    "plt.legend()\n",
    "\n",
    "print(f\"The peak is shifted from ABSMAG_SDSS_R {alex_bins[np.argmax(alex_counts)]:.1f} to my {my_bins[np.argmax(my_counts)]:.1f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=make_map(ra, dec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dn4000 Comparison (BGS, SDSS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdss = pd.read_csv(SDSS_v1_DAT_FILE, delimiter=' ', names=('RA', 'Dec', 'z', 'logLgal', 'V_max', 'quiescent', 'chi'), index_col=False)\n",
    "sdss_galprops = pd.read_csv(\"../data/sdss_galprops_v1.0.dat\", delimiter=' ', names=('Mag_g', 'Mag_r', 'sigma_v', 'Dn4000', 'concentration', 'log_M_star'))\n",
    "sdss = pd.merge(sdss, sdss_galprops, left_index=True, right_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(dn4000, bins=np.linspace(-0.5, 5.0, 100), alpha=0.6, label=\"BGS Y1\")\n",
    "plt.hist(sdss.Dn4000, bins=np.linspace(-0.5, 5.0, 100), alpha=0.8, label=\"SDSS\")\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.xlabel('Dn4000')\n",
    "plt.ylabel('Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(dn4000, bins=np.linspace(-0.5, 5.0, 100), alpha=0.6, label=\"BGS Y1\")\n",
    "plt.hist(sdss.Dn4000, bins=np.linspace(-0.5, 5.0, 100), alpha=0.8, label=\"SDSS\")\n",
    "plt.legend()\n",
    "plt.xlabel('Dn4000')\n",
    "plt.ylabel('Count')\n",
    "plt.xlim(0.9,2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdss_catalog = coord.SkyCoord(ra=sdss.RA.to_numpy()*u.degree, dec=sdss.Dec.to_numpy()*u.degree, frame='icrs')\n",
    "BGS_catalog = coord.SkyCoord(ra=ra*u.degree, dec=dec*u.degree, frame='icrs')\n",
    "\n",
    "neighbor_indexes, d2d, d3d = coord.match_coordinates_sky(BGS_catalog, sdss_catalog, storekdtree='sdss')\n",
    "ang_distances = d2d.to(u.arcsec).value\n",
    "\n",
    "match_found_filter = ang_distances < 3.0\n",
    "bgs_matches = dn4000[match_found_filter]\n",
    "sdss_indexes = neighbor_indexes[match_found_filter]\n",
    "sdss_matches = sdss.iloc[sdss_indexes].Dn4000.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{np.isclose(bgs_matches, sdss_matches, atol=0.05).sum() / len(bgs_matches)} of the matches are within 0.05 of each other.\")\n",
    "print(f\"{np.isclose(bgs_matches, sdss_matches, atol=0.1).sum() / len(bgs_matches)} of the matches are within 0.1 of each other.\")\n",
    "print(f\"{np.isclose(bgs_matches, sdss_matches, atol=0.2).sum() / len(bgs_matches)} of the matches are within 0.2 of each other.\")\n",
    "print(f\"{np.isclose(bgs_matches, sdss_matches, atol=0.3).sum() / len(bgs_matches)} of the matches are within 0.3 of each other.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=make_map(ra, dec)\n",
    "fig=make_map(sdss.RA.to_numpy(), sdss.Dec.to_numpy(), fig=fig, alpha=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(sdss_matches, bgs_matches, s=1, alpha=.2)\n",
    "plt.xlabel(\"SDSS Dn4000\")\n",
    "plt.ylabel(\"BGS Dn4000\")\n",
    "plt.xlim(1, 2.3)\n",
    "plt.ylim(1, 2.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'SDSS_Dn4000': sdss_matches, 'BGS_Dn4000': bgs_matches})\n",
    "df['diff_frac'] =  (df['BGS_Dn4000'] - df['SDSS_Dn4000']) / df['SDSS_Dn4000']\n",
    "bins = np.linspace(-1, 5, 60)\n",
    "labels = bins[0:len(bins)-1] \n",
    "df['dn4000_sdssbin'] = pd.cut(x = sdss_matches, bins = bins, labels = labels, include_lowest = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=80)\n",
    "diff_mean = df.groupby('dn4000_sdssbin').diff_frac.mean()\n",
    "diff_std= df.groupby('dn4000_sdssbin').diff_frac.std()\n",
    "\n",
    "plt.errorbar(labels, diff_mean, yerr=diff_std)\n",
    "plt.xlabel(\"SDSS Dn4000\")\n",
    "plt.ylabel(\"< (BGS-SDSS) / SDSS >\")\n",
    "plt.xlim(0.8, 2.4)\n",
    "plt.ylim(-0.75, 0.75)\n",
    "plt.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dn4000 Lgal Bin Analysis\n",
    "\n",
    "Run Color Analysis and Dn4000 Comparison first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a plot of Dn4000 in each logLgal bin\n",
    "fig,axes=plt.subplots(dpi=80, figsize=(10, 3*len(BGS_LOGLGAL_BINS)//2), ncols=2, nrows=len(BGS_LOGLGAL_BINS)//2)\n",
    "axes = np.ravel(axes)\n",
    "\n",
    "for i in range(0, len(BGS_LOGLGAL_BINS)-1):\n",
    "    galaxy_idx_for_this_bin = logLgal_bin_idx == i+1\n",
    "\n",
    "    junk=axes[i].hist(dn4000[galaxy_idx_for_this_bin], bins=np.arange(1,2.2,0.02), label=f\"Dn4000 for logLgal Bin {i+1}\", align='mid')\n",
    "    axes[i].legend()\n",
    "    axes[i].set_xlim(1, 2.2)\n",
    "    axes[i].set_xticks(np.arange(1, 2.2, 0.1))\n",
    "\n",
    "    # draw a vertical line at get_SDSS_Dcrit(logLgal)\n",
    "    axes[i].axvline(x=get_SDSS_Dcrit(BGS_LOGLGAL_BINS[i]), color='r', linestyle='-')\n",
    "\n",
    "axes = np.reshape(axes, (2, len(BGS_LOGLGAL_BINS)//2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Randoms Analysis for Footprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOMS_DENSITY = 2500 # per square degree, Ashley Ross paper on LSS pipeline or elsewhere in docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "rtable = Table.read(BGS_Y3_RAND_FILE, format='fits')\n",
    "rtable.columns\n",
    "rtable.keep_columns(['LOCATION', 'FIBER', 'TARGETID', 'RA', 'DEC', 'PRIORITY', 'TILEID', 'TILELOCID', 'NTILE', 'TILES'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_dec = rtable['DEC'].astype(\"<f8\")\n",
    "r_ra = rtable['RA'].astype(\"<f8\")\n",
    "r_ntiles = rtable['NTILE'].astype(\"<i8\")\n",
    "r_tileid = rtable['TILEID'].astype(\"<i8\")\n",
    "\n",
    "# TODO TILEID is just one, TILES has them all... look at rtable['TILES'][135000] for example\n",
    "randoms_df = pd.DataFrame({'RA': r_ra, 'Dec': r_dec, 'NTILE': r_ntiles, 'TILEID': r_tileid})\n",
    "\n",
    "\n",
    "onepass_footprint = len(r_dec) / RANDOMS_DENSITY # in degrees squared\n",
    "onepass_frac_area = onepass_footprint / DEGREES_ON_SPHERE\n",
    "\n",
    "three_pass_filter = r_ntiles >= 3 # 3pass coverage\n",
    "r_dec3 = r_dec[three_pass_filter]\n",
    "r_ra3 = r_ra[three_pass_filter]\n",
    "\n",
    "threepass_footprint = len(r_dec3) / RANDOMS_DENSITY # in degrees squared\n",
    "threepass_frac_area = threepass_footprint / DEGREES_ON_SPHERE\n",
    "\n",
    "# My estimation procedure, which we don't use\n",
    "estimate = estimate_frac_area(r_ra, r_dec)\n",
    "estimate3 = estimate_frac_area(r_ra3, r_dec3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"INCORRECT RESULTS THAT USE NTILE, NOT NTILE_MINE\")\n",
    "print(f\"BGS 1pass Footprint calculated from randoms is {onepass_footprint} square degrees or frac_area={onepass_frac_area}\")\n",
    "print(f\"BGS 3pass Footprint calculated from randoms is {threepass_footprint} square degrees or frac_area={threepass_frac_area}\")\n",
    "#print(f\"BGS 1pass Footprint estimated from my algorithm: frac_area={estimate}\")\n",
    "#print(f\"BGS 3pass Footprint estimated from my algorithm: frac_area={estimate3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make map showing why we cannot use NTILE >= 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make maps\n",
    "#tiles_BGS = read_tiles_Y1_file()\n",
    "tiles_BGS = read_tiles_Y3_main()\n",
    "#tiles_BGS = tiles_BGS.loc[tiles_BGS.FAFLAVOR == 'sv3bright']\n",
    "\n",
    "# Plot the galaxy positions (randoms) and return the set of tile_id associated with them\n",
    "plot_positions(randoms_df, randoms_df[randoms_df.NTILE >= 3], tiles_df=tiles_BGS, DEG_LONG=10, ra_min=180, dec_min=-5, split=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make map with NTILE_MINE\n",
    "\n",
    "Uses a NN lookup of the tiles for each 'galaxy'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD FROM LAST TIME WE DID THIS\n",
    "randoms_df = pickle.load(open(BIN_FOLDER + \"randoms_df.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO MAKE IT ANEW\n",
    "NTILE_MIN = 10\n",
    "ntiles_inside, nearest_tile_ids = find_tiles_for_galaxies(tiles_BGS, randoms_df, NTILE_MIN)\n",
    "randoms_df['NTILE_MINE'] = ntiles_inside\n",
    "\n",
    "print(np.sum(ntiles_inside >= 3) / len(ntiles_inside))\n",
    "print(np.sum(ntiles_inside >= 3))\n",
    "\n",
    "# Not sure if I really need the nearest tile IDs for anything\n",
    "pickle.dump(randoms_df, open(OUTPUT_FOLDER + \"randoms_df_y3kibo.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the galaxy positions (randoms) and return the set of tile_id associated with them\n",
    "plot_positions(randoms_df, randoms_df[randoms_df.NTILE_MINE >= 3], tiles_df=tiles_BGS, DEG_LONG=5, split=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recalculate Footprint given this definition\n",
    "for i in range(1, 11):\n",
    "    n_pass_filter = randoms_df.NTILE_MINE >= i\n",
    "    n_pass_footprint = len(randoms_df[n_pass_filter].RA) / RANDOMS_DENSITY # in degrees squared\n",
    "    n_pass_frac_area = n_pass_footprint / DEGREES_ON_SPHERE\n",
    "    print(f\"BGS {i}pass Footprint calculated from randoms is {n_pass_footprint} square degrees or frac_area={n_pass_frac_area}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ian-conda311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
