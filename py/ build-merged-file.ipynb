{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloc import *\n",
    "from bgs_helpers import *\n",
    "from astropy.table import Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates SV3 FUJI merged file (no Y3 supplement yet)\n",
    "create_merged_file(BGS_SV3_ANY_FULL_FILE, IAN_BGS_SV3_MERGED_NOY3_FILE, 'sv3', photoz_wspec=False)\n",
    "\n",
    "# Now remove galaxies in the patches of SV3 that have poor overlap with Y3\n",
    "sv3_table: Table = Table.read(IAN_BGS_SV3_MERGED_NOY3_FILE, format='fits')\n",
    "print(len(sv3_table))\n",
    "\n",
    "sv3_table['region'] = tile_to_region(sv3_table['NEAREST_TILEIDS'][:,0])\n",
    "to_remove = np.isin(sv3_table['region'], sv3_poor_y3overlap)\n",
    "sv3_table.remove_rows(to_remove)\n",
    "print(len(sv3_table))\n",
    "\n",
    "sv3_table.write(IAN_BGS_SV3_MERGED_NOY3_FILE, format='fits', overwrite='True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates Y1 IRON merged file\n",
    "create_merged_file(BGS_Y1_ANY_FULL_FILE, IAN_BGS_Y1_MERGED_FILE, \"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates Y3 LOA merged file\n",
    "create_merged_file(BGS_Y3_ANY_FULL_FILE, IAN_BGS_Y3_MERGED_FILE_LOA, \"3\")\n",
    "\n",
    "# Add columns to allow the possibility to make a Y3 catalog that is cut to SV3 regions.\n",
    "table = Table.read(IAN_BGS_Y3_MERGED_FILE_LOA, format='fits')\n",
    "sv3tiles = read_tiles_Y3_sv3()\n",
    "ntiles_inside, nearest_tile_ids = find_tiles_for_galaxies(sv3tiles, table_to_df(table), 10)\n",
    "if 'NTILE_MINE_SV3' in table.columns:\n",
    "    table.remove_columns(['NTILE_MINE_SV3'])\n",
    "if 'NEAREST_TILEIDS_SV3' in table.columns:\n",
    "    table.remove_columns(['NEAREST_TILEIDS_SV3'])\n",
    "table.add_column(ntiles_inside, name=\"NTILE_MINE_SV3\")\n",
    "table.add_column(nearest_tile_ids, name=\"NEAREST_TILEIDS_SV3\")\n",
    "\n",
    "table.write(IAN_BGS_Y3_MERGED_FILE_LOA, format='fits', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now add Y3 galaxies to supplement the NN catalog, especialy valuable at the edges of the regions\n",
    "# They won't go into the main catalog because their NTILE_MINE is < 10\n",
    "supplement_sv3_merged_file_with_y3(IAN_BGS_SV3_MERGED_NOY3_FILE, IAN_BGS_Y3_MERGED_FILE_LOA, IAN_BGS_SV3_MERGED_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a variant of Y3 Loa cut to SV3 regions, for the Fiber Incompleteness Study\n",
    "\n",
    "# First replace photo-z column with one that has no spec-z 'contamination' \n",
    "table = add_photz_columns(IAN_BGS_Y3_MERGED_FILE_LOA, IAN_PHOT_Z_FILE_NOSPEC)\n",
    "\n",
    "# Remove galaxies that are far away from the SV3 regions\n",
    "keep = table['NTILE_MINE_SV3'] >= 1 # Not going to 10+ here because we want to keep nearby galaxies for the NN catalog\n",
    "\n",
    "print(f\"Y3 Loa table cut from {len(table)} to {keep.sum()} rows for like-SV3 version\")\n",
    "table  = table[keep]\n",
    "\n",
    "table.write(IAN_BGS_Y3_MERGED_FILE_LOA_SV3CUT, format='fits', overwrite=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For updating just the Quiescent column of the merged files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for file in [IAN_BGS_Y1_MERGED_FILE, IAN_BGS_Y3_MERGED_FILE_LOA, IAN_BGS_Y3_MERGED_FILE_LOA_SV3CUT, IAN_BGS_SV3_MERGED_FILE, IAN_BGS_SV3_MERGED_NOY3_FILE]:\n",
    "    table = Table.read(file, format='fits')\n",
    "\n",
    "    # if the table doesn't have HALPHA or SFR, print a warning\n",
    "    if 'HALPHA_EW' not in table.columns or 'SFR' not in table.columns or 'LOGMSTAR' not in table.columns:\n",
    "        print(f\"Warning: {file} does not have HALPHA_EW, SFR, or LOGMSTAR columns\")\n",
    "        halpha = None\n",
    "        ssfr = None\n",
    "    else:\n",
    "        halpha = table['HALPHA_EW']\n",
    "        ssfr = table['SFR'] / np.power(10, table['LOGMSTAR'])\n",
    "\n",
    "    # if the table doesn't have DN4000_MODEL, print a warning\n",
    "    if 'DN4000_MODEL' not in table.columns:\n",
    "        print(f\"Warning: {file} does not have DN4000_MODEL column\")\n",
    "        dn4000 = None\n",
    "    else:\n",
    "        dn4000 = table['DN4000_MODEL']\n",
    "\n",
    "    quiescent = is_quiescent_BGS_dn4000(table['LOG_L_GAL'], dn4000, table['G_R_BEST'])\n",
    "    #x, y, z, zz, quiescent, missing = is_quiescent_BGS_kmeans(table['LOG_L_GAL'], dn4000, halpha, ssfr, table['G_R_BEST'], model=QUIESCENT_MODEL_V2)\n",
    "    table['QUIESCENT'] = quiescent\n",
    "\n",
    "    table.write(file, format='fits', overwrite=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
