{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as c\n",
    "import h5py\n",
    "from astropy.wcs import WCS\n",
    "import astropy.coordinates as coord\n",
    "import astropy.units as u\n",
    "from pyutils import *\n",
    "import types\n",
    "import numpy.ma as ma\n",
    "import sys\n",
    "from random import randint\n",
    "from matplotlib.patches import Circle\n",
    "from ctypes import c_uint64\n",
    "import pickle\n",
    "from astropy.table import Table\n",
    "\n",
    "#ROOT_FOLDER = \"/Volumes/Seagate Backup Plus Drive/galaxy-groups-data/\"\n",
    "#ROOT_FOLDER = \"/mnt/f/galaxy-groups-data/\"\n",
    "ROOT_FOLDER = \"bin/\"\n",
    "BIG_FILES_FOLDER=\"/export/sirocco2/tinker/DESI/MXXL_MOCKS/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic read-in of HDF5 data from MXXL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_CUT_INDEX = 300000 #21201544 #3000000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = h5py.File(BIG_FILES_FOLDER + 'weights_3pass.hdf5', 'r')\n",
    "print(list(weights))\n",
    "print(list(weights['Data']))\n",
    "print(list(weights['Weight']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common PLT helpers\n",
    "prop_cycle = plt.rcParams['axes.prop_cycle']\n",
    "colors = prop_cycle.by_key()['color']\n",
    "def get_color(i):\n",
    "    co = colors[i%len(colors)]\n",
    "    return co\n",
    "\n",
    "DPI = 1200\n",
    "\n",
    "\n",
    "# Shared bins for various purposes\n",
    "Mhalo_bins = np.logspace(10, 15.5, 40)\n",
    "Mhalo_labels = Mhalo_bins[0:len(Mhalo_bins)-1] \n",
    "\n",
    "L_gal_bins = np.logspace(6, 12.5, 40)\n",
    "L_gal_labels = L_gal_bins[0:len(L_gal_bins)-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments on MXXL Data Directly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple plots of basic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_gal_type = weights['Data/galaxy_type'][0:DATA_CUT_INDEX] # 0 1 2 3 possible\n",
    "bins = plt.hist(small_gal_type, bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_z_obs = weights['Data/z_obs'][0:DATA_CUT_INDEX]\n",
    "bins = plt.hist(small_z_obs, bins=50)\n",
    "plt.xlabel(\"$z_{obs}$\")\n",
    "plt.title(\"Histogram of Observed Redshifts\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ra = weights['Data/ra'][0:DATA_CUT_INDEX]\n",
    "dec = weights['Data/dec'][0:DATA_CUT_INDEX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a map of the galaxies\n",
    "\n",
    "ra_angles = coord.Angle(ra*u.degree)\n",
    "ra_angles = ra_angles.wrap_at(180*u.degree)\n",
    "dec_angles = coord.Angle(dec*u.degree)\n",
    "\n",
    "fig = plt.figure(figsize=(12,9))\n",
    "ax = fig.add_subplot(111, projection=\"mollweide\")\n",
    "ax.scatter(ra_angles.radian, dec_angles.radian, alpha=0.002)\n",
    "# This looks like Alex' paper, good\n",
    "# TODO how to get frac_area from this?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mxxl_halo_id = weights['Data/mxxl_id'][0:DATA_CUT_INDEX]\n",
    "np.sum(mxxl_halo_id == 0) / len(mxxl_halo_id)\n",
    "# TODO why do 2.5% of galaxies have 0 for the MXXL Halo ID? This may be messing us up\n",
    "\n",
    "weird_indexes = np.argwhere(np.invert(mxxl_halo_id.astype(bool)))\n",
    "weird_types = small_gal_type[weird_indexes]\n",
    "trash = plt.hist(weird_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_app_mag = weights['Data/app_mag'][0:DATA_CUT_INDEX]\n",
    "bins = plt.hist(small_app_mag, bins=50)\n",
    "plt.xlabel(\"Apparent Mag\")\n",
    "plt.title(\"Histogram of Apparent Mags\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_abs_mag = weights['Data/abs_mag'][0:DATA_CUT_INDEX]\n",
    "small_colours = weights['Data/g_r'][0:DATA_CUT_INDEX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating luminosity distances from the cosmology is a bit slow\n",
    "my_abs_mag = app_mag_to_abs_mag(small_app_mag, small_z_obs)\n",
    "#my_abs_mag_k = app_mag_to_abs_mag_k(small_app_mag, small_z_obs, small_colours)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare my_abs_mag to abs_mag. \n",
    "x = plt.hist(my_abs_mag, label=\"my abs_mag\", bins=50)\n",
    "y = plt.hist(small_abs_mag, label=\"alex abs_mag\", bins=50)\n",
    "#z = plt.hist(my_abs_mag_k, label=\"my k abs_mag\", bins=50)\n",
    "plt.xlabel(\"Absolute Mag\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Compare Absolute Mags\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At what distance (luminosity distance) would the objects appear to be 19.5 mag?\n",
    "v_max = get_max_observable_volume(my_abs_mag, small_z_obs, 19.5)\n",
    "v_max2 = get_max_observable_volume(small_abs_mag, small_z_obs, 19.5)\n",
    "\n",
    "bins = plt.hist(np.log10(v_max), label=\"my abs_mag\", bins=50)\n",
    "bins = plt.hist(np.log10(v_max2), label=\"alex abs_mag\", bins=50)\n",
    "plt.title(\"Compare V_max\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"log(V_max) [Mpc]\")\n",
    "plt.ylabel(\"Count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a reasonable z fudge factor for 'close enough' redshifts given galaxies $v_{\\mathrm{pec}}$?\n",
    "\n",
    "Galaxies move at hundreds of km/s usually, or thousands in a rich cluster.\n",
    "\n",
    "Two galaxies moving at 600 km/s towards each other along LOS but at same cosmological redshift would have a total redshift difference of 0.004. This suggests a z +/- 0.002 is totally reasonable. In richer areas this could be as high as z +/- 0.010. \n",
    "\n",
    "Adopting z +/- 0.003 for now seems fine. Can refine later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is a reasonable z +/- fudge factor for 'close enough' redshifts? \n",
    "# Consider peculiar velocities.\n",
    "z_test = [0.001, 0.002, 0.003, 0.005, 0.01] * u.dimensionless_unscaled\n",
    "v_pec = z_test.to(u.km / u.s, u.equivalencies.doppler_redshift())\n",
    "for i in range(len(z_test)):\n",
    "    print(f\"z={z_test[i]:.3f} is {v_pec[i]:.0f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Truth Abs Mag for Correcting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_mag = weights['Data/app_mag'][:]\n",
    "z_obs = weights['Data/z_obs'][:]\n",
    "APP_MAG_CUT = 19.5\n",
    "bright_filter = app_mag < APP_MAG_CUT \n",
    "redshift_filter = z_obs > 0 \n",
    "keep = np.all([bright_filter, redshift_filter], axis=0)\n",
    "\n",
    "app_mag = app_mag[keep]\n",
    "z_obs = z_obs[keep]\n",
    "\n",
    "my_abs_mag = app_mag_to_abs_mag(app_mag, z_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(min(my_abs_mag), max(my_abs_mag), 100)\n",
    "densities, bins  = np.histogram(my_abs_mag, bins=bins, density=True)\n",
    "t = plt.hist(my_abs_mag, bins, density=True)\n",
    "\n",
    "with open('bin/abs_mag_weight.npy', 'wb') as f:\n",
    "    np.save(f, densities, allow_pickle=False)\n",
    "    np.save(f, bins, allow_pickle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bin/abs_mag_weight.npy', 'rb') as f:\n",
    "    densities = np.load(f)\n",
    "    bins = np.load(f)\n",
    "\n",
    "plt.plot(bins[0:99], densities)\n",
    "#plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine map of apparent mag to z distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds a map of apparent mags to a pdf of redshifts.plt\n",
    "# Build the map all the way to 20th mag\n",
    "app_mag = weights['Data/app_mag'][:]\n",
    "z_obs = weights['Data/z_obs'][:]\n",
    "APP_MAG_CUT = 20.0\n",
    "bright_filter = app_mag < APP_MAG_CUT \n",
    "redshift_filter = z_obs > 0 \n",
    "keep = np.all([bright_filter, redshift_filter], axis=0)\n",
    "app_mag = app_mag[keep]\n",
    "z_obs = z_obs[keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_mag_bins, the_map = build_app_mag_to_z_map(app_mag, z_obs)\n",
    "\n",
    "counts, app_mag_bins_2  = np.histogram(app_mag, bins=app_mag_bins, density=False)\n",
    "plt.figure()\n",
    "t = plt.hist(app_mag, app_mag_bins, density=False)\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.figure()\n",
    "trash=plt.hist(the_map[0],bins=30, density=True)\n",
    "trash=plt.hist(the_map[50],bins=30, density=True)\n",
    "trash=plt.hist(the_map[100],bins=30, density=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Density of Galaxies per square degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_mag = weights['Data/app_mag'][:]\n",
    "print(f\"There are ~{np.sum(app_mag < 19.5) / 14000:.0f} galaxies/deg^2 < 19.5 mag\")\n",
    "print(f\"There are ~{np.sum(np.all([app_mag > 19.5, app_mag < 20.0], axis=0)) / 14000:.0f} galaxies/deg^2 between 19.5 and 20.0 mag\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nearest Neighbor Angular Separation and Same-Halo Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = weights\n",
    "dec = input['Data/dec'][:]\n",
    "ra = input['Data/ra'][:]\n",
    "z_obs = input['Data/z_obs'][:]\n",
    "app_mag = input['Data/app_mag'][:]\n",
    "\n",
    "APP_MAG_CUT = 20.0\n",
    "bright_filter = app_mag < APP_MAG_CUT # makes a filter array (True/False values)\n",
    "redshift_filter = z_obs > 0 # makes a filter array (True/False values)\n",
    "#location_filter_1 = ra < 270.0\n",
    "#location_filter_2 = ra > 120.0\n",
    "#location_filter_3 = dec > 0.0\n",
    "#location_filter_4 = dec < 45.0\n",
    "keep = np.all([bright_filter, redshift_filter], axis=0)\n",
    "#keep = np.all([bright_filter, redshift_filter, location_filter_1, location_filter_2, location_filter_3, location_filter_4], axis=0)\n",
    "\n",
    "dec = dec[keep]\n",
    "ra = ra[keep]\n",
    "z_obs = z_obs[keep]\n",
    "app_mag = app_mag[keep]\n",
    "sim_halo_id = input['Data/mxxl_id'][:]\n",
    "sim_halo_id = sim_halo_id[keep]\n",
    "\n",
    "\n",
    "len(dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BIT_CHOICE = 0\n",
    "FIBER_ASSIGNED_SELECTOR = 2**BIT_CHOICE\n",
    "fassigned = (input['Weight/bitweight0'][:] & FIBER_ASSIGNED_SELECTOR).astype(bool) # choose 1 of the 2048 fiber assignment realizations with this bitstring\n",
    "fnotassigned = np.invert(fassigned)\n",
    "\n",
    "fassigned = fassigned[keep]\n",
    "fnotassigned = fnotassigned[keep]\n",
    "indexes_not_assigned = np.argwhere(fnotassigned)\n",
    "\n",
    "print(np.sum(fassigned) / len(dec))\n",
    "\n",
    "with open('bin/prob_obs.npy', 'rb') as f:\n",
    "    prob_obs = np.load(f)\n",
    "prob_obs_cut = prob_obs[keep]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate P_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bitsum(bitstring):\n",
    "    return bin(c_uint64(bitstring).value).count(\"1\")\n",
    "v_bitsum = np.vectorize(bitsum)\n",
    "\n",
    "def summate(a):\n",
    "    return np.sum(v_bitsum(a))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip this if iips were loaded OK. Takes ~8 minutes.\n",
    "\n",
    "# Read all 32 64-bitstrings into memory from the file\n",
    "num_bitstrings = 32\n",
    "galaxy_count = len(input['Weight/bitweight0'])\n",
    "bitweights = np.empty((num_bitstrings, galaxy_count), dtype='i8')\n",
    "\n",
    "for i in range(num_bitstrings):\n",
    "    bitweights[i] = input['Weight/bitweight{0}'.format(i)][:]\n",
    "    \n",
    "prob_obs = np.apply_along_axis(summate, 0, bitweights) / 2048\n",
    "\n",
    "with open('bin/prob_obs.npy', 'wb') as f:\n",
    "    np.save(f, prob_obs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specimen = 123\n",
    "bit_selector = c_uint64(2).value\n",
    "print('{:064b}'.format(bit_selector))\n",
    "print('')\n",
    "for i in range(num_bitstrings):\n",
    "    value = bitweights[(i,specimen)]\n",
    "    converted = c_uint64(value).value\n",
    "    print('{:064b}'.format(converted), '{:2.0f}'.format(bitsum(value)), bool(converted & bit_selector))\n",
    "\n",
    "print(\"Averaged Probability of being targetted: \", prob_obs[specimen])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_obs_cut = prob_obs[keep]\n",
    "\n",
    "pobs_bins_temp = np.linspace(0,1)\n",
    "trash=plt.hist(prob_obs, bins=pobs_bins_temp, label=\"All galaxies\")\n",
    "trash2=plt.hist(prob_obs_cut, bins=pobs_bins_temp, label=f\"Galaxies below {APP_MAG_CUT} mag\")\n",
    "plt.yscale('log')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_obs_dim = prob_obs[np.invert(keep)]\n",
    "trash=plt.hist(prob_obs_dim, bins=pobs_bins_temp, alpha=0.5, label=f\"Galaxies above {APP_MAG_CUT} mag\")\n",
    "trash2=plt.hist(prob_obs_cut, bins=pobs_bins_temp, alpha=0.5, label=f\"Galaxies below {APP_MAG_CUT} mag\")\n",
    "plt.yscale('log')\n",
    "plt.xlabel('$P_{obs}$')\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Same Halo / Similar z Analysis\n",
    "\n",
    "What fraction of time nearest neighbors in same halo? \n",
    "\n",
    "What is the distribution of Angular distances?\n",
    "\n",
    "What fraction of time is nearest neighbors at a similar enough redshift?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now bin so that things with ang distances higher than the max we care about are thrown out\n",
    "BIN_COUNT = 25\n",
    "bins = np.logspace(np.log10(3), np.log10(60*60), BIN_COUNT)\n",
    "print(\"Angular Distance Bin Markers\", bins)\n",
    "\n",
    "z_bins = [0.1, 0.2, 0.3, 1.0] # nothing can be higher than rightmost bin value\n",
    "z_bins = SimpleRedshiftGuesser.z_bins\n",
    "print(\"Redshift Bin Markers\", z_bins)\n",
    "\n",
    "POBS_BIN_COUNT = 25\n",
    "POBS_bins = np.linspace(0.01, 1.0, POBS_BIN_COUNT)\n",
    "print(\"Pobs Bin Markers\", POBS_bins)\n",
    "\n",
    "LOST_GALAXIES_ONLY = True\n",
    "\n",
    "if LOST_GALAXIES_ONLY:\n",
    "    treename = 'mxxl_same_halo_analysis_fiberassigned_b' + str(BIT_CHOICE)\n",
    "    catalog = coord.SkyCoord(ra=ra[fassigned]*u.degree, dec=dec[fassigned]*u.degree, frame='icrs')\n",
    "    sim_halo_id_catalog = sim_halo_id[fassigned]\n",
    "    z_obs_catalog = z_obs[fassigned]\n",
    "else:\n",
    "    treename = 'mxxl_same_halo_analysis_all'\n",
    "    catalog = coord.SkyCoord(ra=ra*u.degree, dec=dec*u.degree, frame='icrs')\n",
    "    sim_halo_id_catalog = sim_halo_id\n",
    "    z_obs_catalog = z_obs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get NN's angular distance distribution and same halo truth from MXXL\n",
    "\n",
    "# Though this is binned by z of the target and not the NN, it shouldn't be able to affect results\n",
    "# by recipricality of NN\n",
    "z_bin = np.digitize(z_obs, z_bins)\n",
    "\n",
    "if LOST_GALAXIES_ONLY:\n",
    "    nn_bins = np.arange(5)+1\n",
    "else:\n",
    "    nn_bins = [2]#[2,3,4] # this means closest 3. '1' will find the same object.\n",
    "\n",
    "all_ang_bincounts = np.ones((len(z_bins), len(nn_bins), len(bins)))\n",
    "all_same_halo_bincounts = np.zeros((len(z_bins), len(nn_bins), len(bins)))\n",
    "all_same_z_bincounts = np.zeros((len(z_bins), len(nn_bins), len(bins)))\n",
    "all_sim_z_bincounts = np.zeros((len(z_bins), len(nn_bins), len(bins)))\n",
    "\n",
    "for i in range(len(z_bins)):\n",
    "    for j in range(len(nn_bins)):\n",
    "        if LOST_GALAXIES_ONLY:\n",
    "            filter = np.all([z_bin == i, fnotassigned], axis=0)\n",
    "        else:\n",
    "            filter = z_bin == i\n",
    "        to_match = coord.SkyCoord(ra=ra[filter]*u.degree, dec=dec[filter]*u.degree, frame='icrs')\n",
    "        idx, d2d, d3d = coord.match_coordinates_sky(to_match, catalog, nthneighbor=nn_bins[j], storekdtree=treename)\n",
    "\n",
    "        same_halo = sim_halo_id[filter] == sim_halo_id_catalog[idx]\n",
    "        same_z = np.isclose(z_obs[filter], z_obs_catalog[idx], rtol=0, atol=0.000001)\n",
    "        sim_z = np.isclose(z_obs[filter], z_obs_catalog[idx], rtol=0, atol=SIM_Z_THRESH)\n",
    "        \n",
    "        angdist_bin_ind = np.digitize(d2d.to(u.arcsec).value, bins)\n",
    "        \n",
    "        bincounts = np.bincount(angdist_bin_ind, minlength=len(bins)) + 1 # avoids divide by 0, won't hurt statistics\n",
    "        all_ang_bincounts[i][j] = bincounts\n",
    "\n",
    "        bincounts2 = np.bincount(angdist_bin_ind, minlength=len(bins), weights=same_halo.astype(int))\n",
    "        all_same_halo_bincounts[i][j] = bincounts2\n",
    "\n",
    "        bincount3 = np.bincount(angdist_bin_ind, minlength=len(bins), weights=same_z.astype(int))\n",
    "        all_same_z_bincounts[i][j] = bincount3\n",
    "\n",
    "        bincount4 = np.bincount(angdist_bin_ind, minlength=len(bins), weights=sim_z.astype(int))\n",
    "        all_sim_z_bincounts[i][j] = bincount4\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Galaxies studied: {0}. Same halo: {1}. Similar z: {2}\".format(np.sum(all_ang_bincounts), np.sum(all_same_halo_bincounts), np.sum(all_sim_z_bincounts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO \n",
    "#for b in range(len(all_same_halo_bincounts)):\n",
    "#    print(all_same_halo_bincounts[b], all_same_z_bincounts[b], len(all_same_z_bincounts))\n",
    "\n",
    "np.all(np.isclose(all_same_halo_bincounts, all_same_z_bincounts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getlabel(index, z_bins):\n",
    "    if index==0:\n",
    "        label = \"< {0}\".format(z_bins[index])\n",
    "    else:\n",
    "        label = \"{0} - {1}\".format(z_bins[index-1], z_bins[index])\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots for nearest-neighbor angular distances and same-halo analysis\n",
    "\n",
    "for j in range(len(nn_bins)):\n",
    "    if j < 5:\n",
    "        plt.figure()\n",
    "        for i in range(len(z_bins)):\n",
    "            label = getlabel(i, z_bins)\n",
    "        plt.plot(bins, all_ang_bincounts[i][j], label=label, color=get_color(i))\n",
    "\n",
    "        plt.title(f\"Nearest Neighbor {j} Ang. Distance Distribution\")\n",
    "        plt.ylabel(\"Count\")\n",
    "        plt.xlabel(\"Angular Distance (arcsec)\")\n",
    "        plt.yscale('log')\n",
    "        plt.xscale('log')\n",
    "        plt.legend()\n",
    "        plt.draw()\n",
    "\n",
    "        plt.figure()\n",
    "        for i in range(len(z_bins)):\n",
    "            label = getlabel(i, z_bins)\n",
    "            plt.plot(bins, all_same_halo_bincounts[i][j]/all_ang_bincounts[i][j], label=label, color=get_color(i))\n",
    "            print(\"Total fraction of nearest neighbors in same halo (z {0}, NN-{1}): {2:.3f}\".format(label, j+1, np.sum(all_same_halo_bincounts[i][j]) / np.sum(all_ang_bincounts[i][j])))\n",
    "\n",
    "        plt.title(f\"Nearest Neighbor {j} Same Halo Fraction\")\n",
    "        plt.ylabel(\"NN Same Halo Fraction\")\n",
    "        plt.xlabel(\"Angular Distance (arcsec)\")\n",
    "        plt.xscale('log')\n",
    "        plt.legend()\n",
    "        plt.draw()\n",
    "\n",
    "        plt.figure()\n",
    "        for i in range(len(z_bins)):\n",
    "            label = getlabel(i, z_bins)\n",
    "            plt.plot(bins, all_sim_z_bincounts[i][j]/all_ang_bincounts[i][j], label=label, color=get_color(i))\n",
    "            \n",
    "            print(\"Total fraction of nearest neighbors at sim z (z {0}, NN-{1}): {2:.3f}\".format(label, j+1, np.sum(all_sim_z_bincounts[i][j]) / np.sum(all_ang_bincounts[i][j])))\n",
    "\n",
    "        plt.title(f\"Nearest Neighbor {j} Sim z Fraction\")\n",
    "        plt.ylabel(\"NN Sim z Fraction\")\n",
    "        plt.xlabel(\"Angular Distance (arcsec)\")\n",
    "        plt.xscale('log')\n",
    "        plt.legend()\n",
    "        plt.draw()\n",
    "\n",
    "#print(\"What fraction of the time is the NN >19.5 mag?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Color plots of NN Same Halo in z / ang distance / P_obs space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POBS_bin = np.digitize(prob_obs_cut, POBS_bins)\n",
    "\n",
    "if LOST_GALAXIES_ONLY: \n",
    "    nn_bins = [1]\n",
    "else:\n",
    "    nn_bins = [2] # since catalog includes the targets in this case\n",
    "\n",
    "all_ang_bincounts_2 = np.ones((POBS_BIN_COUNT, len(nn_bins), len(z_bins), BIN_COUNT))\n",
    "all_same_halo_bincounts_2 = np.zeros((POBS_BIN_COUNT, len(nn_bins), len(z_bins), BIN_COUNT))\n",
    "all_sim_z_bincounts_2 = np.zeros((POBS_BIN_COUNT, len(nn_bins), len(z_bins), BIN_COUNT))\n",
    "\n",
    "for i in range(len(POBS_bins)):\n",
    "    for j in range(len(nn_bins)):\n",
    "        if LOST_GALAXIES_ONLY:\n",
    "            filter = np.all([POBS_bin == i, fnotassigned], axis=0)\n",
    "        else:\n",
    "            filter = POBS_bin == i\n",
    "        to_match = coord.SkyCoord(ra=ra[filter]*u.degree, dec=dec[filter]*u.degree, frame='icrs')\n",
    "        idx, d2d, d3d = coord.match_coordinates_sky(to_match, catalog, nthneighbor=nn_bins[j], storekdtree=treename)\n",
    "        same_halo = sim_halo_id[filter] == sim_halo_id_catalog[idx]\n",
    "        sim_z = np.isclose(z_obs[filter], z_obs_catalog[idx], rtol=0, atol=SIM_Z_THRESH)\n",
    "\n",
    "        nn_z_bin_ind = np.digitize(z_obs_catalog[idx], z_bins)\n",
    "        angdist_bin_ind = np.digitize(d2d.to(u.arcsec).value, bins)\n",
    "        \n",
    "        for zb in range(len(z_bins)):\n",
    "            right_z_bin = nn_z_bin_ind == zb\n",
    "            bincounts = np.bincount(angdist_bin_ind, minlength=len(bins), weights=right_z_bin.astype(int)) + 1 # avoids divide by 0, won't hurt statistics\n",
    "            all_ang_bincounts_2[i][j][zb] = bincounts\n",
    "\n",
    "            bincounts2 = np.bincount(angdist_bin_ind, minlength=len(bins), weights=np.all([same_halo, right_z_bin], axis=0).astype(int))\n",
    "            all_same_halo_bincounts_2[i][j][zb] = bincounts2\n",
    "\n",
    "            bincounts3 = np.bincount(angdist_bin_ind, minlength=len(bins), weights=np.all([sim_z, right_z_bin], axis=0).astype(int))\n",
    "            all_sim_z_bincounts_2[i][j][zb] = bincounts3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frac_same = all_same_halo_bincounts_2 / all_ang_bincounts_2\n",
    "frac_sim_z = all_sim_z_bincounts_2 / all_ang_bincounts_2\n",
    "success_bins = [0,0.3,1.0]\n",
    "frac_same_binned = np.digitize(frac_same, bins=success_bins)\n",
    "#for i in range(len(frac_same_binned)):\n",
    "#    frac_same_binned[i] = success_bins[frac_same_binned[i-1]]\n",
    "frac_same_over50 = (frac_same > 0.5).astype(int)\n",
    "frac_at50 = close_enough(frac_same, 0.5, threshold=0.05).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=len(z_bins), ncols=4, figsize=(24, 4*len(z_bins)))\n",
    "\n",
    "for zb in range(len(z_bins)):\n",
    "    density = all_ang_bincounts_2[:,0,zb,:] #/ np.sum(all_ang_bincounts_2[:,0,zb,:])\n",
    "    print(f\"Galaxies in this z-bin: {np.sum(density)}\")\n",
    "\n",
    "    cplot = axes[zb][0].pcolor(bins, POBS_bins, frac_same_binned[:,0,zb,:], shading='auto', cmap='RdYlGn')\n",
    "    #cplot = axes[zb][0].pcolor(bins, POBS_bins, frac_same_over50[:,0,zb,:], shading='auto', cmap='RdYlGn', norm=c.Normalize(vmin=0, vmax=0.8))\n",
    "    fig.colorbar(cplot, ax=axes[zb][0])\n",
    "    axes[zb][0].set_title(f\"Nearest Neighbor Same Halo Over 40% (NN z {getlabel(zb, z_bins)})\")\n",
    "    axes[zb][0].set_ylabel(\"Lost Galaxy $P_{obs}$\")\n",
    "    axes[zb][0].set_xlabel(\"Angular Distance (arcsec) to NN\")\n",
    "    axes[zb][0].set_xscale('log')\n",
    "    axes[zb][0].set_xlim(3,250)\n",
    "\n",
    "    cplot = axes[zb][1].pcolor(bins, POBS_bins, frac_same[:,0,zb,:], shading='auto', cmap='RdYlGn', norm=c.Normalize(vmin=0, vmax=0.8))\n",
    "    fig.colorbar(cplot, ax=axes[zb][1])\n",
    "    axes[zb][1].set_title(f\"Nearest Neighbor Same Halo (NN z {getlabel(zb, z_bins)})\")\n",
    "    axes[zb][1].set_ylabel(\"Lost Galaxy $P_{obs}$\")\n",
    "    axes[zb][1].set_xlabel(\"Angular Distance (arcsec) to NN\")\n",
    "    axes[zb][1].set_xscale('log')\n",
    "\n",
    "    cplot = axes[zb][2].pcolor(bins, POBS_bins, frac_sim_z[:,0,zb,:], shading='auto', cmap='RdYlGn', norm=c.Normalize(vmin=0, vmax=0.8))\n",
    "    fig.colorbar(cplot, ax=axes[zb][2])\n",
    "    axes[zb][2].set_title(f\"Nearest Neighbor Sim z Fraction (NN z {getlabel(zb, z_bins)})\")\n",
    "    axes[zb][2].set_ylabel(\"Lost Galaxy $P_{obs}$\")\n",
    "    axes[zb][2].set_xlabel(\"Angular Distance (arcsec) to NN\")\n",
    "    axes[zb][2].set_xscale('log')\n",
    "\n",
    "    #cplot = axes[zb][3].pcolor(bins, POBS_bins, density, shading='auto', cmap='YlGn', norm=c.LogNorm(vmin=0.0001, vmax=0.1))\n",
    "    cplot = axes[zb][3].pcolor(bins, POBS_bins, density, shading='auto', cmap='YlGn', norm=c.LogNorm(vmin=10, vmax=5000))\n",
    "    fig.colorbar(cplot, ax=axes[zb][3])\n",
    "    axes[zb][3].set_title(f\"Counts (NN z {getlabel(zb, z_bins)})\")\n",
    "    axes[zb][3].set_ylabel(\"Lost Galaxy $P_{obs}$\")\n",
    "    axes[zb][3].set_xlabel(\"Angular Distance (arcsec) to NN\")\n",
    "    axes[zb][3].set_xscale('log')\n",
    "\n",
    "    axes[zb][0].scatter(get_NN_30_line(z_bins[zb]-0.01, POBS_bins), POBS_bins)\n",
    "    axes[zb][1].scatter(get_NN_30_line(z_bins[zb]-0.01, POBS_bins), POBS_bins)\n",
    "    axes[zb][2].scatter(get_NN_30_line(z_bins[zb]-0.01, POBS_bins), POBS_bins)\n",
    "    \n",
    "fig.tight_layout() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Using z +/- {0} values as a success metric:\\n \".format(SIM_Z_THRESH))\n",
    "for i in range(BIN_COUNT):\n",
    "    arcsec = bins[i]\n",
    "    tot = np.sum(all_ang_bincounts[:,0,0:i])\n",
    "    frac = np.sum(all_sim_z_bincounts[:,0,0:i]) / np.sum(all_ang_bincounts[:,0,0:i])\n",
    "    frac_assigned = np.sum(all_ang_bincounts[:,0,0:i]) / np.sum(all_ang_bincounts[:,0,:])\n",
    "    print(\"  Up to {0:.1f}\\\": Success frac: {1:.3f}. Assigned frac: {2:.3f}\".format(arcsec, frac, frac_assigned))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine velocities between neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fancy Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_NEIGHBORS = 20\n",
    "fancy_to_match = coord.SkyCoord(ra=ra[fnotassigned]*u.degree, dec=dec[fnotassigned]*u.degree, frame='icrs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbor_indexes = np.zeros(shape=(NUM_NEIGHBORS, len(fancy_to_match)), dtype=np.int32) # indexes point to CATALOG locations\n",
    "ang_distances = np.zeros(shape=(NUM_NEIGHBORS, len(fancy_to_match)))\n",
    "\n",
    "print(f\"Finding nearest {NUM_NEIGHBORS} neighbors... \", end='\\r')   \n",
    "for n in range(0, NUM_NEIGHBORS):\n",
    "    idx, d2d, d3d = coord.match_coordinates_sky(fancy_to_match, catalog, nthneighbor=n+1, storekdtree=treename)\n",
    "    neighbor_indexes[n] = idx # TODO is that right?\n",
    "    ang_distances[n] = d2d.to(u.arcsec).value\n",
    "print(f\"Finding nearest {NUM_NEIGHBORS} neighbors... done!\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with FancyRedshiftGuesser(NUM_NEIGHBORS, debug=False) as scorer:\n",
    "    halo_matches = 0\n",
    "    z_matches = 0\n",
    "\n",
    "    print(f\"Assinging missing redshifts... \")   \n",
    "    # TODO don't loop?\n",
    "    j = 0 # index of the fancy_to_match sized arrays\n",
    "    \n",
    "    #for i in special_id:\n",
    "    for i in indexes_not_assigned: # index of the master arrays\n",
    "\n",
    "        #if i not in [7793057, 11425052]:\n",
    "        #    j+=1\n",
    "        #    continue\n",
    "\n",
    "        if j%10000==0:\n",
    "            print(f\"{j}/{len(fancy_to_match)} complete\", end='\\r')\n",
    "\n",
    "        neighbors = neighbor_indexes[:,j]\n",
    "        neighbors_z = z_obs_catalog[neighbors]\n",
    "        neighbors_ang_dist = ang_distances[:,j]\n",
    "        my_prob_obs = prob_obs_cut[i]\n",
    "        my_app_mag = app_mag[i]\n",
    "\n",
    "        winning_num = scorer.choose_winner(neighbors_z, neighbors_ang_dist, my_prob_obs, my_app_mag, z_obs[i])\n",
    "        winner_index = neighbors[winning_num]\n",
    "\n",
    "        # Track total correct\n",
    "        z_chosen = z_obs_catalog[winner_index] \n",
    "        if np.isclose(z_chosen, z_obs[i], rtol=0, atol=SIM_Z_THRESH):\n",
    "            z_matches += 1\n",
    "        halo_chosen = sim_halo_id_catalog[winner_index]\n",
    "        if halo_chosen == sim_halo_id[i]:\n",
    "            halo_matches += 1\n",
    "\n",
    "        j += 1 \n",
    "\n",
    "    print(f\"{j}/{len(fancy_to_match)} complete\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Halo matches: {halo_matches / len(fancy_to_match)}\")\n",
    "print(f\"z matches: {z_matches / len(fancy_to_match)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View results from a run of the FancyRedshiftGuesser. Must put in the right filename (number)\n",
    "filename = 'bin/redshift_guesser_1691466513.171286.npy'\n",
    "with open(filename, 'rb') as f:\n",
    "    quick_nn = np.load(f)\n",
    "    quick_correct = np.load(f)\n",
    "    nn_used = np.load(f)\n",
    "    nn_correct = np.load(f)\n",
    "\n",
    "print(f\"Quick NN uses: {quick_nn}. Success: {quick_correct / (quick_nn+1)}\")\n",
    "print(f\"NN bin uses: {nn_used}. Success: {nn_correct / (nn_used+1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Galaxy Pairs Angular Separation and Same-Halo Analysis\n",
    "Continuation of the above.\n",
    "\n",
    "THIS IS N^2 CALCULATION do not run on full sky. Adjust data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS IS N^2 CALCULATION do not run on full sky.\n",
    "total_bincounts = np.ones((len(z_bins), BIN_COUNT))\n",
    "total_same_halo_bincounts = np.zeros((len(z_bins), BIN_COUNT))\n",
    "\n",
    "# Examine each galaxy in the sample pair once\n",
    "for i in range(len(ra)-1):\n",
    "    ang_distance = coord.angular_separation(ra[i]*u.degree, dec[i]*u.degree, ra[i+1:len(ra)]*u.degree, dec[i+1:len(ra)]*u.degree).to(u.arcsec)\n",
    "        \n",
    "    same_halo = sim_halo_id[i] == sim_halo_id[i+1:len(ra)]\n",
    "    #print(\"Same halo fraction for {0}:\".format(i), np.sum(same_halo) / len(same_halo))\n",
    "\n",
    "    angdist_bin_ind = np.digitize(ang_distance.value, bins)\n",
    "    #print(bin_ind)\n",
    "    bincounts = np.bincount(angdist_bin_ind)[0:BIN_COUNT]\n",
    "    same_halo_bincounts = np.bincount(angdist_bin_ind, weights= same_halo.astype(int)) [0:BIN_COUNT]\n",
    "\n",
    "    z_bin = np.digitize(z_obs[i], z_bins)\n",
    "    total_bincounts[z_bin] = total_bincounts[z_bin] + bincounts\n",
    "    total_same_halo_bincounts[z_bin] = total_same_halo_bincounts[z_bin] + same_halo_bincounts\n",
    "    #print(total_same_halo_bincounts)\n",
    "\n",
    "#print(\"Total counts in each bin:\", total_bincounts)\n",
    "\n",
    "fraction_same_halo = total_same_halo_bincounts / total_bincounts\n",
    "#print(fraction_same_halo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots for galaxy pairs\n",
    "plt.figure()\n",
    "for i in range(len(z_bins)):\n",
    "    if i==0:\n",
    "        label = \"< {0}\".format(z_bins[i])\n",
    "    else:\n",
    "        label = \"{0} - {1}\".format(z_bins[i-1], z_bins[i])\n",
    "    plt.plot(bins, total_bincounts[i], label=label)\n",
    "plt.legend()\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Angular Separation (arcsec)')\n",
    "plt.ylabel('Count of Galaxies Pairs')\n",
    "plt.title(\"Galaxy Pair Counts (by ang separation and z)\")\n",
    "plt.draw()\n",
    "\n",
    "plt.figure()\n",
    "for i in range(len(z_bins)):\n",
    "    if i==0:\n",
    "        label = \"< {0}\".format(z_bins[i])\n",
    "    else:\n",
    "        label = \"{0} - {1}\".format(z_bins[i-1], z_bins[i])\n",
    "    plt.plot(bins, fraction_same_halo[i], label=label)\n",
    "plt.legend()\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Angular Separation (arcsec)')\n",
    "plt.ylabel('Fraction Pair in Same Halo')\n",
    "plt.ylim(-0.01, 1.0)\n",
    "plt.title(\"Fraction Pair in Same Halo (by ang separation and z)\")\n",
    "plt.draw()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post Group Founder Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(filename):\n",
    "\n",
    "    filename_props = str.replace(filename, \".out\", \"_galprops.dat\")\n",
    "\n",
    "    df = pd.read_csv(filename, delimiter=' ', names=('RA', 'Dec', 'z', 'L_gal', 'V_max', 'P_sat', 'M_halo', 'N_sat', 'L_tot', 'igrp', 'unknown'))\n",
    "    galprops = pd.read_csv(filename_props, delimiter=' ', names=('app_mag', 'g_r', 'galaxy_type', 'mxxl_halo_mass', 'fiber_assigned_0', 'assigned_halo_mass', 'z_obs', 'mxxl_halo_id', 'assigned_halo_id'), dtype={'mxxl_halo_id': np.int32, 'assigned_halo_id': np.int32})\n",
    "    all_data = pd.merge(df, galprops, left_index=True, right_index=True)\n",
    "\n",
    "    # Drop bad data, should have been cleaned up earlier though!\n",
    "    orig_count = len(all_data)\n",
    "    all_data = all_data[all_data.M_halo != 0]\n",
    "    new_count = len(all_data)\n",
    "    if (orig_count != new_count):\n",
    "        print(\"Dropped {0} bad galaxies\".format(orig_count - new_count))\n",
    "\n",
    "    all_data['is_sat'] = (all_data.index != all_data.igrp).astype(int)\n",
    "    all_data['is_sat_truth'] = np.logical_or(all_data.galaxy_type == 1, all_data.galaxy_type == 3).astype(int)\n",
    "    all_data['logLgal'] = np.log10(all_data.L_gal)\n",
    "\n",
    "    #bins = np.logspace(np.log10(min(all_data.M_halo)), np.log10(max(all_data.M_halo)), 30)\n",
    "    #labels = bins[0:len(bins)-1] # using bottom (or top?) value, not middle\n",
    "    all_data['Mh_bin'] = pd.cut(x = all_data['M_halo'], bins = Mhalo_bins, labels = Mhalo_labels, include_lowest = True)\n",
    "    \n",
    "    centrals = all_data[all_data.index == all_data.igrp]\n",
    "    #logmstar_means = centrals.groupby('Mh_bin').log_M_star.mean()\n",
    "    #logmstar_scatter = centrals.groupby('Mh_bin').log_M_star.std()\n",
    "    loglcen_means = centrals.groupby('Mh_bin').logLgal.mean()\n",
    "    loglcen_scatter = centrals.groupby('Mh_bin').logLgal.std()\n",
    "\n",
    "    # Compute f_sat(Lgal)\n",
    "    #L_gal_bins = np.logspace(np.log10(min(all_data.L_gal)), np.log10(max(all_data.L_gal)), 30)\n",
    "    #L_gal_labels = L_gal_bins[0:len(L_gal_bins)-1] # using bottom (or top?) value, not middle\n",
    "    all_data['Lgal_bin'] = pd.cut(x = all_data['L_gal'], bins = L_gal_bins, labels = L_gal_labels, include_lowest = True)\n",
    "    \n",
    "    f_sat = all_data.groupby('Lgal_bin').is_sat.mean()\n",
    "    Lgal_counts = all_data.groupby('Lgal_bin').RA.count()\n",
    "\n",
    "    dataset = types.SimpleNamespace()\n",
    "    dataset.filename = filename[filename.rfind('/')+1 : len(filename)-4]\n",
    "    dataset.all_data = all_data\n",
    "    dataset.Mhalo_bins = Mhalo_bins\n",
    "    dataset.labels = Mhalo_labels\n",
    "    dataset.centrals = centrals\n",
    "    #dataset.logmstar_means = logmstar_means\n",
    "    #dataset.logmstar_scatter = logmstar_scatter\n",
    "    dataset.loglcen_means = loglcen_means\n",
    "    dataset.loglcen_scatter = loglcen_scatter\n",
    "    dataset.L_gal_bins = L_gal_bins\n",
    "    dataset.L_gal_labels = L_gal_labels\n",
    "    dataset.f_sat = f_sat\n",
    "    dataset.Lgal_counts = Lgal_counts\n",
    "\n",
    "    return dataset\n",
    "\n",
    "    \n",
    "\n",
    "def plots(*frames):\n",
    "    contains_20_data = False\n",
    "    for f in frames:\n",
    "        if ('20' in f.name):\n",
    "            contains_20_data = True\n",
    "    \n",
    "    plt.figure(dpi=DPI)\n",
    "    for f in frames:\n",
    "        if ('20' not in f.name):\n",
    "            plt.errorbar(f.labels, f.loglcen_means, yerr=f.loglcen_scatter, label=f.name, color=f.color)\n",
    "    plt.xscale('log')\n",
    "    plt.xlabel('$M_{halo}$')\n",
    "    plt.ylabel('$log(L_{cen})$')\n",
    "    plt.title(\"Central Luminosity vs. Halo Mass\")\n",
    "    plt.legend()\n",
    "    plt.draw()\n",
    "\n",
    "    if contains_20_data:\n",
    "        plt.figure(dpi=DPI)\n",
    "        for f in frames:\n",
    "            if ('20' in f.name):\n",
    "                plt.errorbar(f.labels, f.loglcen_means, yerr=f.loglcen_scatter, label=f.name, color=f.color)\n",
    "        plt.xscale('log')\n",
    "        plt.xlabel('$M_{halo}$')\n",
    "        plt.ylabel('$log(L_{cen})$')\n",
    "        plt.title(\"Central Luminosity vs. Halo Mass\")\n",
    "        plt.legend()\n",
    "        plt.draw()\n",
    "\n",
    "    plt.figure(dpi=DPI)    \n",
    "    for f in frames:\n",
    "        if ('20' not in f.name):\n",
    "            plt.plot(f.labels, f.loglcen_scatter, color=f.color, label=f.name)\n",
    "    plt.xscale('log')\n",
    "    plt.xlabel('$M_{halo}$')\n",
    "    plt.ylabel('$\\\\sigma(\\\\log(L_{cen})$')\n",
    "    plt.title(\"Central Luminosity Scatter vs. Halo Mass\")\n",
    "    plt.legend()\n",
    "    plt.draw()\n",
    "\n",
    "    if contains_20_data:\n",
    "        plt.figure(dpi=DPI)    \n",
    "        for f in frames:\n",
    "            if ('20' in f.name):\n",
    "                plt.plot(f.labels, f.loglcen_scatter, color=f.color, label=f.name)\n",
    "        plt.xscale('log')\n",
    "        plt.xlabel('$M_{halo}$')\n",
    "        plt.ylabel('$\\\\sigma(\\\\log(L_{cen})$')\n",
    "        plt.title(\"Central Luminosity Scatter vs. Halo Mass\")\n",
    "        plt.legend()\n",
    "        plt.draw()\n",
    "\n",
    "    \"\"\"     \n",
    "    plt.figure()\n",
    "    for f in frames:\n",
    "        plt.scatter(f.centrals.M_halo, f.centrals.L_gal, alpha=0.002)\n",
    "    plt.loglog()\n",
    "    plt.xlabel('M_halo / h')\n",
    "    plt.ylabel('L_gal / $h^2$)')\n",
    "    plt.draw() \n",
    "    \"\"\"\n",
    "\n",
    "    if 'N_sat' in f.all_data.columns:\n",
    "        plt.figure(dpi=DPI)\n",
    "        for f in frames:\n",
    "            Nsat_means = f.all_data.groupby('Mh_bin').N_sat.mean()\n",
    "            plt.plot(f.labels, Nsat_means, f.marker, label=f.name, color=f.color)\n",
    "            #plt.hist(f.centrals.N_sat, np.arange(0,50,1), alpha=0.5)\n",
    "        plt.loglog()    \n",
    "        plt.ylabel(\"$<N_{sat}>$\")    \n",
    "        plt.xlabel('$M_{halo}$')\n",
    "        plt.title(\"Mean Number of Satellites by Halo Mass\")\n",
    "        plt.legend()\n",
    "        plt.draw()\n",
    "\n",
    "    plt.figure(dpi=DPI)\n",
    "    for f in frames:\n",
    "        plt.plot(f.L_gal_labels, f.f_sat, f.marker, label=f.name, color=f.color)\n",
    "    #truth_f_sat = frames[0].all_data.groupby('Lgal_bin').is_sat_truth.mean()\n",
    "    #plt.plot(frames[0].L_gal_labels, truth_f_sat, 'k', label=\"MXXL Truth\")\n",
    "    plt.xscale('log')\n",
    "    plt.xlabel(\"$L_{gal}$\")\n",
    "    plt.ylabel(\"$f_{sat}$\")\n",
    "    plt.title(\"Satellite fraction vs Galaxy Luminosity\")\n",
    "    plt.legend()\n",
    "    plt.draw()\n",
    "\n",
    "    fig,ax1=plt.subplots()\n",
    "    fig.set_dpi(DPI)\n",
    "    for f in frames:\n",
    "        plt.plot(f.L_gal_labels, f.f_sat, f.marker, label=f.name, color=f.color)\n",
    "    #truth_f_sat = frames[0].all_data.groupby('Lgal_bin').is_sat_truth.mean()\n",
    "    #ax1.plot(frames[0].L_gal_labels, truth_f_sat, 'k', label=\"MXXL Truth\")\n",
    "    ax1.set_xscale('log')\n",
    "    ax1.set_xlabel(\"$L_{gal}$\")\n",
    "    ax1.set_ylabel(\"$f_{sat}$\")\n",
    "    ax1.set_title(\"Satellite fraction vs Galaxy Luminosity\")\n",
    "    ax1.legend()\n",
    "    ax1.set_xlim(3E8,1E11)\n",
    "    ax1.set_ylim(0.1,0.5)\n",
    "    ax2 = ax1.twinx()\n",
    "    idx = 0\n",
    "    for f in frames:\n",
    "        widths = np.zeros(len(f.L_gal_bins)-1)\n",
    "        for i in range(0,len(f.L_gal_bins)-1):\n",
    "            widths[i]=(f.L_gal_bins[i+1] - f.L_gal_bins[i]) / len(frames)\n",
    "        ax2.bar(f.L_gal_labels+(widths*idx), f.all_data[f.all_data.is_sat == True].groupby('Lgal_bin').size(), width=widths, color=f.color, alpha=0.5)\n",
    "        idx+=1\n",
    "    ax2.set_ylabel('$N_{sat}$')\n",
    "    fig.tight_layout()\n",
    "\n",
    "\n",
    "    print(\"TOTAL f_sat: \")\n",
    "    for f in frames:\n",
    "        print(f\"  {f.name}:  {f.all_data['is_sat'].sum() / f.all_data['is_sat'].count():.3f}\")\n",
    "        if 'is_sat_truth' in f.all_data.columns:\n",
    "            print(f\"  MXXL Truth: {f.all_data['is_sat_truth'].sum() / f.all_data['is_sat_truth'].count():.3f}\")\n",
    "\n",
    "# It gives same result as NFW version! Good\n",
    "def get_vir_radius_mine(halo_mass):\n",
    "    _cosmo = get_MXXL_cosmology()\n",
    "    rho_m = (_cosmo.critical_density(0) * _cosmo.Om(0))\n",
    "    return np.power(((3/(4*math.pi)) * halo_mass / (200*rho_m)), (1/3)).to(u.kpc).value\n",
    "\n",
    "def post_process(frame):\n",
    "    df: pd.DataFrame = frame.all_data\n",
    "    \n",
    "    # Calculate additional halo properties\n",
    "    masses = df.loc[:, 'mxxl_halo_mass'].to_numpy() * 1E10 * u.solMass\n",
    "    df.loc[:, 'mxxl_halo_vir_radius_guess'] = get_vir_radius_mine(masses)\n",
    "\n",
    "    _cosmo = FlatLambdaCDM(H0=73, Om0=0.25, Ob0=0.045, Tcmb0=2.725, Neff=3.04) \n",
    "    # TODO comoving or proper?\n",
    "    as_per_kpc = _cosmo.arcsec_per_kpc_proper(df.loc[:, 'z'].to_numpy())\n",
    "    df.loc[:, 'mxxl_halo_vir_radius_guess_arcsec'] =  df.loc[:, 'mxxl_halo_vir_radius_guess'] * as_per_kpc.to(u.arcsec / u.kpc).value\n",
    "\n",
    "    # Luminosity distance to z_obs\n",
    "    df.loc[:, 'ldist_true'] = z_to_ldist(df.z_obs.to_numpy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading existing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = types.SimpleNamespace()\n",
    "all.name = \"All <19.5\"\n",
    "all20 = types.SimpleNamespace()\n",
    "all20.name = \"All <20\"\n",
    "fiberonly = types.SimpleNamespace()\n",
    "fiberonly.name = \"Fiber Assigned Only <19.5\"\n",
    "fiberonly20 = types.SimpleNamespace()\n",
    "fiberonly20.name = \"Fiber Assigned Only <20\"\n",
    "nn_kd = types.SimpleNamespace()\n",
    "nn_kd.name = \"Nearest Neighbor <19.5\"\n",
    "nn_kd20 = types.SimpleNamespace()\n",
    "nn_kd20.name = \"Nearest Neighbor <20\"\n",
    "fancy_1 = types.SimpleNamespace()\n",
    "fancy_1.name = \"Fancy v1 <19.5\"\n",
    "fancy_6 = types.SimpleNamespace()\n",
    "fancy_6.name = \"Fancy v6 <19.5\"\n",
    "simple_1 = types.SimpleNamespace()\n",
    "simple_1.name = \"Simple v1 <19.5\"\n",
    "simple_1_20 = types.SimpleNamespace()\n",
    "simple_1_20.name = \"Simple v1 <20\"\n",
    "simple_2 = types.SimpleNamespace()\n",
    "simple_2.name = \"Simple v2 <19.5\"\n",
    "simple_2_20 = types.SimpleNamespace()\n",
    "simple_2_20.name = \"Simple v2 <20\"\n",
    "simple_2_mix = types.SimpleNamespace()\n",
    "simple_2_mix.name = \"Simple v2 mix\"\n",
    "simple_3 = types.SimpleNamespace()\n",
    "simple_3.name = \"Simple v3 <19.5\"\n",
    "simple_3_20 = types.SimpleNamespace()\n",
    "simple_3_20.name = \"Simple v3 <20\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(ROOT_FOLDER + all.name, 'rb') as f:    \n",
    "    all = pickle.load(f)\n",
    "#with open(ROOT_FOLDER + all20.name, 'rb') as f:    \n",
    "#    all20 = pickle.load(f)\n",
    "#with open(ROOT_FOLDER + fiberonly.name, 'rb') as f:    \n",
    "#    fiberonly = pickle.load(f)\n",
    "#with open(ROOT_FOLDER + fiberonly20.name, 'rb') as f:    \n",
    "#    fiberonly20 = pickle.load(f)\n",
    "#with open(ROOT_FOLDER + nn_kd.name, 'rb') as f:    \n",
    "#    nn_kd = pickle.load(f)\n",
    "#with open(ROOT_FOLDER + nn_kd20.name, 'rb') as f:    \n",
    "#   nn_kd20 = pickle.load(f)\n",
    "#with open(ROOT_FOLDER + simple_1.name, 'rb') as f:    \n",
    "#    simple_1 = pickle.load(f)\n",
    "#with open(ROOT_FOLDER + simple_1_20.name, 'rb') as f:    \n",
    "#    simple_1_20 = pickle.load(f)\n",
    "#with open(ROOT_FOLDER + simple_2.name, 'rb') as f:    \n",
    "#    simple_2 = pickle.load(f)\n",
    "#with open(ROOT_FOLDER + simple_2_20.name, 'rb') as f:    \n",
    "#    simple_2_20 = pickle.load(f)\n",
    "#with open(ROOT_FOLDER + simple_3.name, 'rb') as f:    \n",
    "#    simple_3 = pickle.load(f)\n",
    "#with open(ROOT_FOLDER + simple_3_20.name, 'rb') as f:    \n",
    "#    simple_3_20 = pickle.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process New Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = process(ROOT_FOLDER + \"mxxl_3pass_all.out\")\n",
    "all.name = \"All <19.5\"\n",
    "all.color = get_color(0)\n",
    "all.marker = '-'\n",
    "post_process(all)\n",
    "with open(ROOT_FOLDER + all.name, 'wb') as f:\n",
    "    pickle.dump(all, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all20 = process(ROOT_FOLDER + \"mxxl_3pass_all20.out\")\n",
    "all20.name = \"All <20\"\n",
    "all20.color = get_color(0)\n",
    "all20.marker = '--'\n",
    "post_process(all20)\n",
    "with open(ROOT_FOLDER + all20.name, 'wb') as f:\n",
    "    pickle.dump(all20, f)\n",
    "del(all20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiberonly = process(ROOT_FOLDER + \"mxxl_3pass_fiberonly.out\")\n",
    "fiberonly.name = \"Fiber Assigned Only <19.5\"\n",
    "fiberonly.color = get_color(1)\n",
    "fiberonly.marker = '-'\n",
    "post_process(fiberonly)\n",
    "with open(ROOT_FOLDER + fiberonly.name, 'wb') as f:\n",
    "    pickle.dump(fiberonly, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiberonly20 = process(ROOT_FOLDER + \"mxxl_3pass_fiberonly20.out\")\n",
    "fiberonly20.name = \"Fiber Assigned Only <20\"\n",
    "fiberonly20.color = get_color(1)\n",
    "fiberonly20.marker = '--'\n",
    "post_process(fiberonly20)\n",
    "with open(ROOT_FOLDER + fiberonly20.name, 'wb') as f:\n",
    "    pickle.dump(fiberonly20, f)\n",
    "del(fiberonly20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_kd = process(ROOT_FOLDER + \"mxxl_3pass_nn_kd.out\")\n",
    "nn_kd.name = \"Nearest Neighbor <19.5\"\n",
    "nn_kd.color = get_color(2)\n",
    "nn_kd.marker = '-'\n",
    "post_process(nn_kd)\n",
    "with open(ROOT_FOLDER + nn_kd.name, 'wb') as f:\n",
    "    pickle.dump(nn_kd, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_kd20 = process(ROOT_FOLDER + \"mxxl_3pass_nn_kd20.out\")\n",
    "nn_kd20.name = \"Nearest Neighbor <20\"\n",
    "nn_kd20.color = get_color(2)\n",
    "nn_kd20.marker = '--'\n",
    "post_process(nn_kd20)\n",
    "with open(ROOT_FOLDER + nn_kd20.name, 'wb') as f:\n",
    "    pickle.dump(nn_kd20, f)\n",
    "del(nn_kd20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fancy_1 = process(ROOT_FOLDER + \"mxxl_3pass_fancy_1.out\")\n",
    "post_process(fancy_1)\n",
    "fancy_1.name = \"Fancy v1 <19.5\"\n",
    "fancy_1.color = get_color(3)\n",
    "fancy_1.marker = '-'\n",
    "with open(ROOT_FOLDER + fancy_1.name, 'wb') as f:\n",
    "    pickle.dump(fancy_1, f)\n",
    "del(fancy_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fancy_6 = process(ROOT_FOLDER + \"mxxl_3pass_fancy_6.out\")\n",
    "post_process(fancy_6)\n",
    "fancy_6.name = \"Fancy v6 <19.5\"\n",
    "fancy_6.color = get_color(4)\n",
    "fancy_6.marker = '-'\n",
    "with open(ROOT_FOLDER + fancy_6.name, 'wb') as f:\n",
    "    pickle.dump(fancy_6, f)\n",
    "del(fancy_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "simple_1 = process(ROOT_FOLDER + \"mxxl_3pass_simple_1.out\")\n",
    "post_process(simple_1)\n",
    "simple_1.name = \"Simple v1 <19.5\"\n",
    "simple_1.color = get_color(5)\n",
    "simple_1.marker = '-'\n",
    "with open(ROOT_FOLDER + simple_1.name, 'wb') as f:\n",
    "    pickle.dump(simple_1, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_1_20 = process(ROOT_FOLDER + \"mxxl_3pass_simple_1_20.out\")\n",
    "post_process(simple_1_20)\n",
    "simple_1_20.name = \"Simple v1 <20\"\n",
    "simple_1_20.color = get_color(5)\n",
    "simple_1_20.marker = '--'\n",
    "with open(ROOT_FOLDER + simple_1_20.name, 'wb') as f:\n",
    "    pickle.dump(simple_1_20, f)\n",
    "del(simple_1_20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_2 = process(ROOT_FOLDER + \"mxxl_3pass_simple_2.out\")\n",
    "post_process(simple_2)\n",
    "simple_2.name = \"Simple v2 <19.5\"\n",
    "simple_2.color = get_color(6)\n",
    "simple_2.marker = '-'\n",
    "with open(ROOT_FOLDER + simple_2.name, 'wb') as f:\n",
    "    pickle.dump(simple_2, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_2_20 = process(ROOT_FOLDER + \"mxxl_3pass_simple_2_20.out\")\n",
    "post_process(simple_2_20)\n",
    "simple_2_20.name = \"Simple v2 <20\"\n",
    "simple_2_20.color = get_color(6)\n",
    "simple_2_20.marker = '--'\n",
    "with open(ROOT_FOLDER + simple_2_20.name, 'wb') as f:\n",
    "    pickle.dump(simple_2_20, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLD ONE\n",
    "simple_2_mix = process(ROOT_FOLDER + \"mxxl_3pass_simple_2_mix.out\")\n",
    "post_process(simple_2_mix)\n",
    "simple_2_mix.name = \"Simple v2 mix\"\n",
    "simple_2_mix.color = get_color(6)\n",
    "simple_2_mix.marker = '-.'\n",
    "with open(ROOT_FOLDER + simple_2_mix.name, 'wb') as f:\n",
    "    pickle.dump(simple_2_mix, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_3 = process(ROOT_FOLDER + \"mxxl_3pass_simple_3.out\")\n",
    "post_process(simple_3)\n",
    "simple_3.name = \"Simple v3 <19.5\"\n",
    "simple_3.color = get_color(7)\n",
    "simple_3.marker = '-'\n",
    "with open(ROOT_FOLDER + simple_2.name, 'wb') as f:\n",
    "    pickle.dump(simple_3, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_3_20 = process(ROOT_FOLDER + \"mxxl_3pass_simple_3_20.out\")\n",
    "post_process(simple_3_20)\n",
    "simple_3_20.name = \"Simple v3 <20\"\n",
    "simple_3_20.color = get_color(7)\n",
    "simple_3_20.marker = '--'\n",
    "with open(ROOT_FOLDER + simple_3_20.name, 'wb') as f:\n",
    "    pickle.dump(simple_3_20, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO figure out\n",
    "#def save_processed_data(frame):\n",
    "#    frame.all_data.to_feather(ROOT_FOLDER + frame.name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View plots on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type in whatever datasets you want included in the generated plots\n",
    "#plots(all, fiberonly, nn_kd, simple_1)\n",
    "plots(all,fiberonly, nn_kd, simple_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What effect does Fiber Assignment have on group finder properties?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Halo Masses (in group finder abundance matching)\n",
    "def group_finder_centrals_halo_masses_plots(all_to_use, comparisons):\n",
    "\n",
    "    all_centrals = all_to_use.all_data[all_to_use.all_data.index == all_to_use.all_data.igrp]\n",
    "    angdist_bin_ind = np.digitize(all_centrals.M_halo, all_to_use.Mhalo_bins)\n",
    "    all_bincounts = np.bincount(angdist_bin_ind)[0:len(all_to_use.Mhalo_bins)]\n",
    "    all_density = all_bincounts / np.sum(all_bincounts)\n",
    "\n",
    "    fig,axes=plt.subplots(nrows=1, ncols=2, figsize=(12,4))\n",
    "    fig.set_dpi(DPI)\n",
    "    axes[0].set_xscale('log')\n",
    "    axes[0].set_ylim(-0.2, 0.2)\n",
    "    axes[0].set_xlim(5E10,2E15)\n",
    "    axes[0].set_xlabel('$M_{halo}$')\n",
    "    axes[0].set_ylabel('Normalized log(Comparion / All)')\n",
    "    axes[0].axline((3E10,0), (3E15,0), linestyle='--', color='k')\n",
    "    axes[0].set_title(\"Group Finder Halo Masses of Centrals\")\n",
    "\n",
    "    axes[1].plot(all_to_use.Mhalo_bins, all_density, label=\"All Galaxies\") \n",
    "    axes[1].set_xscale('log')\n",
    "    axes[1].set_yscale('log')\n",
    "    axes[1].set_xlim(5E10,2E15)\n",
    "    axes[1].set_xlabel('$M_{halo}$')\n",
    "    axes[1].set_ylabel('Density of Galaxies')\n",
    "    axes[1].set_title(\"Group Finder Halo Masses of Centrals\")\n",
    "\n",
    "    for comparison in comparisons:\n",
    "\n",
    "        centrals = comparison.all_data[comparison.all_data.index == comparison.all_data.igrp]\n",
    "        angdist_bin_ind = np.digitize(centrals.M_halo, all_to_use.Mhalo_bins)\n",
    "        bincounts = np.bincount(angdist_bin_ind)[0:len(all_to_use.Mhalo_bins)]\n",
    "        density = bincounts / np.sum(bincounts)\n",
    "\n",
    "        axes[0].plot(all_to_use.Mhalo_bins, np.log10(density / all_density), linestyle=comparison.marker, color=comparison.color, label=comparison.name) \n",
    "        axes[1].plot(all_to_use.Mhalo_bins, density, linestyle=comparison.marker, color=comparison.color, label=comparison.name) \n",
    "\n",
    "    axes[0].legend()\n",
    "    axes[1].legend()\n",
    "\n",
    "    # Look up the centrals from all in fiberonly\n",
    "    for comparison in comparisons:\n",
    "\n",
    "        centrals = comparison.all_data[comparison.all_data.index == comparison.all_data.igrp]\n",
    "        catalog = coord.SkyCoord(ra=all_centrals.RA.to_numpy()*u.degree, dec=all_centrals.Dec.to_numpy()*u.degree, frame='icrs')\n",
    "        to_match = coord.SkyCoord(ra=centrals.RA.to_numpy()*u.degree, dec=centrals.Dec.to_numpy()*u.degree, frame='icrs')\n",
    "        idx, d2d, d3d = coord.match_coordinates_sky(to_match, catalog, nthneighbor=1, storekdtree=False)\n",
    "\n",
    "        perfect_match = np.isclose(d2d.to(u.arcsec).value, 0, rtol=0.0, atol=0.0001) \n",
    "        # 0.0001 arcsec precision on matching doesn't hit floating point noise. You get same with 0.001\n",
    "        print(f\"What fraction of centrals in \\'{comparison.name}\\' are centrals in \\'all\\'? {np.sum(perfect_match) / len(d2d)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import astropy\n",
    "print(np.__version__)\n",
    "print(astropy.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_finder_centrals_halo_masses_plots(all, [simple_2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare assigned implied abs mags to truth from MXXL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_to_check = simple_2\n",
    "\n",
    "not_assigned = np.invert(run_to_check.all_data.fiber_assigned_0.astype(bool))\n",
    "app_mags = run_to_check.all_data.app_mag[not_assigned].to_numpy()\n",
    "my_assigned_abs_mag = app_mag_to_abs_mag(app_mags, run_to_check.all_data.z[not_assigned].to_numpy())\n",
    "my_raw_abs_mag = app_mag_to_abs_mag(app_mags, run_to_check.all_data.z_obs[not_assigned].to_numpy())\n",
    "\n",
    "print(len(my_raw_abs_mag), len(my_assigned_abs_mag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare absolute mags. Using my way of computing for both.\n",
    "abs_mag_bins = np.linspace(-27, -10, num=50)\n",
    "plt.figure(dpi=DPI)\n",
    "x = plt.hist(my_raw_abs_mag, bins=abs_mag_bins, label=\"Truth\", alpha=0.5)\n",
    "y = plt.hist(my_assigned_abs_mag, bins=abs_mag_bins, label=f\"{run_to_check.name} Assigned\", alpha=0.5)\n",
    "plt.xlabel(\"Absolute Mag\")\n",
    "plt.ylabel(\"Count\")\n",
    "#plt.yscale('log')\n",
    "plt.title(\"Compare Lost Galaxies Abs Mags\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Purity and Completeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_interior_bin_labels(bin_edges):\n",
    "    labels = []\n",
    "    for i in range(0,len(bin_edges)-1):\n",
    "        labels.append(f\"{bin_edges[i]:.2e} - {bin_edges[i+1]:.2e}\")\n",
    "    return labels\n",
    "\n",
    "def test_purity_and_completeness(*sets):\n",
    "\n",
    "    for s in sets:\n",
    "        print(s.name)\n",
    "        data = s.all_data\n",
    "\n",
    "        assigned_sats = data[data.is_sat == True]\n",
    "        print(f\"Purity of sats: {np.sum(assigned_sats.is_sat_truth) / len(assigned_sats.index):.3f}\")\n",
    "\n",
    "        true_sats = data[data.is_sat_truth == True]\n",
    "        print(f\"Completeness of sats: {np.sum(true_sats.is_sat) / len(true_sats.index):.3f}\")\n",
    "\n",
    "        assigned_centrals = data[data.is_sat == False]\n",
    "        print(f\"Purity of centrals: {1 - (np.sum(assigned_centrals.is_sat_truth) / len(assigned_centrals.index)):.3f}\")\n",
    "\n",
    "        true_centrals = data[data.is_sat_truth == False]\n",
    "        print(f\"Completeness of centrals: {1 - (np.sum(true_centrals.is_sat) / len(true_centrals.index)):.3f}\")\n",
    "\n",
    "        assigned_true_sats = assigned_sats[assigned_sats.is_sat_truth == True]\n",
    "        assigned_sats_g = assigned_sats.groupby('Lgal_bin').size().to_numpy()\n",
    "        assigned_sats_correct_g = assigned_true_sats.groupby('Lgal_bin').size().to_numpy()\n",
    "        s.keep=np.nonzero(assigned_sats_g)\n",
    "        s.purity_g = assigned_sats_correct_g[s.keep] / assigned_sats_g[s.keep]\n",
    "\n",
    "        true_sats_assigned = true_sats[true_sats.is_sat == True]\n",
    "        true_sats_g = true_sats.groupby('Lgal_bin').size().to_numpy()\n",
    "        true_sats_correct_g = true_sats_assigned.groupby('Lgal_bin').size().to_numpy()\n",
    "        s.keep2=np.nonzero(true_sats_g)\n",
    "        s.completeness_g = true_sats_correct_g[s.keep2] / true_sats_g[s.keep2]\n",
    "\n",
    "        assigned_true_centrals = assigned_centrals[assigned_centrals.is_sat_truth == False]\n",
    "        assigned_centrals_g = assigned_centrals.groupby('Lgal_bin').size().to_numpy()\n",
    "        assigned_centrals_correct_g = assigned_true_centrals.groupby('Lgal_bin').size().to_numpy()\n",
    "        s.keep3=np.nonzero(assigned_centrals_g)\n",
    "        s.purity_c_g = assigned_centrals_correct_g[s.keep3] / assigned_centrals_g[s.keep3]\n",
    "\n",
    "        true_centrals_assigned = true_centrals[true_centrals.is_sat == False]\n",
    "        true_centrals_g = true_centrals.groupby('Lgal_bin').size().to_numpy()\n",
    "        true_centrals_correct_g = true_centrals_assigned.groupby('Lgal_bin').size().to_numpy()\n",
    "        s.keep4=np.nonzero(true_centrals_g)\n",
    "        s.completeness_c_g = true_centrals_correct_g[s.keep4] / true_centrals_g[s.keep4]\n",
    "\n",
    "\n",
    "def purity_complete_plots(*sets):\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12, 8))\n",
    "    fig.set_dpi(DPI/2)\n",
    "\n",
    "    axes[1][0].set_title('Satellite Purity')\n",
    "    axes[1][0].set_xscale('log')\n",
    "    axes[1][0].set_xlabel('$L_{gal}$')\n",
    "    axes[1][0].set_xlim(2E8,1E11)\n",
    "    axes[1][0].set_ylim(0.4,1.0)\n",
    "\n",
    "    axes[1][1].set_title('Satellite Completeness')\n",
    "    axes[1][1].set_xscale('log')\n",
    "    axes[1][1].set_xlabel('$L_{gal}$')\n",
    "    axes[1][1].set_xlim(2E8,1E11)\n",
    "    axes[1][1].set_ylim(0.4,1.0)\n",
    "\n",
    "    axes[0][0].set_title('Central Purity')\n",
    "    axes[0][0].set_xscale('log')\n",
    "    axes[0][0].set_xlabel('$L_{gal}$')\n",
    "    axes[0][0].set_xlim(2E8,1E11)\n",
    "    axes[0][0].set_ylim(0.4,1.0)\n",
    "\n",
    "    axes[0][1].set_title('Central Completeness')\n",
    "    axes[0][1].set_xscale('log')\n",
    "    axes[0][1].set_xlabel('$L_{gal}$')\n",
    "    axes[0][1].set_xlim(2E8,1E11)\n",
    "    axes[0][1].set_ylim(0.4,1.0)\n",
    "\n",
    "    for s in sets:\n",
    "        axes[1][0].plot(s.L_gal_bins[s.keep], s.purity_g, s.marker, label=f\"{s.name}\", color=s.color)\n",
    "        axes[1][1].plot(s.L_gal_bins[s.keep2], s.completeness_g, s.marker, label=f\"{s.name}\", color=s.color)\n",
    "        axes[0][0].plot(s.L_gal_bins[s.keep3], s.purity_c_g, s.marker, label=f\"{s.name}\", color=s.color)\n",
    "        axes[0][1].plot(s.L_gal_bins[s.keep4], s.completeness_c_g, s.marker, label=f\"{s.name}\", color=s.color)\n",
    "\n",
    "    \n",
    "    axes[0][0].legend()\n",
    "    fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_purity_and_completeness(all, simple_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "purity_complete_plots(all, simple_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find fraction of time the NN is in the same halo, similar z, etc\n",
    "\n",
    "There is another version of this directly on the MXXL data above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resulting_halo_analysis(*sets):\n",
    "\n",
    "    for data in sets:\n",
    "\n",
    "        print(data.name)\n",
    "\n",
    "        #same_halo_mass = np.isclose(data.all_data['assigned_halo_mass'], data.all_data['mxxl_halo_mass'], atol=0.0, rtol=1e-03)\n",
    "        #same_mxxl_halo = data.all_data['assigned_halo_mass']\n",
    "        #data.all_data['same_mxxl_halo'] = same_mxxl_halo\n",
    "\n",
    "        lost_galaxies = data.all_data[data.all_data.fiber_assigned_0 == 0]\n",
    "        print(len(lost_galaxies), \"lost galaxies\")\n",
    "\n",
    "        # TODO understand this MXXL quirk\n",
    "        lost_galaxies = lost_galaxies[lost_galaxies['assigned_halo_id'] != 0]\n",
    "        print(len(lost_galaxies), \"lost galaxies after removing ones with no MXXL halo ID (no idea why)\")\n",
    "\n",
    "        lost_galaxies_same_halo = np.equal(lost_galaxies['assigned_halo_id'], lost_galaxies['mxxl_halo_id'])\n",
    "        print(\"Fraction of time assigned halo ID is the same as the galaxy's actual halo ID: {0:.3f}\".format(np.sum(lost_galaxies_same_halo) / len(lost_galaxies_same_halo)))\n",
    "        \n",
    "        lost_galaxies_same_halo_mass = np.isclose(lost_galaxies['assigned_halo_mass'], lost_galaxies['mxxl_halo_mass'], atol=0.0, rtol=1e-03)\n",
    "        print(\"Fraction of time assigned halo mass is \\'the same\\' as the galaxy's actual halo mass: {0:.3f}\".format(np.sum(lost_galaxies_same_halo_mass) / len(lost_galaxies_same_halo_mass)))\n",
    "      \n",
    "        z_thresh=0.01\n",
    "        lost_galaxies_similar_z = np.isclose(lost_galaxies['z'], lost_galaxies['z_obs'], atol=z_thresh, rtol=0.0)         \n",
    "        print(\"Fraction of time assigned z is the target z +/- {0:.3f}:\".format(z_thresh), np.sum(lost_galaxies_similar_z) / len(lost_galaxies_similar_z))\n",
    "\n",
    "        z_thresh=0.005\n",
    "        lost_galaxies_similar_z = np.isclose(lost_galaxies['z'], lost_galaxies['z_obs'], atol=z_thresh, rtol=0.0)         \n",
    "        print(\"Fraction of time assigned z is the target z +/- {0:.3f}:\".format(z_thresh), np.sum(lost_galaxies_similar_z) / len(lost_galaxies_similar_z))\n",
    "\n",
    "        z_thresh=0.003\n",
    "        lost_galaxies_similar_z = np.isclose(lost_galaxies['z'], lost_galaxies['z_obs'], atol=z_thresh, rtol=0.0)         \n",
    "        print(\"Fraction of time assigned z is the target z +/- {0:.3f}:\".format(z_thresh), np.sum(lost_galaxies_similar_z) / len(lost_galaxies_similar_z))\n",
    "\n",
    "        z_thresh=0.001\n",
    "        lost_galaxies_similar_z = np.isclose(lost_galaxies['z'], lost_galaxies['z_obs'], atol=z_thresh, rtol=0.0)        \n",
    "        print(\"Fraction of time assigned z is the target z +/- {0:.3f}:\".format(z_thresh), np.sum(lost_galaxies_similar_z) / len(lost_galaxies_similar_z))\n",
    "        \n",
    "        # TODO as a function of reshift. But we essentially already have this from the direct MXXL data plots\n",
    "\n",
    "        #z_bins = np.linspace(min(data.all_data.z), max(data.all_data.z), 20)\n",
    "        #z_labels = z_bins[0:len(z_bins)-1] \n",
    "        #data.all_data['z_bin'] = pd.cut(x = data.all_data['z'], bins = z_bins, labels = z_labels, include_lowest = True)\n",
    "\n",
    "        #groupby_z = lost_galaxies.groupby('z_bin')['same_halo_mass'].sum() / lost_galaxies.groupby('z_bin')['same_halo_mass'].count()\n",
    "\n",
    "        #plt.plot(z_labels, groupby_z)\n",
    "        #plt.xlabel('$z_{eff}$ (effective/assigned redshift)')\n",
    "        #plt.ylabel('Fraction Assigned Halo = True Host Halo')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resulting_halo_analysis(simple_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Galaxy Neighborhood Examiner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fancy_1.all_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lost_galaxies = data.loc[data['fiber_assigned_0'] == 0]\n",
    "#lost_galaxies_2 = nn.all_data.loc[nn.all_data['fiber_assigned_0'] == 0]\n",
    "obs_galaxies = data.loc[data['fiber_assigned_0'] == 1]\n",
    "#obs_galaxies_2 = nn.all_data.loc[nn.all_data['fiber_assigned_0'] == 1]\n",
    "print(\"Lost galaxies: \", len(lost_galaxies), \"Observed Galaxies: \", len(obs_galaxies))\n",
    "\n",
    "# TODO could use angular size / redshift relation as part of this :-)\n",
    "def getsize(z):\n",
    "    if z < 0.05:\n",
    "        return 300\n",
    "    elif z < 0.1:\n",
    "        return 200\n",
    "    elif z < 0.2:\n",
    "        return 120\n",
    "    elif z < 0.2:\n",
    "        return 75\n",
    "    elif z < 0.3:\n",
    "        return 45\n",
    "    elif z < 0.4:\n",
    "        return 25\n",
    "    elif z < 0.5:\n",
    "        return 15\n",
    "    elif z < 0.6:\n",
    "        return 8\n",
    "    else:\n",
    "        return 3\n",
    "\n",
    "nearby_angle = coord.Angle('5m')\n",
    "\n",
    "def neighbor_exam(target):\n",
    "    z_eff = target.z\n",
    "    target_dist_true = z_to_ldist(target.z_obs)\n",
    "\n",
    "    ra_max = (coord.Angle(target.RA*u.degree) + nearby_angle).value\n",
    "    ra_min = (coord.Angle(target.RA*u.degree) - nearby_angle).value\n",
    "    dec_max = (coord.Angle(target.Dec*u.degree) + nearby_angle).value\n",
    "    dec_min = (coord.Angle(target.Dec*u.degree) - nearby_angle).value\n",
    "\n",
    "    nearby = obs_galaxies.query('RA < @ra_max and RA > @ra_min and Dec < @dec_max and Dec > @dec_min')\n",
    "\n",
    "    close_neighbors = 0\n",
    "    if len(nearby) > 0:\n",
    "        close_neighbors = np.isclose(nearby.ldist_true.to_numpy(), target_dist_true, rtol=0.0, atol=20)\n",
    "\n",
    "    return (np.sum(close_neighbors), len(nearby), np.sum(close_neighbors)/len(nearby))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = coord.SkyCoord(ra=data.RA.to_numpy()*u.degree, dec=data.Dec.to_numpy()*u.degree, frame='icrs')\n",
    "\n",
    "# This is too slow when called 1 at a time, not using. \n",
    "# TODO Could be faster when batched for the whole sample?\n",
    "def neighbors_within(max_angle: coord.Angle, to_match: coord.Angle, catalog: np.ndarray, treekey: str):\n",
    "\n",
    "    angular_distance = coord.Angle(0*u.arcsec)\n",
    "    nth = 1 # cap at 100 for now, TODO remove when safe\n",
    "    neighbor_ind = []\n",
    "    neighbor_dist = []\n",
    "\n",
    "    while angular_distance < max_angle and nth < 100:\n",
    "        idx, d2d, d3d = coord.match_coordinates_sky(to_match, catalog, nthneighbor=nth, storekdtree=treekey)\n",
    "        angular_distance = d2d\n",
    "        nth = nth + 1\n",
    "        neighbor_ind.append(idx)\n",
    "        neighbor_dist.append(angular_distance)\n",
    "\n",
    "    return neighbor_ind, neighbor_dist\n",
    "\n",
    "def examine_around(target):\n",
    "\n",
    "    target_observed = target.fiber_assigned_0\n",
    "    #target = data.loc[index]\n",
    "\n",
    "    target_pos = coord.SkyCoord(ra=target.RA*u.degree, dec=target.Dec*u.degree, frame='icrs')\n",
    "\n",
    "    z_eff = target.z\n",
    "    #target_dist_true = z_to_ldist(target.z_obs)\n",
    "\n",
    "    ra_max = (coord.Angle(target.RA*u.degree) + nearby_angle).value\n",
    "    ra_min = (coord.Angle(target.RA*u.degree) - nearby_angle).value\n",
    "    dec_max = (coord.Angle(target.Dec*u.degree) + nearby_angle).value\n",
    "    dec_min = (coord.Angle(target.Dec*u.degree) - nearby_angle).value\n",
    "\n",
    "    # TODO replace with a non-angular search so all redshifts are treated equally\n",
    "    #indexes, angular_distances = neighbors_within(nearby_angle, target_pos, catalog, 'treekey_nnkd')\n",
    "    #nearby = data.iloc[indexes]\n",
    "    nearby = data.query('RA < @ra_max and RA > @ra_min and Dec < @dec_max and Dec > @dec_min')\n",
    "    nearby = nearby.drop(target.name) # drop the target itself from this df\n",
    "\n",
    "    nearby_obs = nearby.loc[nearby['fiber_assigned_0'] == 1]\n",
    "    nearby_unobs = nearby.loc[nearby['fiber_assigned_0'] == 0]\n",
    "\n",
    "    z_match = nearby_obs.query('z == @z_eff')\n",
    "    #assert len(z_match) == 1, len(z_match) # TODO need a better way to verify which row is the one that we assigned the z from\n",
    "    if len(z_match) > 0:\n",
    "        z_match = z_match.iloc[0]\n",
    "    #nearby_obs = nearby_obs.drop(z_match.name)\n",
    "\n",
    "    good_obs_z_filter = list(map(lambda a: close_enough(target.z_obs, a), nearby_obs.z))\n",
    "    nearby_obs_good_z = nearby_obs.loc[good_obs_z_filter]\n",
    "    nearby_obs_good_z_dim = nearby_obs_good_z.loc[nearby_obs_good_z.app_mag > 19.5]\n",
    "    nearby_obs_good_z = nearby_obs_good_z.loc[np.invert(nearby_obs_good_z.app_mag > 19.5)]\n",
    "\n",
    "    if len(good_obs_z_filter) > 0:\n",
    "        nearby_obs_other = nearby_obs.loc[np.invert(good_obs_z_filter)]\n",
    "    else:\n",
    "        nearby_obs_other = nearby_obs\n",
    "    nearby_obs_other_dim = nearby_obs_other.loc[nearby_obs_other.app_mag > 19.5]\n",
    "    nearby_obs_other = nearby_obs_other.loc[np.invert(nearby_obs_other.app_mag > 19.5)]\n",
    "\n",
    "    good_unobs_z_filter = list(map(lambda a: close_enough(target.z_obs, a), nearby_unobs.z))\n",
    "\n",
    "    nearby_unobs_good_z = nearby_unobs.loc[good_unobs_z_filter]\n",
    "    if good_unobs_z_filter:\n",
    "        nearby_unobs_other = nearby_unobs.loc[np.invert(good_unobs_z_filter)]\n",
    "        nearby_unobs_other_dim = nearby_unobs_other.loc[nearby_unobs_other.app_mag > 19.5]\n",
    "        nearby_unobs_other = nearby_unobs_other.loc[np.invert(nearby_unobs_other.app_mag > 19.5)]\n",
    "    else:\n",
    "        nearby_unobs_other = nearby_unobs_good_z # empty df\n",
    "        nearby_unobs_other_dim = nearby_unobs_good_z\n",
    "\n",
    "    nearby_unobs_good_z_dim = nearby_unobs_good_z.loc[nearby_unobs_good_z.app_mag > 19.5]\n",
    "    nearby_unobs_good_z = nearby_unobs_good_z.loc[np.invert(nearby_unobs_good_z.app_mag > 19.5)]\n",
    "\n",
    "    if target_observed:\n",
    "        title = \"Observed Galaxy {0}: z_true={1:.3f}, z_NN={2:.3f}\".format(target.name, target.z_obs, target.z)\n",
    "    else:\n",
    "        title = \"Lost Galaxy {0}: z_true={1:.3f}, z_NN={2:.3f}\".format(target.name, target.z_obs, target.z)\n",
    "\n",
    "    if len(nearby) > 1:\n",
    "\n",
    "        fig,ax = plt.subplots(1)\n",
    "        fig.set_size_inches(10,10)\n",
    "        ax.set_aspect('equal')\n",
    "\n",
    "        # Add virial radii or MXXL Halos to the observed galaxies\n",
    "        for k in range(len(nearby_obs)):\n",
    "            current = nearby_obs.iloc[k]\n",
    "            radius = current.mxxl_halo_vir_radius_guess_arcsec / 3600 # arcsec to degrees, like the plot\n",
    "            circ = Circle((current.RA,current.Dec), radius, color=get_color(0), alpha=0.10)\n",
    "            ax.add_patch(circ)\n",
    "\n",
    "        textsize = 9\n",
    "        dimalpha = 0.4\n",
    "\n",
    "        plt.scatter(nearby_obs_other.RA, nearby_obs_other.Dec, s=list(map(getsize, nearby_obs_other.z)), color=get_color(0), label=\"Obs ({0})\".format(len(nearby_obs_other)))\n",
    "        if len(nearby_obs_other_dim) > 0:\n",
    "            plt.scatter(nearby_obs_other_dim.RA, nearby_obs_other_dim.Dec, s=list(map(getsize, nearby_obs_other_dim.z)), color=get_color(2), alpha=dimalpha, label=\"Obs dim ({0})\".format(len(nearby_obs_other_dim)))\n",
    "        \n",
    "        plt.scatter(nearby_obs_good_z.RA, nearby_obs_good_z.Dec, s=list(map(getsize, nearby_obs_good_z.z)), color=get_color(2), label=\"Obs good z ({0})\".format(len(nearby_obs_good_z)))\n",
    "        if len(nearby_obs_good_z_dim) > 0:\n",
    "            plt.scatter(nearby_obs_good_z_dim.RA, nearby_obs_good_z_dim.Dec, s=list(map(getsize, nearby_obs_good_z_dim.z)), color=get_color(0), alpha=dimalpha, label=\"Obs good z dim ({0})\".format(len(nearby_obs_good_z_dim)))\n",
    "\n",
    "        plt.scatter(nearby_unobs_other.RA, nearby_unobs_other.Dec, marker='x', s=list(map(getsize, nearby_unobs_other.z)), color=get_color(0), label=\"Unobs ({0})\".format(len(nearby_unobs_other)))\n",
    "        if len(nearby_unobs_other_dim) > 0:\n",
    "            plt.scatter(nearby_unobs_other_dim.RA, nearby_unobs_other_dim.Dec, marker='x', s=list(map(getsize, nearby_unobs_other_dim.z)), color=get_color(0), alpha=dimalpha, label=\"Unobs dim ({0})\".format(len(nearby_unobs_other_dim)))\n",
    "        \n",
    "        plt.scatter(nearby_unobs_good_z.RA, nearby_unobs_good_z.Dec, marker='x', s=list(map(getsize, nearby_unobs_good_z.z)), color=get_color(2), label=\"Unobs good z ({0})\".format(len(nearby_unobs_good_z)))\n",
    "        if len(nearby_unobs_good_z_dim) > 0:\n",
    "            plt.scatter(nearby_unobs_good_z_dim.RA, nearby_unobs_good_z_dim.Dec, marker='x', s=list(map(getsize, nearby_unobs_good_z_dim.z)), color=get_color(2), alpha=dimalpha, label=\"Unobs good z dim ({0})\".format(len(nearby_unobs_good_z_dim)))\n",
    "        \n",
    "        # redshift data labels\n",
    "        for k in range(len(nearby_obs)):\n",
    "            plt.text(nearby_obs.iloc[k].RA, nearby_obs.iloc[k].Dec, \"{0:.3f}\".format(nearby_obs.iloc[k].z), size=textsize)\n",
    "        for k in range(len(nearby_unobs)):\n",
    "            plt.text(nearby_unobs.iloc[k].RA, nearby_unobs.iloc[k].Dec, \"{0:.3f}\".format(nearby_unobs.iloc[k].z), size=textsize)\n",
    "\n",
    "        # Circle assigned one\n",
    "        if len(z_match) > 0:\n",
    "            plt.scatter(z_match.RA, z_match.Dec, color=get_color(3), facecolors='none', s=getsize(z_match.z)*2, label=\"Assigned\")\n",
    "            plt.text(z_match.RA, z_match.Dec, \"{0:.3f}\".format(z_match.z), size=textsize)\n",
    "\n",
    "        # Target galaxy\n",
    "        if target_observed:\n",
    "            plt.scatter(target.RA, target.Dec, s=getsize(target.z_obs), color=get_color(1), label=\"Target\")\n",
    "        else:\n",
    "            plt.scatter(target.RA, target.Dec, s=getsize(target.z_obs), marker='X', color=get_color(1), label=\"Target\")  \n",
    "        plt.text(target.RA, target.Dec, \"{0:.3f}\".format(target.z_obs), size=textsize)\n",
    "\n",
    "        plt.xlim(ra_min, ra_max)\n",
    "        plt.ylim(dec_min, dec_max)\n",
    "        plt.xlabel('RA')\n",
    "        plt.xlabel('Dec')\n",
    "        plt.legend()\n",
    "        plt.title(title)\n",
    "        plt.draw()\n",
    "    \n",
    "    else:\n",
    "        print(\"Skipping empty plot for {0}\".format(title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLOTS_TO_MAKE = 10\n",
    "GALAXY_POOL = lost_galaxies\n",
    "\n",
    "#START_INDEX = 777\n",
    "#for i in range(START_INDEX, START_INDEX + PLOTS_TO_MAKE):\n",
    "#    index = lost_galaxies.index[i]\n",
    "#    examine_around(index)\n",
    "print(\"Number of galaxies to choose from: \", len(GALAXY_POOL))\n",
    "indexes = np.random.randint(0, len(GALAXY_POOL)-1, size=PLOTS_TO_MAKE)\n",
    "for i in indexes:\n",
    "    target = GALAXY_POOL.iloc[i]\n",
    "    examine_around(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idea: analyze entire neighborhood and look for groups of similar z galaxies, choose a z from the biggest group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX = 300\n",
    "close = np.empty(MAX)\n",
    "total = np.empty(MAX)\n",
    "frac = np.empty(MAX)\n",
    "for i in range(0,MAX):\n",
    "    target = lost_galaxies.iloc[i]\n",
    "    close[i], total[i], frac[i] = neighbor_exam(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_finished = 81408\n",
    "finished_close = close[0:max_finished]\n",
    "finished_total = total[0:max_finished]\n",
    "with open(ROOT_FOLDER + 'mxxl_lostgal_neighborhood_close.npy', 'wb') as f:\n",
    "    np.save(f, finished_close)\n",
    "with open(ROOT_FOLDER + 'mxxl_lostgal_neighborhood_total.npy', 'wb') as f:\n",
    "    np.save(f, finished_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(ROOT_FOLDER + 'mxxl_lostgal_neighborhood_close.npy', 'rb') as f:\n",
    "    close = np.load(f)\n",
    "\n",
    "with open(ROOT_FOLDER + 'mxxl_lostgal_neighborhood_total.npy', 'rb') as f:\n",
    "    total = np.load(f)\n",
    "\n",
    "frac = close / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(0,30,31)\n",
    "trash = plt.hist(close, bins=bins)\n",
    "plt.title(\"Lost Galaxies Neighbors at ~Correct z\")\n",
    "plt.xlabel(\"Count of Similar z Neighbors\")\n",
    "plt.ylabel(\"Count of Lost Galaxies\")\n",
    "print(\"Hopeless Fraction: \", np.sum(close==0) / len(close))\n",
    "print(\"Essentially Hopeless Fraction: \", (np.sum(close==0) + np.sum(close==1)) / len(close))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viable = close > 1\n",
    "frac[viable]\n",
    "trash=plt.hist(frac[viable], bins=30)\n",
    "plt.title(\"Viable Lost Galaxies: Correct z Neighbor Fraction\")\n",
    "plt.xlabel(\"Fraction with Similar z\")\n",
    "plt.ylabel(\"Count of Viable Lost Galaxies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UCHUU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[('R_MAG_APP', '>f4'), ('R_MAG_ABS', '>f4'), ('G_R_REST', '>f4'), ('G_R_OBS', '>f4'), ('DEC', '>f8'), ('HALO_MASS', '>f4'), ('CEN', '>i4'), ('RES', '>i4'), ('RA', '>f8'), ('Z_COSMO', '>f4'), ('Z', '>f4'), ('STATUS', '>i4'), ('FIRST_ACC_SCALE', '>f4'), ('M_ACC', '>f4'), ('M_VIR_ALL', '>f4'), ('R_VIR', '>f4'), ('V_PEAK', '>f4'), ('R_S', '>f4'), ('V_RMS', '>f4'), ('NGC', '>f4'), ('SGC', '>f4'), ('HALO_ID', '>i8'), ('PID', '>i8')]))\n",
    "\n",
    "filename='/export/sirocco2/tinker/DESI/UCHUU_MOCKS/BGS_LC_Uchuu.fits'\n",
    "u_table = Table.read(filename, format='fits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "APP_MAG_CUT = 19.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec = u_table['DEC']\n",
    "ra = u_table['RA']\n",
    "z_obs = u_table['Z']\n",
    "app_mag = u_table['R_MAG_APP']\n",
    "g_r = u_table['G_R_REST'] # TODO before using ensure it should be rest and not observed\n",
    "central = u_table['CEN']\n",
    "uchuu_halo_mass = u_table['HALO_MASS']\n",
    "uchuu_halo_id = u_table['HALO_ID']\n",
    "\n",
    "bright_filter = app_mag < APP_MAG_CUT \n",
    "redshift_filter = z_obs > 0 \n",
    "keep = np.all([bright_filter, redshift_filter], axis=0)\n",
    "\n",
    "dec = dec[keep]\n",
    "ra = ra[keep]\n",
    "z_obs = z_obs[keep]\n",
    "app_mag = app_mag[keep]\n",
    "g_r = g_r[keep]\n",
    "central = central[keep]\n",
    "uchuu_halo_mass = uchuu_halo_mass[keep]\n",
    "uchuu_halo_id = uchuu_halo_id[keep]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading UCHUU truth data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the FITS file:\n",
    "#[('R_MAG_APP', '>f4'), ('R_MAG_ABS', '>f4'), ('G_R_REST', '>f4'), ('G_R_OBS', '>f4'), ('DEC', '>f8'), ('HALO_MASS', '>f4'), ('CEN', '>i4'), ('RES', '>i4'), ('RA', '>f8'), ('Z_COSMO', '>f4'), ('Z', '>f4'), ('STATUS', '>i4'), ('FIRST_ACC_SCALE', '>f4'), ('M_ACC', '>f4'), ('M_VIR_ALL', '>f4'), ('R_VIR', '>f4'), ('V_PEAK', '>f4'), ('R_S', '>f4'), ('V_RMS', '>f4'), ('NGC', '>f4'), ('SGC', '>f4'), ('HALO_ID', '>i8'), ('PID', '>i8')]))\n",
    "\n",
    "def read_uchuu(filename):\n",
    "    dat = Table.read(filename, format='fits')\n",
    "    all_data = dat.to_pandas()\n",
    "\n",
    "    dataset = types.SimpleNamespace()\n",
    "    dataset.filename = filename\n",
    "    dataset.all_data = all_data\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def process_uchuu(uchuu):\n",
    "    \"\"\"\n",
    "    Processes the uchuu data so it is a dataframe similar to the ones we build from the group finder outputs.\n",
    "\n",
    "    Then we can generate the same plots with it to compare.\n",
    "    \"\"\"\n",
    "\n",
    "    print(len(uchuu.all_data))\n",
    "\n",
    "    # remove all rows that are dimmer than our threshold. Drop so we don't waste memory\n",
    "    uchuu.all_data = uchuu.all_data[uchuu.all_data.R_MAG_APP <= 19.5].reset_index(drop=True)\n",
    "\n",
    "    # Remove columns we don't care about\n",
    "    if 'NGC' in uchuu.all_data.columns:\n",
    "        uchuu.all_data = uchuu.all_data.drop(columns=['NGC', 'SGC', 'PID', 'RES', 'V_RMS', 'R_S', 'G_R_REST'])\n",
    "\n",
    "    # Drop bad data\n",
    "    uchuu.all_data = uchuu.all_data[uchuu.all_data.HALO_MASS >= 0].reset_index(drop=True)\n",
    "\n",
    "    print(len(uchuu.all_data))\n",
    "\n",
    "    # For total parity with other ones, missing P_sat, N_sat, L_tot, g_R (and MXXL props of course)\n",
    "    uchuu.all_data = uchuu.all_data.rename(columns={\"DEC\": \"Dec\", \"Z\": \"z\", \"V_PEAK\": \"V_max\", \"R_MAG_APP\": \"app_mag\"}) \n",
    "    uchuu.all_data['is_sat'] = (uchuu.all_data.CEN == 0).astype(int)\n",
    "\n",
    "    uchuu.all_data['logLgal'] = abs_mag_r_to_log_solar_L(uchuu.all_data.R_MAG_ABS)\n",
    "    uchuu.all_data['L_gal'] = np.power(10, uchuu.all_data.logLgal)\n",
    "    uchuu.all_data['M_halo'] = uchuu.all_data.HALO_MASS * 10**10 # TODO check this\n",
    "    uchuu.all_data['Mh_bin'] = pd.cut(x = uchuu.all_data['M_halo'], bins = Mhalo_bins, labels = Mhalo_labels, include_lowest = True)\n",
    "\n",
    "    centrals = uchuu.all_data[uchuu.all_data.CEN == 1]\n",
    "    loglcen_means = centrals.groupby('Mh_bin').logLgal.mean()\n",
    "    loglcen_scatter = centrals.groupby('Mh_bin').logLgal.std()\n",
    "\n",
    "    # Compute f_sat(Lgal)\n",
    "    uchuu.all_data['Lgal_bin'] = pd.cut(x = uchuu.all_data['L_gal'], bins = L_gal_bins, labels = L_gal_labels, include_lowest = True)\n",
    "    f_sat = uchuu.all_data.groupby('Lgal_bin').is_sat.mean()\n",
    "    Lgal_counts = uchuu.all_data.groupby('Lgal_bin').RA.count()\n",
    "\n",
    "    uchuu.Mhalo_bins = Mhalo_bins\n",
    "    uchuu.labels = Mhalo_labels\n",
    "    uchuu.centrals = centrals\n",
    "    uchuu.loglcen_means = loglcen_means\n",
    "    uchuu.loglcen_scatter = loglcen_scatter\n",
    "    uchuu.L_gal_bins = L_gal_bins\n",
    "    uchuu.L_gal_labels = L_gal_labels\n",
    "    uchuu.f_sat = f_sat\n",
    "    uchuu.Lgal_counts = Lgal_counts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uchuu = read_uchuu('/export/sirocco2/tinker/DESI/UCHUU_MOCKS/BGS_LC_Uchuu.fits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_uchuu(uchuu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uchuu.name = \"Uchuu <19.5\"\n",
    "uchuu.color = get_color(8)\n",
    "uchuu.marker = '-'\n",
    "#post_process(uchuu)\n",
    "with open(ROOT_FOLDER + uchuu.name, 'wb') as f:\n",
    "    pickle.dump(uchuu, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"UCHUU Statistics for < 19.5 r-mag sample\")\n",
    "print(f\"Number of centrals  : {len(uchuu.centrals)}\")\n",
    "print(f\"Number of satellites: {np.sum(uchuu.all_data.is_sat)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots(all,simple_2, uchuu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#UCHUU Statistics for < 19.5 r-mag sample\n",
    "#Number of centrals  : 25329452\n",
    "#Number of satellites: 6946979"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3067ead486e059ec00ffe7555bdb889e6e264a24dc711bf108106cc7baee8d5d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
