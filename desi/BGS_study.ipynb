{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as c\n",
    "from astropy.wcs import WCS\n",
    "import astropy.coordinates as coord\n",
    "import astropy.units as u\n",
    "from pyutils import *\n",
    "import astropy.io.fits as fits\n",
    "import healpy as hp\n",
    "from astropy.table import Table,join\n",
    "from dataloc import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO ensure this is right\n",
    "def get_app_mag(flux):\n",
    "    \"\"\"This converts nanomaggies into Pogson magnitudes\"\"\"\n",
    "    return 22.5 - 2.5*np.log10(flux)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a merged master BGS data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fastspecfit for DN4000\n",
    "hdul = fits.open(BGS_FASTSPEC_FILE, memmap=True)\n",
    "#hdul[1].columns\n",
    "data = hdul[1].data\n",
    "fastspecfit_id = data['TARGETID']\n",
    "DN4000 = data['DN4000'] # TODO there is also DN4000_OBS and DN4000_MODEL (and inverse variance)\n",
    "hdul.close()\n",
    "\n",
    "print(len(ids))\n",
    "print(len(DN4000))\n",
    "\n",
    "fastspecfit_table = Table([fastspecfit_id, DN4000], names=('TARGETID', 'DN4000'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main file\n",
    "main_table = Table.read(BGS_ANY_FULL_FILE, format='fits')\n",
    "print(len(main_table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prob obs file\n",
    "p_table = Table.read(BGS_PROB_OBS_FILE, format='fits')\n",
    "print(len(p_table))\n",
    "\n",
    "# Join them all on TARGETID\n",
    "joined_table = join(main_table, p_table, keys=\"TARGETID\")\n",
    "print(len(joined_table))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The lost galaxies will not have fastspecfit rows I think\n",
    "final_table = join(joined_table, fastspecfit_table, join_type='left', keys=\"TARGETID\")\n",
    "print(len(final_table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check that everything went as intended\n",
    "assert len(final_table) == len(main_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to needed columns only and save\n",
    "final_table.keep_columns(['TARGETID', 'SPECTYPE', 'DEC', 'RA', 'Z_not4clus', 'FLUX_R', 'FLUX_G', 'BITWEIGHTS', 'PROB_OBS', 'ZWARN', 'DELTACHI2', 'NTILE', 'ABSMAG_SDSS_R', 'ABSMAG_SDSS_G', 'DN4000'])\n",
    "final_table.write(IAN_BGS_MERGED_FILE, format='fits', overwrite='True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(main_table)\n",
    "del(p_table)\n",
    "del(fastspecfit_table)\n",
    "del(final_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine data in Merged BGS File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = Table.read(IAN_BGS_MERGED_FILE, format='fits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See two equivalent ways of determining which rows are for unobserved galaxies\n",
    "one=table['ZWARN'] == 999999\n",
    "two=table['Z_not4clus'].mask\n",
    "assert(np.all(one == two))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(table['Z_not4clus'], bins=50)\n",
    "plt.title(\"Z_not4clus\")\n",
    "plt.yscale('log')\n",
    "print(np.min(table['Z_not4clus']), np.max(table['Z_not4clus']))\n",
    "print(table['Z_not4clus'].mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.unique(table['ZWARN']))\n",
    "#print(np.unique(table['ZWARN_MTL']))\n",
    "print(np.unique(table['SPECTYPE']))\n",
    "print(np.unique(table['NTILE']))\n",
    "#print(np.unique(table['TARGET_STATE']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cut to the galaxy data we actually need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO this gets easilly out of sync with the .py file that does the 'production' filtering\n",
    "\n",
    "APP_MAG_CUT = 19.5\n",
    "Z_MIN = 0.01\n",
    "Z_MAX = 0.8\n",
    "obj_type = table['SPECTYPE'].data.data\n",
    "dec = table['DEC']\n",
    "ra = table['RA']\n",
    "z_obs = table['Z_not4clus'].data.data\n",
    "target_id = table['TARGETID']\n",
    "flux_r = table['FLUX_R']\n",
    "flux_g = table['FLUX_G']\n",
    "app_mag_r = get_app_mag(table['FLUX_R'])\n",
    "app_mag_g = get_app_mag(table['FLUX_G'])\n",
    "g_r_apparent = app_mag_g - app_mag_r\n",
    "sdss_g_r = table['ABSMAG_SDSS_G'] - table['ABSMAG_SDSS_R'] \n",
    "p_obs = table['PROB_OBS'] \n",
    "unobserved = table['Z_not4clus'].mask\n",
    "deltachi2 = table['DELTACHI2'].data.data\n",
    "ntiles = table['NTILE']\n",
    "abs_mag_sdss = table['ABSMAG_SDSS_R']\n",
    "dn4000 = table['DN4000'].data.data\n",
    "\n",
    "before_count = len(dec)\n",
    "print(before_count, \"objects in FITS file\")\n",
    "\n",
    "\n",
    "# Make filter array (True/False values)\n",
    "three_pass_filter = table['NTILE'] >= 3 # 3pass coverage\n",
    "galaxy_filter = np.logical_or(obj_type == b'GALAXY', obj_type == b'')\n",
    "galaxy_observed_filter = obj_type == b'GALAXY'\n",
    "app_mag_filter = app_mag_r < APP_MAG_CUT\n",
    "redshift_filter = z_obs > Z_MIN\n",
    "redshift_hi_filter = z_obs < Z_MAX\n",
    "deltachi2_filter = deltachi2 > 40\n",
    "abs_mag_sdss_filter = abs_mag_sdss < 100\n",
    "observed_requirements = np.all([galaxy_observed_filter, app_mag_filter, redshift_filter, redshift_hi_filter, deltachi2_filter, abs_mag_sdss_filter], axis=0)\n",
    "\n",
    "treat_as_unobserved = np.all([galaxy_observed_filter, app_mag_filter, np.invert(deltachi2_filter)], axis=0)\n",
    "unobserved = np.all([app_mag_filter, np.logical_or(unobserved, treat_as_unobserved)], axis=0)\n",
    "keep = np.all([three_pass_filter, np.logical_or(observed_requirements, unobserved)], axis=0)\n",
    "unobserved_3 = np.all([three_pass_filter, unobserved], axis=0)\n",
    "\n",
    "print(f\"There are {np.sum(galaxy_filter)} galaxy targets in the sample, of which {np.sum(galaxy_observed_filter)} are observed.\") \n",
    "print(f\"Of these, {np.sum(observed_requirements)} are in the bright (<{APP_MAG_CUT} mag) sample and pass our quality checks.\")\n",
    "print(f\"Of those, {np.sum(keep)} are in the 3-pass coverage area.\")\n",
    "print(f\"There are {np.sum(unobserved_3)} unobserved galaxies in the 3-pass coverage area, including bad observed galaxies.\")\n",
    "\n",
    "# Filter to observed 3 pass galaxies, which rest of the file usually wants\n",
    "obj_type = obj_type[keep]\n",
    "dec = dec[keep]\n",
    "ra = ra[keep]\n",
    "z_obs = z_obs[keep]\n",
    "target_id = target_id[keep] \n",
    "flux_r = flux_r[keep]\n",
    "app_mag_r = app_mag_r[keep]\n",
    "app_mag_g = app_mag_g[keep]\n",
    "g_r_apparent = g_r_apparent[keep]\n",
    "p_obs = p_obs[keep]\n",
    "unobserved = unobserved[keep]\n",
    "deltachi2 = deltachi2[keep]\n",
    "ntiles = ntiles[keep]\n",
    "abs_mag_sdss = abs_mag_sdss[keep]\n",
    "sdss_g_r = sdss_g_r[keep]\n",
    "dn4000 = dn4000[keep]\n",
    "indexes_not_assigned = np.argwhere(unobserved)\n",
    "\n",
    "after_count = len(dec)\n",
    "\n",
    "print(f\"\\nAfter all filters we have {after_count} of the original {before_count} rows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Color Analysis\n",
    "\n",
    "Lesson from this analysis: the BGS data, workign with my 0.1^G-R with GAMA k-corrections, does not distribute a per logLgal bin G-R; the global 0.76 split seems to work for all bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = app_mag_to_abs_mag(app_mag_g, z_obs)\n",
    "R = app_mag_to_abs_mag(app_mag_r, z_obs)\n",
    "\n",
    "G_R = G - R\n",
    "\n",
    "Gk = k_correct_bgs(G, z_obs, g_r_apparent, band='g')\n",
    "Rk = k_correct_bgs(R, z_obs, g_r_apparent, band='r')\n",
    "\n",
    "G_R_k = Gk - Rk\n",
    "\n",
    "Gk_GAMA = k_correct_gama(G, z_obs, g_r_apparent, band='g')\n",
    "Rk_GAMA = k_correct_gama(R, z_obs, g_r_apparent, band='r')\n",
    "\n",
    "G_R_k_GAMA = Gk_GAMA - Rk_GAMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison of g-r computed a few ways\n",
    "bins = np.linspace(0, 2.0, 200)\n",
    "\n",
    "junk=plt.hist(g_r_apparent, bins=bins, alpha=0.5, label=\"g-r\")\n",
    "#junk=plt.hist(sdss_g_r, bins=bins, alpha=0.5, label='From LSS Pipeline')\n",
    "#junk=plt.hist(G_R, bins=bins, alpha=0.5, label=\"G-R\")\n",
    "junk=plt.hist(G_R_k, bins=bins, alpha=0.5, label=\"0.1^(G-R) BGS poly\")\n",
    "junk=plt.hist(G_R_k_GAMA, bins=bins, alpha=0.5, label=\"0.1^(G-R) GAMA poly\")\n",
    "plt.xlabel(\"g-r\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend()\n",
    "plt.xlim(0.0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can see global GLOBAL_RED_COLOR_CUT=0.76 here\n",
    "junk=plt.hist(G_R_k, bins=300, alpha=0.5, label=\"0.1^(G-R) GAMA-style\")\n",
    "plt.legend()\n",
    "plt.xlim(0.5, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyutils import *\n",
    "print(BGS_LOGLGAL_BINS)\n",
    "print(BINWISE_RED_COLOR_CUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_quiescent_BGS_gmr(np.array([5.8, 9.0, 14.5]), np.array([0.5, 0.9, 0.9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get logLgal bins\n",
    "log_L_gal = abs_mag_r_to_log_solar_L(Rk) \n",
    "logLgal_bin_idx = np.digitize(log_L_gal, BGS_LOGLGAL_BINS)\n",
    "# 0 is less than the lowest, len(BGS_LOGLGAL_BINS) is greater than the highest entry in BGS_LOGLGAL_BINS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.min(log_L_gal))\n",
    "print(np.max(log_L_gal))\n",
    "print(np.min(logLgal_bin_idx))\n",
    "print(np.max(logLgal_bin_idx))\n",
    "plt.hist(log_L_gal, bins=BGS_LOGLGAL_BINS, align='mid')\n",
    "#plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a plot of G_R_k in each logLgal bin\n",
    "for i in range(0, len(BGS_LOGLGAL_BINS)+1):\n",
    "    galaxy_idx_for_this_bin = logLgal_bin_idx == i\n",
    "\n",
    "    plt.figure(dpi=80, figsize=(10, 6))\n",
    "    junk=plt.hist(G_R_k[galaxy_idx_for_this_bin], bins=np.arange(0,1.3,0.02), label=f\"0.1^(G-R) Bin {i}\", align='mid')\n",
    "    plt.legend()\n",
    "    plt.xlim(0.4, 1.2)\n",
    "    plt.xticks(np.arange(0.4, 1.2, 0.04))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mag1 = abs_mag_sdss\n",
    "mag2 = R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Absolute Magnitudes\n",
    "# Difference is how we k-correct I believe\n",
    "bins = np.linspace(-25, -10, 100)\n",
    "my_counts, my_bins, my_p = plt.hist(mag2, label=\"my abs_mag\", bins=bins, alpha=0.5)\n",
    "alex_counts, alex_bins, alex_p = plt.hist(mag1, label=\"ABSMAG_SDSS_R\", bins=bins, alpha=0.5)\n",
    "plt.xlabel(\"Absolute Mag\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Compare Absolute Mags\")\n",
    "#plt.yscale('log')\n",
    "plt.legend()\n",
    "\n",
    "print(f\"The peak is shifted from ABSMAG_SDSS_R {alex_bins[np.argmax(alex_counts)]:.1f} to my {my_bins[np.argmax(my_counts)]:.1f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=make_map(ra, dec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dn4000 Comparison (BGS, SDSS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdss = pd.read_csv(SDSS_v1_DAT_FILE, delimiter=' ', names=('RA', 'Dec', 'z', 'logLgal', 'V_max', 'quiescent', 'chi'), index_col=False)\n",
    "sdss_galprops = pd.read_csv(\"../data/sdss_galprops_v1.0.dat\", delimiter=' ', names=('Mag_g', 'Mag_r', 'sigma_v', 'Dn4000', 'concentration', 'log_M_star'))\n",
    "sdss = pd.merge(sdss, sdss_galprops, left_index=True, right_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(dn4000, bins=np.linspace(-0.5, 5.0, 100), alpha=0.6, label=\"BGS Y1\")\n",
    "plt.hist(sdss.Dn4000, bins=np.linspace(-0.5, 5.0, 100), alpha=0.8, label=\"SDSS\")\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.xlabel('Dn4000')\n",
    "plt.ylabel('Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(dn4000, bins=np.linspace(-0.5, 5.0, 100), alpha=0.6, label=\"BGS Y1\")\n",
    "plt.hist(sdss.Dn4000, bins=np.linspace(-0.5, 5.0, 100), alpha=0.8, label=\"SDSS\")\n",
    "plt.legend()\n",
    "plt.xlabel('Dn4000')\n",
    "plt.ylabel('Count')\n",
    "plt.xlim(0.9,2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdss_catalog = coord.SkyCoord(ra=sdss.RA.to_numpy()*u.degree, dec=sdss.Dec.to_numpy()*u.degree, frame='icrs')\n",
    "BGS_catalog = coord.SkyCoord(ra=ra*u.degree, dec=dec*u.degree, frame='icrs')\n",
    "\n",
    "neighbor_indexes, d2d, d3d = coord.match_coordinates_sky(BGS_catalog, sdss_catalog, storekdtree='sdss')\n",
    "ang_distances = d2d.to(u.arcsec).value\n",
    "\n",
    "match_found_filter = ang_distances < 3.0\n",
    "bgs_matches = dn4000[match_found_filter]\n",
    "sdss_indexes = neighbor_indexes[match_found_filter]\n",
    "sdss_matches = sdss.iloc[sdss_indexes].Dn4000.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{np.isclose(bgs_matches, sdss_matches, atol=0.05).sum() / len(bgs_matches)} of the matches are within 0.05 of each other.\")\n",
    "print(f\"{np.isclose(bgs_matches, sdss_matches, atol=0.1).sum() / len(bgs_matches)} of the matches are within 0.1 of each other.\")\n",
    "print(f\"{np.isclose(bgs_matches, sdss_matches, atol=0.2).sum() / len(bgs_matches)} of the matches are within 0.2 of each other.\")\n",
    "print(f\"{np.isclose(bgs_matches, sdss_matches, atol=0.3).sum() / len(bgs_matches)} of the matches are within 0.3 of each other.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=make_map(ra, dec)\n",
    "fig=make_map(sdss.RA.to_numpy(), sdss.Dec.to_numpy(), fig=fig, alpha=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(sdss_matches, bgs_matches, s=1, alpha=.2)\n",
    "plt.xlabel(\"SDSS Dn4000\")\n",
    "plt.ylabel(\"BGS Dn4000\")\n",
    "plt.xlim(1, 2.3)\n",
    "plt.ylim(1, 2.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'SDSS_Dn4000': sdss_matches, 'BGS_Dn4000': bgs_matches})\n",
    "df['diff_frac'] =  (df['BGS_Dn4000'] - df['SDSS_Dn4000']) / df['SDSS_Dn4000']\n",
    "bins = np.linspace(-1, 5, 60)\n",
    "labels = bins[0:len(bins)-1] \n",
    "df['dn4000_sdssbin'] = pd.cut(x = sdss_matches, bins = bins, labels = labels, include_lowest = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=80)\n",
    "diff_mean = df.groupby('dn4000_sdssbin').diff_frac.mean()\n",
    "diff_std= df.groupby('dn4000_sdssbin').diff_frac.std()\n",
    "\n",
    "plt.errorbar(labels, diff_mean, yerr=diff_std)\n",
    "plt.xlabel(\"SDSS Dn4000\")\n",
    "plt.ylabel(\"< (BGS-SDSS) / SDSS >\")\n",
    "plt.xlim(0.8, 2.4)\n",
    "plt.ylim(-0.75, 0.75)\n",
    "plt.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dn4000 Lgal Bin Analysis\n",
    "\n",
    "Run Color Analysis and Dn4000 Comparison first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a plot of Dn4000 in each logLgal bin\n",
    "fig,axes=plt.subplots(dpi=80, figsize=(10, 3*len(BGS_LOGLGAL_BINS)//2), ncols=2, nrows=len(BGS_LOGLGAL_BINS)//2)\n",
    "axes = np.ravel(axes)\n",
    "\n",
    "for i in range(0, len(BGS_LOGLGAL_BINS)-1):\n",
    "    galaxy_idx_for_this_bin = logLgal_bin_idx == i+1\n",
    "\n",
    "    junk=axes[i].hist(dn4000[galaxy_idx_for_this_bin], bins=np.arange(1,2.2,0.02), label=f\"Dn4000 for logLgal Bin {i+1}\", align='mid')\n",
    "    axes[i].legend()\n",
    "    axes[i].set_xlim(1, 2.2)\n",
    "    axes[i].set_xticks(np.arange(1, 2.2, 0.1))\n",
    "\n",
    "    # draw a vertical line at get_SDSS_Dcrit(logLgal)\n",
    "    axes[i].axvline(x=get_SDSS_Dcrit(BGS_LOGLGAL_BINS[i]), color='r', linestyle='-')\n",
    "\n",
    "axes = np.reshape(axes, (2, len(BGS_LOGLGAL_BINS)//2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Randoms Analysis for Footprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rtable = Table.read(BGS_RAND_FILE, format='fits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_dec = rtable['DEC']\n",
    "r_ra = rtable['RA']\n",
    "r_ntiles = rtable['NTILE']\n",
    "\n",
    "RANDOMS_DENSITY = 2500 # per square degree, Ashley Ross paper on LSS pipeline or elsewhere in docs\n",
    "\n",
    "onepass_footprint = len(r_dec) / RANDOMS_DENSITY # in degrees squared\n",
    "onepass_frac_area = onepass_footprint / DEGREES_ON_SPHERE\n",
    "\n",
    "three_pass_filter = r_ntiles >= 3 # 3pass coverage\n",
    "r_dec3 = r_dec[three_pass_filter]\n",
    "r_ra3 = r_ra[three_pass_filter]\n",
    "\n",
    "threepass_footprint = len(r_dec3) / RANDOMS_DENSITY # in degrees squared\n",
    "threepass_frac_area = threepass_footprint / DEGREES_ON_SPHERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"BGS Y1 1pass Footprint calculated from randoms is {onepass_footprint} square degrees or frac_area={onepass_frac_area}\")\n",
    "print(f\"BGS Y1 3pass Footprint calculated from randoms is {threepass_footprint} square degrees or frac_area={threepass_frac_area}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View Healpix Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BGS_N = hp.read_map(ROOT_FOLDER + 'BGS_BRIGHT_mapprops_healpix_nested_nside256_N.fits')\n",
    "BGS_S = hp.read_map(ROOT_FOLDER + 'BGS_BRIGHT_mapprops_healpix_nested_nside256_S.fits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp.mollview(\n",
    "    BGS_N,\n",
    "    coord=[\"G\", \"E\"],\n",
    "    title=\"Histogram equalized Ecliptic\",\n",
    "    unit=\"Galaxies\",\n",
    "    norm=\"hist\",\n",
    "    min=0,\n",
    "    max=1,\n",
    ")\n",
    "hp.graticule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp.mollview(\n",
    "    BGS_S,\n",
    "    coord=[\"G\", \"E\"],\n",
    "    title=\"Histogram equalized Ecliptic\",\n",
    "    unit=\"Galaxies\",\n",
    "    norm=\"hist\",\n",
    "    min=0,\n",
    "    max=1,\n",
    ")\n",
    "hp.graticule()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ian-conda311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
